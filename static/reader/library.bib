@article{RichardSocherAlexPerelyginJeanY.WuJasonChuangChristopherD.Manning2013,
   abstract = {Â© 2013 Association for Computational Linguistics. Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
   author = {Andrew Y. Ng and Christopher Potts Richard Socher, Alex Perelygin, Jean Y.Wu, Jason Chuang, Christopher D. Manning},
   doi = {10.1371/journal.pone.0073791},
   issn = {19326203},
   issue = {9},
   journal = {PLoS ONE},
   pages = {1631-1642},
   pmid = {24086296},
   title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
   volume = {8},
   url = {http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf%5Cnhttp://www.aclweb.org/anthology/D13-1170%5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf%5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
   year = {2013},
}
@article{Cheng2019,
   abstract = {Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low. This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchor-free and NMS-free object detection model called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors.},
   author = {Zehua Cheng and Yuxiang Wu and Zhenghua Xu and Thomas Lukasiewicz and Weiyang Wang},
   pages = {1-10},
   title = {Segmentation is All You Need},
   url = {http://arxiv.org/abs/1904.13300},
   year = {2019},
}
@article{BoZong2018,
   abstract = {Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.},
   author = {Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and Cristian Lumezanu and Daeki Cho and Haifeng Chen},
   doi = {10.1002/dvdy.21778},
   issn = {10588388},
   issue = {2016},
   journal = {Iclr2018},
   pages = {1-13},
   pmid = {18985768},
   title = {Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
   year = {2018},
}
@article{Zaremba2014,
   abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
   author = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
   issue = {2013},
   pages = {1-8},
   title = {Recurrent Neural Network Regularization},
   url = {http://arxiv.org/abs/1409.2329},
   year = {2014},
}
@article{Gholami2011,
   author = {Amir Gholami and Kiseok Kwon and Bichen Wu and Zizheng Tai and Xiangyu Yue and Peter Jin and Sicheng Zhao and Kurt Keutzer and U C Berkeley},
   title = {SqueezeNext : Hardware-Aware Neural Network Design},
   year = {2011},
}
@article{Chena,
   author = {Tianshi Chen and Jia Wang and Yunji Chen and Olivier Temam},
   title = {DianNao : A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning},
}
@article{Chen,
   author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan},
   issue = {1},
   title = {TVM : An Automated End-to-End Optimizing Compiler for Deep Learning},
}
@article{Zhang,
   author = {Tianyi Zhang and Zhiqiu Lin and Guandao Yang and Christopher De Sa},
   title = {QPyTorch : A Low-Precision Arithmetic Simulation Framework},
}
@article{Mellempudi,
   author = {Naveen Mellempudi and Dipankar Das},
   title = {Mixed Precision Training With 8-bit Floating Point},
}
@article{Narang2018,
   author = {Sharan Narang and Gregory Diamos and Erich Elsen and Paulius Micikevicius and Jonah Alben and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
   pages = {1-12},
   title = {Mixed Precision Training},
   year = {2018},
}
@article{Dally2018,
   author = {William J Dally and Yu Wang},
   pages = {1-13},
   title = {DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING},
   year = {2018},
}
@article{,
   author = {Adrian M Caulfield and Eric S Chung and Puneet Kaur and Joo-young Kim Daniel and Lo Todd and Massengill Kalin},
   isbn = {9781509035083},
   title = {A Cloud-Scale Acceleration Architecture},
}
@article{Lattnera,
   author = {Chris Lattner},
   issue = {4},
   title = {LLVM : A Compilation Framework for Lifelong Program Analysis & Transformation},
}
@article{Howard2012,
   author = {Andrew G Howard and Weijun Wang},
   title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
   year = {2012},
}
@article{Pedram2016,
   author = {Ardavan Pedram and Stephen Richardson},
   issue = {Xx},
   pages = {1-8},
   title = {Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon Era},
   volume = {XX},
   year = {2016},
}
@article{Tan2019,
   abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
   author = {Mingxing Tan and Quoc V. Le},
   month = {5},
   title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
   url = {http://arxiv.org/abs/1905.11946},
   year = {2019},
}
@article{Chen2018,
   abstract = {We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware-specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.},
   author = {Tianqi Chen and Lianmin Zheng and Eddie Yan and Ziheng Jiang and Thierry Moreau and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
   month = {5},
   title = {Learning to Optimize Tensor Programs},
   url = {http://arxiv.org/abs/1805.08166},
   year = {2018},
}
@article{Alqahtani2019,
   abstract = {Deep learning has permeated through many aspects of computing/processing systems in recent years. While distributed training architectures/frameworks are adopted for training large deep learning models quickly, there has not been a systematic study of the communication bottlenecks of these architectures and their effects on the computation cycle time and scalability. In order to analyze this problem for synchronous Stochastic Gradient Descent (SGD) training of deep learning models, we developed a performance model of computation time and communication latency under three different system architectures: Parameter Server (PS), peer-to-peer (P2P), and Ring allreduce (RA). To complement and corroborate our analytical models with quantitative results, we evaluated the computation and communication performance of these system architectures of the systems via experiments performed with Tensorflow and Horovod frameworks. We found that the system architecture has a very significant effect on the performance of training. RA-based systems achieve scalable performance as they successfully decouple network usage from the number of workers in the system. In contrast, 1PS systems suffer from low performance due to network congestion at the parameter server side. While P2P systems fare better than 1PS systems, they still suffer from significant network bottleneck. Finally, RA systems also excel by virtue of overlapping computation time and communication time, which PS and P2P architectures fail to achieve.},
   author = {Salem Alqahtani and Murat Demirbas},
   month = {9},
   title = {Performance Analysis and Comparison of Distributed Machine Learning Systems},
   url = {http://arxiv.org/abs/1909.02061},
   year = {2019},
}
@misc{Verbraeken,
   abstract = {The demand for artiÃ¯Â¿Â¿cial intelligence has grown signiÃ¯Â¿Â¿cantly over the last decade and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, in order to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges, Ã¯Â¿Â¿rst and foremost the eÃ¯Â¿Â¿cient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the Ã¯Â¿Â¿eld by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available. 1 INTRODUCTION The rapid development of new technologies in recent years has led to an unprecedented growth of data collection. Machine Learning (ML) algorithms are increasingly being used to analyze datasets and build decision making systems for which an algorithmic solution is not feasible due to the complexity of the problem. Examples include controlling self-driving cars [23], recognizing speech [8], or predicting consumer behavior [82]. In some cases, the long runtime of training the models steers solution designers towards using distributed systems for an increase of parallelization and total amount of I/O bandwidth, as the training data required for sophisticated applications can easily be in the order of terabytes [29]. In other cases, a centralized solution is not even an option when data is inherently distributed or too big to store on single machines. Examples include transaction processing in larger enterprises on data that is stored in diÃ¯Â¿Â¿erent locations [19] or astronomical data that is too large to move and centralize [125]. In order to make these types of datasets accessible as training data for machine learning problems, algorithms have to be chosen and implemented that enable parallel computation, data distribution, and resilience to failures. A rich and diverse ecosystem of research has been conducted in this Ã¯Â¿Â¿eld, which we categorize and discuss in this article. In contrast to prior surveys on distributed machine learning ([120][124]) or related Ã¯Â¿Â¿elds ([153][87][123][122][171][144]) we apply a wholistic view to the problem and discuss the practical aspects of state-of-the-art machine learning from a distributed systems angle.},
   author = {Joost Verbraeken and Matthijs Wolting and Jonathan Katzy and Jeroen Klop-Penburg and Jan S Rellermeyer},
   keywords = {CCS Concepts: â¢ General and reference â Surveys an,Distributed Systems,â¢ Computer systems organization â Distributed arch,â¢ Computing methodolo-gies â Machine learning},
   title = {A Survey on Distributed Machine Learning},
}
@misc{Ho,
   abstract = {We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing , as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.},
   author = {Qirong Ho and James Cipar and Henggang Cui and Jin Kyu Kim and Seunghak Lee and Phillip B Gibbons and Garth A Gibson and Gregory R Ganger and Eric P Xing},
   title = {More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server},
}
@inproceedings{Wei2015,
   abstract = {At the core of Machine Learning (ML) analytics is often an expert-suggested model, whose parameters are refined by iteratively processing a training dataset until convergence. The completion time (i.e. convergence time) and quality of the learned model not only depends on the rate at which the refinements are generated but also the quality of each refinement. While data-parallel ML applications often employ a loose consistency model when updating shared model parameters to maximize parallelism, the accumulated error may seriously impact the quality of refinements and thus delay completion time, a problem that usually gets worse with scale. Although more immediate propagation of updates reduces the accumulated error, this strategy is limited by physical network bandwidth. Additionally, the performance of the widely used stochastic gradient descent (SGD) algorithm is sensitive to step size. Simply increasing communication often fails to bring improvement without tuning step size accordingly and tedious hand tuning is usually needed to achieve optimal performance. This paper presents BÃ¶sen, a system that maximizes the network communication efficiency under a given intermachine network bandwidth budget to minimize parallel error, while ensuring theoretical convergence guarantees for large-scale data-parallel ML applications. Furthermore, BÃ¶sen prioritizes messages most significant to algorithm convergence, further enhancing algorithm convergence. Finally, BÃ¶sen is the first distributed implementation of the recently presented adaptive revision algorithm, which provides orders of magnitude improvement over a carefully tuned fixed schedule of step size refinements for some SGD algorithms. Experiments on two clusters with up to 1024 cores show that our mechanism significantly improves upon static communication schedules.},
   author = {Jinliang Wei and Wei Dai and Aurick Qiao and Qirong Ho and Henggang Cui and Gregory R. Ganger and Phillip B. Gibbons and Garth A. Gibson and Eric P. Xing},
   doi = {10.1145/2806777.2806778},
   journal = {ACM SoCC 2015 - Proceedings of the 6th ACM Symposium on Cloud Computing},
   month = {8},
   pages = {381-394},
   publisher = {Association for Computing Machinery, Inc},
   title = {Managed communication and consistency for fast data-parallel iterative analytics},
   year = {2015},
}
@article{Jia2018a,
   abstract = {The computational requirements for training deep neural networks (DNNs) have grown to the point that it is now standard practice to parallelize training. Existing deep learning systems commonly use data or model parallelism, but unfortunately, these strategies often result in suboptimal parallelization performance. In this paper, we define a more comprehensive search space of parallelization strategies for DNNs called SOAP, which includes strategies to parallelize a DNN in the Sample, Operation, Attribute, and Parameter dimensions. We also propose FlexFlow, a deep learning framework that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy's performance and is three orders of magnitude faster than prior approaches that have to execute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks on two GPU clusters and show that FlexFlow can increase training throughput by up to 3.8x over state-of-the-art approaches, even when including its search time, and also improves scalability.},
   author = {Zhihao Jia and Matei Zaharia and Alex Aiken},
   month = {7},
   title = {Beyond Data and Model Parallelism for Deep Neural Networks},
   url = {http://arxiv.org/abs/1807.05358},
   year = {2018},
}
@inproceedings{Watcharapichat2016,
   abstract = {Distributed systems for the training of deep neural networks (DNNs) with large amounts of data have vastly improved the accuracy of machine learning models for image and speech recognition. DNN systems scale to large cluster deployments by having worker nodes train many model replicas in parallel; to ensure model convergence, parameter servers periodically synchronise the replicas. This raises the challenge of how to split resources between workers and parameter servers so that the cluster CPU and network resources are fully utilised without introducing bottlenecks. In practice, this requires manual tuning for each model configuration or hardware type. We describe Ako, a decentralised dataflow-based DNN system without parameter servers that is designed to saturate cluster resources. All nodes execute workers that fully use the CPU resources to update model replicas. To synchronise replicas as often as possible subject to the available network bandwidth, workers exchange partitioned gradient updates directly with each other. The number of partitions is chosen so that the used network bandwidth remains constant, independently of cluster size. Since workers eventually receive all gradient partitions after several rounds, convergence is unaffected. For the ImageNet benchmark on a 64-node cluster, Ako does not require any resource allocation decisions, yet converges faster than deployments with parameter servers.},
   author = {Pijika Watcharapichat and Victoria Lopez Morales and Raul Castro Fernandez and Peter Pietzuch},
   doi = {10.1145/2987550.2987586},
   journal = {Proceedings of the 7th ACM Symposium on Cloud Computing, SoCC 2016},
   keywords = {Decentralised architecture,Deep learning,Distributed machine learning},
   month = {10},
   pages = {84-97},
   publisher = {Association for Computing Machinery, Inc},
   title = {Ako: Decentralised deep learning with partial gradient exchange},
   year = {2016},
}
@inproceedings{Narayanan2019,
   abstract = {DNN training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to parallelizing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present PipeDream, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. Unlike traditional pipelining, DNN training is bi-directional, where a forward pass through the computation graph is followed by a backward pass that uses state and intermediate data computed during the forward pass. NaÃ¯ve pipelining can thus result in mismatches in state versions used in the forward and backward passes, or excessive pipeline flushes and lower hardware efficiency. To address these challenges, PipeDream versions model parameters for numerically correct gradient computations, and schedules forward and backward passes of different minibatches concurrently on different workers with minimal pipeline stalls. PipeDream also automatically partitions DNN layers among workers to balance work and minimize communication. Extensive experimentation with a range of DNN tasks, models, and hardware configurations shows that PipeDream trains models to high accuracy up to 5.3Ã faster than commonly used intra-batch parallelism techniques.},
   author = {Deepak Narayanan and Aaron Harlap and Amar Phanishayee and Vivek Seshadri and Nikhil R. Devanur and Gregory R. Ganger and Phillip B. Gibbons and Matei Zaharia},
   doi = {10.1145/3341301.3359646},
   journal = {SOSP 2019 - Proceedings of the 27th ACM Symposium on Operating Systems Principles},
   month = {10},
   pages = {1-15},
   publisher = {Association for Computing Machinery, Inc},
   title = {Pipedream: Generalized pipeline parallelism for DNN training},
   year = {2019},
}
@inproceedings{Shao2019,
   abstract = {Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarsegrained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with fine-grained chiplets for deep learning inference, an application area with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deeplearning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with batch size of one, delivering inference latency of 0.50 ms.},
   author = {Yakun Sophia Shao and Jason Clemons and Rangharajan Venkatesan and Brian Zimmer and Matthew Fojtik and Nan Jiang and Ben Keller and Alicia Klinefelter and Nathaniel Pinckney and Priyanka Raina and Stephen G. Tell and Yanqing Zhang and William J. Dally and Joel Emer and C. Thomas Gray and Brucek Khailany and Stephen W. Keckler},
   doi = {10.1145/3352460.3358302},
   issn = {10724451},
   journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
   keywords = {Accelerator architecture,Multi-chip module,Neural networks},
   month = {10},
   pages = {14-27},
   publisher = {IEEE Computer Society},
   title = {Simba: Scaling deep-learning inference with multi-chip-module-based architecture},
   year = {2019},
}
@misc{Bailis2017,
   abstract = {Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.},
   author = {Peter Bailis and Kunle Olukotun and Christopher RÃ© and Matei Zaharia and Stanford Dawn},
   title = {Infrastructure for Usable Machine Learning: The Stanford DAWN Project},
   url = {http://dawn.cs.stanford.edu/},
   year = {2017},
}
@misc{Miao,
   abstract = {Deep learning has improved state-of-the-art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, such as learned parameters and training logs, and comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and accelerate the modeling process. To manage the data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads without losing accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we show that archiving versioned models using deltas poses a new dataset versioning problem and we develop efficient algorithms for solving it. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.},
   author = {Hui Miao and Ang Li and Larry S Davis and Amol Deshpande},
   isbn = {1611.06224v1},
   title = {Towards Unified Data and Lifecycle Management for Deep Learning},
}
@inproceedings{Baylor2017,
   abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components - a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.},
   author = {Denis Baylor and Eric Breck and Heng Tze Cheng and Noah Fiedel and Chuan Yu Foo and Zakaria Haque and Salem Haykal and Mustafa Ispir and Vihan Jain and Levent Koc and Chiu Yuen Koo and Lukasz Lew and Clemens Mewald and Akshay Naresh Modi and Neoklis Polyzotis and Sukriti Ramesh and Sudip Roy and Steven Euijong Whang and Martin Wicke and Jarek Wilkiewicz and Xin Zhang and Martin Zinkevich},
   doi = {10.1145/3097983.3098021},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Continuous training,End-to-end platform,Large-scale machine learning},
   month = {8},
   pages = {1387-1395},
   publisher = {Association for Computing Machinery},
   title = {TFX: A TensorFlow-based production-scale machine learning platform},
   volume = {Part F1296},
   year = {2017},
}
@inproceedings{Jia2019,
   abstract = {Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis. We propose TASO, the first DNN computation graph optimizer that automatically generates graph substitutions. TASO takes as input a list of operator specifications and generates candidate substitutions using the given operators as basic building blocks. All generated substitutions are formally verified against the operator specifications using an automated theorem prover. To optimize a given DNN computation graph, TASO performs a cost-based backtracking search, applying the substitutions to find an optimized graph, which can be directly used by existing DNN frameworks. Our evaluation on five real-world DNN architectures shows that TASO outperforms existing DNN frameworks by up to 2.8Ã, while requiring significantly less human effort. For example, TensorFlow currently contains approximately 53,000 lines of manual optimization rules, while the operator specifications needed by TASO are only 1,400 lines of code.},
   author = {Zhihao Jia and Oded Padon and James Thomas and Todd Warszawski and Matei Zaharia and Alex Aiken},
   doi = {10.1145/3341301.3359630},
   journal = {SOSP 2019 - Proceedings of the 27th ACM Symposium on Operating Systems Principles},
   keywords = {Computation graph substitutions,Deep neural network,Formal verification,Superoptimization},
   month = {10},
   pages = {47-62},
   publisher = {Association for Computing Machinery, Inc},
   title = {TASO: Optimizing deep learning computation with automatic generation of graph substitutions},
   year = {2019},
}
@misc{Li,
   abstract = {We investigate the role of data complexity in the context of binary classification problems. The universal data complexity is defined for a data set as the Kolmogorov complexity of the mapping enforced by the data set. It is closely related to several existing principles used in machine learning such as Occam's razor, the minimum description length, and the Bayesian approach. The data complexity can also be defined based on a learning model, which is more realistic for applications. We demonstrate the application of the data complexity in two learning problems, data decomposition and data pruning. In data decomposition, we illustrate that a data set is best approximated by its principal subsets which are Pareto optimal with respect to the complexity and the set size. In data pruning, we show that outliers usually have high complexity contributions, and propose methods for estimating the complexity contribution. Since in practice we have to approximate the ideal data complexity measures, we also discuss the impact of such approximations.},
   author = {Ling Li and Yaser S Abu-Mostafa},
   title = {Data Complexity in Machine Learning},
   url = {http://resolver.caltech.edu/CaltechCSTR:2006.004},
}
@article{Yang2019a,
   abstract = {Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.},
   author = {Guandao Yang and Tianyi Zhang and Polina Kirichenko and Junwen Bai and Andrew Gordon Wilson and Christopher De Sa},
   month = {4},
   title = {SWALP : Stochastic Weight Averaging in Low-Precision Training},
   url = {http://arxiv.org/abs/1904.11943},
   year = {2019},
}
@inproceedings{Koliousis2018,
   abstract = {Deep learning models are trained on servers with many GPUs, and training must scale with the number of GPUs. Systems such as TensorFlow and Caffe2 train models with parallel synchronous stochastic gradient descent: they process a batch of training data at a time, partitioned across GPUs, and average the resulting partial gradients to obtain an updated global model. To fully utilise all GPUs, systems must increase the batch size, which hinders statistical efficiency. Users tune hyper-parameters such as the learning rate to compensate for this, which is complex and model-specific. We describe CROSSBOW, a new single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size-however small-while scaling to multiple GPUs. CROSSBOW uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method. We introduce SMA, a synchronous variant of model averaging in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model. CROSSBOW achieves high hardware efficiency with small batch sizes by potentially training multiple model replicas per GPU, automatically tuning the number of replicas to maximise throughput. Our experiments show that CROSSBOW improves the training time of deep learning models on an 8-GPU server by 1.3-4x compared to TensorFlow.},
   author = {Alexandros Koliousis and Pijika Watcharapichat and Matthias Weidlich and Luo Mai and Paolo Costa and Peter Pietzuch},
   doi = {10.14778/3342263.3342276},
   issn = {21508097},
   issue = {11},
   journal = {Proceedings of the VLDB Endowment},
   pages = {1399-1413},
   publisher = {VLDB Endowment},
   title = {CROSSBOW: Scaling deep learning with small batch sizes on Multi-GPU servers},
   volume = {12},
   year = {2018},
}
@article{Mayer2019,
   abstract = {Deep Learning (DL) has had an immense success in the recent past, leading to state-of-the-art results in various domains such as image recognition and natural language processing. One of the reasons for this success is the increasing size of DL models and the proliferation of vast amounts of training data being available. To keep on improving the performance of DL, increasing the scalability of DL systems is necessary. In this survey, we perform a broad and thorough investigation on challenges, techniques and tools for scalable DL on distributed infrastructures. This incorporates infrastructures for DL, methods for parallel DL training, multi-tenant resource scheduling and the management of training and model data. Further, we analyze and compare 11 current open-source DL frameworks and tools and investigate which of the techniques are commonly implemented in practice. Finally, we highlight future research trends in DL systems that deserve further research.},
   author = {Ruben Mayer and Hans-Arno Jacobsen},
   month = {3},
   title = {Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques and Tools},
   url = {http://arxiv.org/abs/1903.11314},
   year = {2019},
}
@article{Guan2019,
   abstract = {We propose XPipe, an efficient asynchronous pipeline model parallelism approach for multi-GPU DNN training. XPipe is designed to make use of multiple GPUs to concurrently and continuously train different parts of a DNN model. To improve GPU utilization and achieve high throughput, it splits a mini-batch into a set of micro-batches and allows the overlapping of the pipelines of multiple micro-batches, including those belonging to different mini-batches. Most importantly, the novel weight prediction strategy adopted by XPipe enables it to effectively address the weight inconsistency and staleness issues incurred by the asynchronous pipeline parallelism. As a result, XPipe incorporates the advantages of both synchronous and asynchronous pipeline model parallelism approaches. Concretely, it can achieve very comparable (even slightly better) model accuracy as its synchronous counterpart, while obtaining higher throughput than it. Experimental results show that XPipe outperforms other state-of-the-art synchronous and asynchronous model parallelism approaches.},
   author = {Lei Guan and Wotao Yin and Dongsheng Li and Xicheng Lu},
   month = {10},
   title = {XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training},
   url = {http://arxiv.org/abs/1911.04610},
   year = {2019},
}
@article{Sergeev2018,
   abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
   author = {Alexander Sergeev and Mike Del Balso},
   month = {2},
   title = {Horovod: fast and easy distributed deep learning in TensorFlow},
   url = {http://arxiv.org/abs/1802.05799},
   year = {2018},
}
@article{Dehghani2018,
   abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
   author = {Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Åukasz Kaiser},
   month = {7},
   title = {Universal Transformers},
   url = {http://arxiv.org/abs/1807.03819},
   year = {2018},
}
@article{Yang2019,
   abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
   author = {Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
   month = {6},
   title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
   url = {http://arxiv.org/abs/1906.08237},
   year = {2019},
}
@misc{,
   abstract = {High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents a general technique toward 8-bit low precision inference of convolutional neural networks , including 1) channel-wise scale factors of weights, especially for depthwise convolution, 2) Winograd convolution, and 3) topology-wise 8-bit support. We experiment the techniques on top of a widely-used deep learning framework. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We perform a systemat-ical and comprehensive study on 18 widely-used convolutional neural networks and demonstrate the effectiveness of 8-bit low precision inference across a wide range of applications and use cases, including image classification, object detection, image segmentation, and super resolution. We show that the inference throughput and latency are improved by 1.6X and 1.5X respectively with minimal within 0.6% 1 to no loss in accuracy from FP32 baseline. We believe the methodology can provide the guidance and reference design of 8-bit low precision inference for other frameworks. All the code and models will be publicly available soon.},
   title = {HIGHLY EFFICIENT 8-BIT LOW PRECISION INFERENCE OF CONVOLUTIONAL NEURAL NETWORKS},
   url = {http://nvdla.org/primer.html},
}
@article{Ganesh2020,
   abstract = {Transformer-based models pre-trained on large-scale corpora achieve state-of-the-art accuracy for natural language processing tasks, but are too resource-hungry and compute-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy is model compression, which has attracted extensive attention. This paper summarizes the branches of research on compressing Transformers, focusing on the especially popular BERT model. BERT's complex architecture means that a compression technique that is highly effective on one part of the model, e.g., attention layers, may be less successful on another part, e.g., fully connected layers. In this systematic study, we identify the state of the art in compression for each part of BERT, clarify current best practices for compressing large-scale Transformer models, and provide insights into the inner workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving a lightweight, accurate, and generic natural language processing model.},
   author = {Prakhar Ganesh and Yao Chen and Xin Lou and Mohammad Ali Khan and Yin Yang and Deming Chen and Marianne Winslett and Hassan Sajjad and Preslav Nakov},
   month = {2},
   title = {Compressing Large-Scale Transformer-Based Models: A Case Study on BERT},
   url = {http://arxiv.org/abs/2002.11985},
   year = {2020},
}
@misc{Kitaev,
   abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L 2) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
   author = {Nikita Kitaev and Åukasz Kaiser and Anselm Levskaya and Google Research},
   title = {REFORMER: THE EFFICIENT TRANSFORMER},
   url = {https://hackingsemantics.xyz/2019/leaderboards/},
}
@article{Bstract2020,
   author = {A Bstract},
   title = {The Relation Between Self Attention and CNN},
   year = {2020},
}
@article{,
   title = {Sparse transformer concentrated attention through explicit selection},
}
@misc{,
   abstract = {Self-attention-based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self-attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called Explicit Sparse Transformer. Explicit Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing and computer vision tasks, including neural machine translation , image captioning, and language modeling, all demonstrate the advantages of Explicit Sparse Transformer in model performance. We also show that our proposed sparse attention method achieves comparable or better results than the previous sparse attention method, but significantly reduces training and testing time. For example, the inference speed is twice that of sparsemax in Transformer model.},
   title = {EXPLICIT SPARSE TRANSFORMER: CONCENTRATED ATTENTION THROUGH EXPLICIT SELECTION},
}
@article{Zhai2020,
   abstract = {Automatic speech synthesis is a challenging task that is becoming increasingly important as edge devices begin to interact with users through speech. Typical text-to-speech pipelines include a vocoder, which translates intermediate audio representations into an audio waveform. Most existing vocoders are difficult to parallelize since each generated sample is conditioned on previous samples. WaveGlow is a flow-based feed-forward alternative to these auto-regressive models (Prenger et al., 2019). However, while WaveGlow can be easily parallelized, the model is too expensive for real-time speech synthesis on the edge. This paper presents SqueezeWave, a family of lightweight vocoders based on WaveGlow that can generate audio of similar quality to WaveGlow with 61x - 214x fewer MACs. Code, trained models, and generated audio are publicly available at https://github.com/tianrengao/SqueezeWave.},
   author = {Bohan Zhai and Tianren Gao and Flora Xue and Daniel Rothchild and Bichen Wu and Joseph E. Gonzalez and Kurt Keutzer},
   month = {1},
   title = {SqueezeWave: Extremely Lightweight Vocoders for On-device Speech Synthesis},
   url = {http://arxiv.org/abs/2001.05685},
   year = {2020},
}
@article{Wu2018,
   abstract = {Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too expensive for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets, a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy. Despite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.},
   author = {Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer},
   month = {12},
   title = {FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search},
   url = {http://arxiv.org/abs/1812.03443},
   year = {2018},
}
@article{Bachlechner2020,
   abstract = {Deep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation, we propose ReZero, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10.},
   author = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},
   month = {3},
   title = {ReZero is All You Need: Fast Convergence at Large Depth},
   url = {http://arxiv.org/abs/2003.04887},
   year = {2020},
}
@article{Shen2020,
   abstract = {The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103.},
   author = {Sheng Shen and Zhewei Yao and Amir Gholami and Michael Mahoney and Kurt Keutzer},
   month = {3},
   title = {Rethinking Batch Normalization in Transformers},
   url = {http://arxiv.org/abs/2003.07845},
   year = {2020},
}
@article{Mattson2019,
   abstract = {Machine learning (ML) needs industry-standard performance benchmarks to support design and competitive evaluation of the many emerging software and hardware solutions for ML. But ML training presents three unique benchmarking challenges absent from other domains: optimizations that improve training throughput can increase the time to solution, training is stochastic and time to solution exhibits high variance, and software and hardware systems are so diverse that fair benchmarking with the same binary, code, and even hyperparameters is difficult. We therefore present MLPerf, an ML benchmark that overcomes these challenges. Our analysis quantitatively evaluates MLPerf's efficacy at driving performance and scalability improvements across two rounds of results from multiple vendors.},
   author = {Peter Mattson and Christine Cheng and Cody Coleman and Greg Diamos and Paulius Micikevicius and David Patterson and Hanlin Tang and Gu-Yeon Wei and Peter Bailis and Victor Bittorf and David Brooks and Dehao Chen and Debojyoti Dutta and Udit Gupta and Kim Hazelwood and Andrew Hock and Xinyuan Huang and Atsushi Ike and Bill Jia and Daniel Kang and David Kanter and Naveen Kumar and Jeffery Liao and Guokai Ma and Deepak Narayanan and Tayo Oguntebi and Gennady Pekhimenko and Lillian Pentecost and Vijay Janapa Reddi and Taylor Robie and Tom St. John and Tsuguchika Tabaru and Carole-Jean Wu and Lingjie Xu and Masafumi Yamazaki and Cliff Young and Matei Zaharia},
   month = {10},
   title = {MLPerf Training Benchmark},
   url = {http://arxiv.org/abs/1910.01500},
   year = {2019},
}
@article{Shen2018,
   abstract = {The attention mechanism has seen wide applications in computer vision and natural language processing. Recent works developed the dot-product attention mechanism and applied it to various vision and language tasks. However, the memory and computational costs of dot-product attention grows quadratically with the spatiotemporal size of the input. Such growth prohibits the application of the mechanism on large inputs, e.g., long sequences, high-resolution images, or large videos. To remedy this drawback, this paper proposes a novel efficient attention mechanism, which is equivalent to dot-product attention but has substantially less memory and computational costs. The resource efficiency allows more widespread and flexible incorporation of efficient attention modules into a neural network, which leads to improved accuracies. Empirical evaluations on object recognition and image classification demonstrated the effectiveness of its advantages. Models with efficient attention achieved state-of-the-art performance on MS-COCO 2017 and significant improvement on ImageNet. Further, the resource efficiency of the mechanism democratizes attention to complicated models, which were unable to incorporate original dot-product attention due to prohibitively high costs. As an exemplar, an efficient attention-augmented model achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.},
   author = {Zhuoran Shen and Mingyuan Zhang and Haiyu Zhao and Shuai Yi and Hongsheng Li},
   title = {Efficient Attention: Attention with Linear Complexities},
   year = {2018},
}
@article{Kingma2014,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Ba},
   month = {12},
   title = {Adam: A Method for Stochastic Optimization},
   url = {http://arxiv.org/abs/1412.6980},
   year = {2014},
}
@article{,
   title = {Tiramisu-a polyhedral compiler for expressing fast and portable code},
}
@article{Wu2020,
   abstract = {Computer vision has achieved great success using standardized image representations -- pixel arrays, and the corresponding deep learning operators -- convolutions. In this work, we challenge this paradigm: we instead (a) represent images as a set of visual tokens and (b) apply visual transformers to find relationships between visual semantic concepts. Given an input image, we dynamically extract a set of visual tokens from the image to obtain a compact representation for high-level semantics. We then use visual transformers to operate over the visual tokens to densely model relationships between them. We find that this paradigm of token-based image representation and processing drastically outperforms its convolutional counterparts on image classification and semantic segmentation. To demonstrate the power of this approach on ImageNet classification, we use ResNet as a convenient baseline and use visual transformers to replace the last stage of convolutions. This reduces the stage's MACs by up to 6.9x, while attaining up to 4.53 points higher top-1 accuracy. For semantic segmentation, we use a visual-transformer-based FPN (VT-FPN) module to replace a convolution-based FPN, saving 6.5x fewer MACs while achieving up to 0.35 points higher mIoU on LIP and COCO-stuff.},
   author = {Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Masayoshi Tomizuka and Kurt Keutzer and Peter Vajda},
   month = {6},
   title = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
   url = {http://arxiv.org/abs/2006.03677},
   year = {2020},
}
@article{Ilboudo2020,
   abstract = {Machine learning algorithms aim to find patterns from observations, which may include some noise, especially in robotics domain. To perform well even with such noise, we expect them to be able to detect outliers and discard them when needed. We therefore propose a new stochastic gradient optimization method, whose robustness is directly built in the algorithm, using the robust student-t distribution as its core idea. Adam, the popular optimization method, is modified with our method and the resultant optimizer, so-called TAdam, is shown to effectively outperform Adam in terms of robustness against noise on diverse task, ranging from regression and classification to reinforcement learning problems. The implementation of our algorithm can be found at https://github.com/Mahoumaru/TAdam.git},
   author = {Wendyam Eric Lionel Ilboudo and Taisuke Kobayashi and Kenji Sugimoto},
   title = {TAdam: A Robust Stochastic Gradient Optimizer},
   year = {2020},
}
@article{Wang2020,
   abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
   author = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
   month = {6},
   title = {Linformer: Self-Attention with Linear Complexity},
   url = {http://arxiv.org/abs/2006.04768},
   year = {2020},
}
@misc{Wei2019a,
   author = {Jinliang Wei and Garth A Gibson and Eric P Xing and Phillip B Gibbons and Gregory R Ganger and Vijay Vasudevan and Google Brain},
   title = {Scheduling for Efficient Large-Scale Machine Learning Training},
   year = {2019},
}
@inproceedings{Wei2019,
   abstract = {Machine learning (ML) training is commonly parallelized using data parallelism. A fundamental limitation of data parallelism is that conflicting (concurrent) parameter accesses during ML training usually diminishes or even negates the benefits provided by additional parallel compute resources. Although it is possible to avoid conflicting parameter accesses by carefully scheduling the computation, existing systems rely on programmer manual parallelization and it remains a question when such parallelization is possible. We present Orion, a system that automatically parallelizes serial imperative ML programs on distributed shared memory. The core of Orion is a static dependence analysis mechanism that determines when dependence-preserving parallelization is effective and maps a loop computation to an optimized distributed computation schedule. Our evaluation shows that for a number of ML applications, Orion can parallelize a serial program while preserving critical dependences and thus achieve a significantly faster convergence rate than data-parallel programs and a matching convergence rate and comparable computation throughput to state-of-the-art manual parallelizations including model-parallel programs.},
   author = {Jinliang Wei and Garth A. Gibson and Phillip B. Gibbons and Eric P. Xing},
   doi = {10.1145/3302424.3303954},
   journal = {Proceedings of the 14th EuroSys Conference 2019},
   month = {3},
   publisher = {Association for Computing Machinery, Inc},
   title = {Automating dependence-aware parallelization of machine learning training on distributed shared memory},
   year = {2019},
}
@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   month = {1},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {http://arxiv.org/abs/1701.06538},
   year = {2017},
}
@article{Child2019,
   abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt\{n\})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
   author = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
   title = {Generating Long Sequences with Sparse Transformers},
   year = {2019},
}
@article{Gao2020,
   abstract = {SpGEMM (General Sparse Matrix-Matrix Multiplication) has attracted much attention from researchers in fields of multigrid methods and graph analysis. Many optimization techniques have been developed for certain application fields and computing architecture over the decades. The objective of this paper is to provide a structured and comprehensive overview of the research on SpGEMM. Existing optimization techniques have been grouped into different categories based on their target problems and architectures. Covered topics include SpGEMM applications, size prediction of result matrix, matrix partitioning and load balancing, result accumulating, and target architecture-oriented optimization. The rationales of different algorithms in each category are analyzed, and a wide range of SpGEMM algorithms are summarized. This survey sufficiently reveals the latest progress and research status of SpGEMM optimization from 1977 to 2019. More specifically, an experimentally comparative study of existing implementations on CPU and GPU is presented. Based on our findings, we highlight future research directions and how future studies can leverage our findings to encourage better design and implementation.},
   author = {Jianhua Gao and Weixing Ji and Zhaonian Tan and Yueyan Zhao},
   month = {2},
   title = {A Systematic Survey of General Sparse Matrix-Matrix Multiplication},
   url = {http://arxiv.org/abs/2002.11273},
   year = {2020},
}
@misc{Lia,
   abstract = {Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot-product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L) 2) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real-world datasets show that it compares favorably to the state-of-the-art.},
   author = {Shiyang Li and Xiaoyong Jin and Yao Xuan and Xiyou Zhou and Wenhu Chen and Yu-Xiang Wang and Xifeng Yan},
   title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
}
@misc{Gray,
   abstract = {We're releasing highly optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. The kernels allow for efficient evaluation and differentiation of linear layers, including convolutional layers, with flexibly configurable block-sparsity patterns in the weight matrix. We find that depending on the sparsity, these kernels can run orders of magnitude faster than the best available alternatives such as cuBLAS. Using the kernels we improve upon the state-of-the-art in text sentiment analysis and generative modeling of text and images. By releasing our kernels in the open we aim to spur further advancement in model and algorithm design.},
   author = {Scott Gray and Alec Radford and Diederik P Kingma},
   title = {GPU Kernels for Block-Sparse Weights},
}
@inproceedings{Ousterhout2013,
   abstract = {Large-scale data analytics frameworks are shifting towards shorter task durations and larger degrees of parallelism to provide low latency. Scheduling highly parallel jobs that complete in hundreds of milliseconds poses a major challenge for task schedulers, which will need to schedule millions of tasks per second on appropriate machines while offering millisecond-level latency and high availability. We demonstrate that a decentralized, randomized sampling approach provides near-optimal performance while avoiding the throughput and availability limitations of a centralized design. We implement and deploy our scheduler, Sparrow, on a 110-machine cluster and demonstrate that Sparrow performs within 12% of an ideal scheduler. Â© 2013 ACM.},
   author = {Kay Ousterhout and Patrick Wendell and Matei Zaharia and Ion Stoica},
   doi = {10.1145/2517349.2522716},
   journal = {SOSP 2013 - Proceedings of the 24th ACM Symposium on Operating Systems Principles},
   pages = {69-84},
   title = {Sparrow: Distributed, low latency scheduling},
   year = {2013},
}
@article{Liu2019,
   abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
   author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
   title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
   year = {2019},
}
@misc{Lattner,
   abstract = {This document introduces the LLVM compiler infrastructure and instruction set, a simple approach that enables sophisticated code transformations at link time, runtime, and in the field. It is a pragmatic approach to compilation, interfering with programmers and tools as little as possible, while still retaining extensive high-level information from source-level compilers for later stages of an application's lifetime. We describe the LLVM instruction set, the design of the LLVM system, and some of its key components.},
   author = {Chris Lattner and Vikram Adve},
   title = {The LLVM Instruction Set and Compilation Strategy},
}
@article{Ribeiro2020,
   abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
   author = {Marco Tulio Ribeiro and Tongshuang Wu and Carlos Guestrin and Sameer Singh},
   month = {5},
   title = {Beyond Accuracy: Behavioral Testing of NLP models with CheckList},
   url = {http://arxiv.org/abs/2005.04118},
   year = {2020},
}
@misc{Hamilton,
   abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
   author = {William L Hamilton and Rex Ying and Jure Leskovec},
   title = {Inductive Representation Learning on Large Graphs},
}
@article{Molchanov2019,
   abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation-a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10Ã theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.},
   author = {Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
   issue = {2015},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   pages = {1-17},
   title = {Pruning convolutional neural networks for resource efficient inference},
   year = {2019},
}
@article{Kjolstad2017,
   abstract = {Tensor and linear algebra is pervasive in data analytics and the physical sciences. Often the tensors, matrices or even vectors are sparse. Computing expressions involving a mix of sparse and dense tensors, matrices and vectors re-quires writing kernels for every operation and combination of formats of interest. The number of possibilities is infi-nite, which makes it impossible to write library code for all. This problem cries out for a compiler approach. This paper presents a new technique that compiles compound tensor algebra expressions combined with descriptions of tensor formats into efficient loops. The technique is evaluated in a prototype compiler called taco, demonstrating competitive performance to best-in-class hand-written codes for tensor and matrix operations.},
   author = {Fredrik Kjolstad and Shoaib Kamil and Stephen Chou and David Lugato and Saman Amarasinghe},
   doi = {10.1145/3133901},
   issn = {2475-1421},
   issue = {OOPSLA},
   journal = {Proceedings of the ACM on Programming Languages},
   month = {10},
   pages = {1-29},
   publisher = {Association for Computing Machinery (ACM)},
   title = {The tensor algebra compiler},
   volume = {1},
   year = {2017},
}
@misc{,
   abstract = {We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decou-ples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.},
   title = {FIXING WEIGHT DECAY REGULARIZATION IN ADAM},
}
@article{Wu2016,
   abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.},
   author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Åukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
   doi = {abs/1609.08144},
   issn = {1609.08144},
   pages = {1-23},
   pmid = {18319728},
   title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
   year = {2016},
}
@misc{Nakamoto,
   abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
   author = {Satoshi Nakamoto},
   title = {Bitcoin: A Peer-to-Peer Electronic Cash System},
   url = {www.bitcoin.org},
}
@misc{,
   title = {Descriptions of SHA-256, SHA-384, and SHA-512},
}
@article{SuyogGupta,
   author = {Suyog Gupta},
   title = {Deep Learning with Limited Numerical Precision},
}
@article{Samadi2014,
   author = {Mehrzad Samadi and Davoud Anoushe Jamshidi and Janghaeng Lee and Scott Mahlke},
   doi = {10.1145/2541940.2541948},
   pages = {35-50},
   title = {Paraprox: Pattern-Based Approximation for Data Parallel Applications},
   year = {2014},
}
@article{,
   title = {Tensorflow debugger debugging dataflow graphs for machine learning},
}
@article{,
   title = {Tensorflow Quantum a software framework for quantum machine learning},
}
@article{,
   title = {Tensorflow Lite micro},
}
@article{,
   title = {TinyML for Edge AI},
}
@article{Noh2015,
   author = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},
   doi = {10.1109/ICCV.2015.178},
   pages = {1520-1528},
   title = {A Survey of Auto Driving},
   year = {2015},
}
@article{Zhang2015,
   author = {Chen Zhang and Peng Li and Guangyu Sun and Yijin Guan and Bingjun Xiao and Jason Cong},
   doi = {10.1145/2684746.2689060},
   pages = {161-170},
   title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
   year = {2015},
}
@article{,
   title = {Geometric Deep Learning, Grids, Groups, Graphs, Geodesics and Gauges},
}
@article{,
   title = {The Google File System},
}
@article{,
   title = {Ray: a Distributed Framework for Emerging AI Applications},
}
@article{,
   title = {Logarithmic Arithmetic as an Alternative to Floating Point},
}
@article{,
   title = {Measuring the Effects of Data Parallelism},
}
@article{Barany2014,
   author = {GergÃ¶ Barany},
   doi = {10.1145/2617548.2617552},
   pages = {1-9},
   title = {Using Python for Model Inference},
   year = {2014},
}
@article{Allen1987,
   author = {Randy Allen and Ken Kennedy},
   doi = {10.1145/29873.29875},
   issue = {4},
   journal = {ACM Transactions on Programming Languages and Systems},
   pages = {491-542},
   title = {Compiler Auto Vectorization with Imitation Learning },
   volume = {9},
   year = {1987},
}
@article{,
   title = {Strong Inference},
}
@article{,
   title = {Physics Informed Neural Networks with Hard Constraints for Inverse Design},
}
@article{,
   title = {NVIDIA SIMNET: AN AI-ACCELERATED MULTI-PHYSICS SIMULATION FRAMEWORK},
}
@article{Qiu2020,
   abstract = {We introduce a novel and efficient algorithm called the stochastic approximate gradient descent (SAGD), as an alternative to the stochastic gradient descent for cases where unbiased stochastic gradients cannot be trivially obtained. Traditional methods for such problems rely on general-purpose sampling techniques such as Markov chain Monte Carlo, which typically requires manual intervention for tuning parameters and does not work efficiently in practice. Instead, SAGD makes use of the Langevin algorithm to construct stochastic gradients that are biased in finite steps but accurate asymptotically, enabling us to theoretically establish the convergence guarantee for SAGD. Inspired by our theoretical analysis, we also provide useful guidelines for its practical implementation. Finally, we show that SAGD performs well experimentally in popular statistical and machine learning problems such as the expectation-maximization algorithm and the variational autoencoders.},
   author = {Yixuan Qiu and Xiao Wang},
   doi = {10.1609/aaai.v34i04.5992},
   issn = {23318422},
   journal = {arXiv},
   title = {Stochastic approximate gradient descent via the langevin Algorithm},
   year = {2020},
}
@article{Rajbhandari2020,
   abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: It trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8. 3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.},
   author = {Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
   doi = {10.1109/SC41405.2020.00024},
   isbn = {9781728199986},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   pages = {1-24},
   title = {Zero: Memory optimizations toward training trillion parameter models},
   volume = {2020-Novem},
   year = {2020},
}
@article{Li2021,
   abstract = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},
   author = {Mingzhen Li and Yi Liu and Xiaoyan Liu and Qingxiao Sun and Xin You and Hailong Yang and Zhongzhi Luan and Lin Gan and Guangwen Yang and Depei Qian},
   doi = {10.1109/TPDS.2020.3030548},
   issn = {15582183},
   issue = {3},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   keywords = {Neural networks,compiler,deep learning,intermediate representation,optimization},
   pages = {708-727},
   title = {The Deep Learning Compiler: A Comprehensive Survey},
   volume = {32},
   year = {2021},
}
@book{Rajbhandari2021,
   abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.},
   author = {Samyam Rajbhandari and Olatunji Ruwase and Jeff Rasley and Shaden Smith and Yuxiong He},
   issue = {1},
   journal = {Proceedings of ACM Conference (Conference'17)},
   publisher = {Association for Computing Machinery},
   title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
   volume = {1},
   url = {http://arxiv.org/abs/2104.07857},
   year = {2021},
}
@article{Ren2021,
   abstract = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency. ZeRO-Offload enables large model training by offloading data and compute to CPU. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU, and reduce CPU compute time while maximizing memory savings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single NVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone for a 1.4B parameter model, the largest that can be trained without running out of memory. ZeRO-Offload is also designed to scale on multiple-GPUs when available, offering near linear speedup on up to 128 GPUs. Additionally, it can work together with model parallelism to train models with over 70 billion parameters on a single DGX-2 box, a 4.5x increase in model size compared to using model parallelism alone. By combining compute and memory efficiency with ease-of-use, ZeRO-Offload democratizes large-scale model training making it accessible to even data scientists with access to just a single GPU.},
   author = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
   title = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
   url = {http://arxiv.org/abs/2101.06840},
   year = {2021},
}
@article{Du2021,
   abstract = {There have been various types of pretraining architectures including autoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks are different in nature, with three main categories being classification, unconditional generation, and conditional generation. However, none of the pretraining frameworks performs the best for all tasks, which introduces inconvenience for model development and selection. We propose a novel pretraining framework GLM (General Language Model) to address this challenge. Compared to previous work, our architecture has three major benefits: (1) it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; (2) it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; (3) it naturally handles variable-length blank filling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pre-training data. Moreover, GLM with 1.25x parameters of BERT-Large achieves the best performance in NLU, conditional and unconditional generation at the same time, which demonstrates its generalizability to different downstream tasks.},
   author = {Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and Jiezhong Qiu and Zhilin Yang and Jie Tang},
   title = {All NLP Tasks Are Generation Tasks: A General Pretraining Framework},
   url = {http://arxiv.org/abs/2103.10360},
   year = {2021},
}
@article{Wang2019,
   abstract = {Modern deep learning models have been exploited in various domains, including computer vision (CV), natural language processing (NLP), search and recommendation. In practical AI clusters, workloads training these models are run using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One critical issue for efficiently operating practical AI clouds, is to characterize the computing and data transfer demands of these workloads, and more importantly, the training performance given the underlying software framework and hardware configurations. In this paper, we characterize deep learning training workloads from Platform of Artificial Intelligence (PAI) in Alibaba. We establish an analytical framework to investigate detailed execution time breakdown of various workloads using different training architectures, to identify performance bottleneck. Results show that weight/gradient communication during training takes almost 62% of the total execution time among all our workloads on average. The computation part, involving both GPU computing and memory access, are not the biggest bottleneck based on collective behavior of the workloads. We further evaluate attainable performance of the workloads on various potential software/hardware mappings, and explore implications on software architecture selection and hardware configurations. We identify that 60% of PS/Worker workloads can be potentially sped up when ported to the AllReduce architecture exploiting the high-speed NVLink for GPU interconnect, and on average 1.7X speedup can be achieved when Ethernet bandwidth is upgraded from 25 Gbps to 100 Gbps.},
   author = {Mengdi Wang and Chen Meng and Guoping Long and Chuan Wu and Jun Yang and Wei Lin and Yangqing Jia},
   doi = {10.1109/IISWC47752.2019.9042047},
   isbn = {9781728140452},
   journal = {Proceedings of the 2019 IEEE International Symposium on Workload Characterization, IISWC 2019},
   pages = {189-202},
   title = {Characterizing Deep Learning Training Workloads on Alibaba-PAI},
   year = {2019},
}
@article{Mishra2021,
   abstract = {As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores.},
   author = {Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
   title = {Accelerating Sparse Deep Neural Networks},
   url = {http://arxiv.org/abs/2104.08378},
   year = {2021},
}
@article{Reddi2020,
   abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
   author = {Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},
   doi = {10.1109/ISCA45697.2020.00045},
   issn = {10636897},
   journal = {Proceedings - International Symposium on Computer Architecture},
   keywords = {Benchmarking,Inference,Machine Learning},
   pages = {446-459},
   title = {MLPerf Inference Benchmark},
   volume = {2020-May},
   year = {2020},
}
@article{Brown2020,
   abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
   author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
   issn = {23318422},
   journal = {arXiv},
   title = {Language models are few-shot learners},
   year = {2020},
}
@article{Baldi2013,
   abstract = {Dropout is a relatively new algorithm for training neural networks which relies on stochastically "dropping out" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.},
   author = {Pierre Baldi and Peter Sadowski},
   doi = {10.17744/mehc.25.2.xhyreggxdcd0q4ny},
   issn = {10495258},
   issue = {1},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1-9},
   title = {Understanding Dropout},
   year = {2013},
}
@article{Press2017,
   abstract = {We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
   author = {Ofir Press and Lior Wolf},
   doi = {10.18653/v1/e17-2025},
   isbn = {9781510838604},
   journal = {15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference},
   pages = {157-163},
   title = {Using the output embedding to improve language models},
   volume = {2},
   year = {2017},
}
@article{Fedus2021,
   abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
   author = {William Fedus and Barret Zoph and Noam Shazeer},
   pages = {1-31},
   title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
   url = {http://arxiv.org/abs/2101.03961},
   year = {2021},
}
@article{Tarnawski2020,
   abstract = {Modern machine learning workloads use large models, with complex structures, that are very expensive to execute. The devices that execute complex models are becoming increasingly heterogeneous as we see a flourishing of domain-specific accelerators being offered as hardware accelerators in addition to CPUs. These trends necessitate distributing the workload across multiple devices. Recent work has shown that significant gains can be obtained with model parallelism, i.e, partitioning a neural networkâs computational graph onto multiple devices. In particular, this form of parallelism assumes a pipeline of devices, which is fed a stream of samples and yields high throughput for training and inference of DNNs. However, for such settings (large models and multiple heterogeneous devices), we require automated algorithms and toolchains that can partition the ML workload across devices. In this paper, we identify and isolate the structured optimization problem at the core of device placement of DNN operators, for both inference and training, especially in modern pipelined settings. We then provide algorithms that solve this problem to optimality. We demonstrate the applicability and efficiency of our approaches using several contemporary DNN computation graphs.},
   author = {Jakub Tarnawski and Amar Phanishayee and Nikhil R. Devanur and Divya Mahajan and Fanny Nina Paravecino},
   issn = {23318422},
   journal = {arXiv},
   title = {Efficient Algorithms for Device Placement of DNN Graph Operators},
   year = {2020},
}
@article{Griewank1992,
   abstract = {In its basic form the reverse mode of automatic differentiation yields gradient vectors at a small multiple of the computational work needed to evaluate the underlying scalar function. The practical applicability of this temporal complexity result, due originally to Linnainmaa, seemed to be severely limited by the fact that the memory requirement of the basic implementation is proportional to the run time, T, of the original evaluation program, It is shown here that, by a recursive scheme related to the multilevel differentiation approach of Volin and Ostrovskii, the growth in both temporal and spatial complexity can be limited to a fixed multiple of log(T). Other compromises between the run time and memory requirement are possible, so that the reverse mode becomes applicable to computational problems of virtually any size. Â© 1992, Taylor & Francis Group, LLC. All rights reserved.},
   author = {Andreas Griewank},
   doi = {10.1080/10556789208805505},
   issn = {10294937},
   issue = {1},
   journal = {Optimization Methods and Software},
   keywords = {Adjoint,Checkpointing,Complexity,Gradient,Recursion},
   pages = {35-54},
   title = {Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation},
   volume = {1},
   year = {1992},
}
@article{Le2020,
   abstract = {Deep neural network models are becoming popular and have used in various tasks such as computer vision, speech recognition, and natural language processing. It is often the case that the training phase of a model is executed in one environment, while the inference phase is executed in another environment. This is because the optimization characteristics for each phase significantly differ. Therefore, it is critical to efficiently compile a trained model for inferencing on different environments. To represent neural network models, users often use Open Neural Network Exchange (ONNX) which is an open standard format for machine learning interoperability. We are developing a compiler for rewriting a model in ONNX into a standalone binary that is executable on different target hardwares such as x86 machines, IBM Power Systems, and IBM System Z. The compiler was written using Multi-level Intermediate Representation (MLIR), a modern compiler infrastructure. In particular, we introduce two internal representations: ONNX IR for representing ONNX operators, and Kernel IR for efficiently lowering ONNX operators into LLVM bitcode. In this paper, we will discuss the overall structure of our compiler and give some practical examples of converting ONNX operators and models. We also cover several issues related to endianness. Our framework is publicly available as an open source project under the ONNX project.},
   author = {Tung D. Le and Gheorghe Teodor Bercea and Tong Chen and Alexandre E. Eichenberger and Haruki Imai and Tian Jin and Kiyokuni Kawachiya and Yasushi Negishi and Kevin OâBrien},
   issn = {23318422},
   journal = {arXiv},
   pages = {1-8},
   title = {Compiling ONNX neural network models using MLIR},
   year = {2020},
}
@article{You2019,
   abstract = {Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains RESNET on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and RESNET-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available online1.},
   author = {Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho Jui Hsieh},
   issn = {23318422},
   journal = {arXiv},
   title = {Large batch optimization for deep learning: Training bert in 76 minutes},
   year = {2019},
}
@article{Goto2008,
   abstract = {We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance. Â© 2008 ACM.},
   author = {Kazushige Goto and Robert A. Van De Geijn},
   doi = {10.1145/1356052.1356053},
   issn = {00983500},
   issue = {3},
   journal = {ACM Transactions on Mathematical Software},
   keywords = {Basic linear algebra subprogrms,Linear algebra,Matrix multiplication},
   pages = {1-25},
   title = {Anatomy of high-performance matrix multiplication},
   volume = {34},
   year = {2008},
}
@article{He2021,
   abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods â covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) â with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithmsâ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
   author = {Xin He and Kaiyong Zhao and Xiaowen Chu},
   doi = {10.1016/j.knosys.2020.106622},
   issn = {09507051},
   issue = {Dl},
   journal = {Knowledge-Based Systems},
   keywords = {Automated machine learning (autoML),Deep learning,Hyperparameter optimization (HPO),Neural architecture search (NAS)},
   title = {AutoML: A survey of the state-of-the-art},
   volume = {212},
   year = {2021},
}
@article{Chen2016,
   abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
   author = {Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
   pages = {1-12},
   title = {Training Deep Nets with Sublinear Memory Cost},
   url = {http://arxiv.org/abs/1604.06174},
   year = {2016},
}
@article{Keskar2017,
   abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
   author = {Nitish Shirish Keskar and Jorge Nocedal and Ping Tak Peter Tang and Dheevatsa Mudigere and Mikhail Smelyanskiy},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   pages = {1-16},
   title = {On large-batch training for deep learning: Generalization gap and sharp minima},
   year = {2017},
}
@article{Hill2020,
   abstract = {Recent work has shown that large text-based neural language models, trained with conventional supervised learning objectives, acquire a surprising propensity for few- and one-shot learning. Here, we show that an embodied agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional reinforcement learning algorithms. After a single introduction to a novel object via continuous visual perception and a language prompt ("This is a dax"), the agent can re-identify the object and manipulate it as instructed ("Put the dax on the bed"). In doing so, it seamlessly integrates short-term, within-episode knowledge of the appropriate referent for the word "dax" with long-term lexical and motor knowledge acquired across episodes (i.e. "bed" and "putting"). We find that, under certain training conditions and with a particular memory writing mechanism, the agentâs one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful for later executing instructions. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for fast-mapping, a fundamental pillar of human cognitive development and a potentially transformative capacity for agents that interact with human users.},
   author = {Felix Hill and Olivier Tieleman and Tamara von Glehn and Nathaniel Wong and Hamza Merzic and Stephen Clark},
   issn = {23318422},
   journal = {arXiv},
   pages = {1-17},
   title = {Grounded language learning fast and slow},
   year = {2020},
}
@article{Jia2018,
   abstract = {The past few years have witnessed growth in the computational requirements for training deep convolutional neural networks. Current approaches parallelize training onto multiple devices by applying a single parallelization strategy (e.g., data or model parallelism) to all layers in a network. Although easy to reason about, these approaches result in suboptimal runtime performance in largescale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our evaluation shows that layer-wise parallelism outperforms state-of-the-art approaches by increasing training throughput, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining original network accuracy.},
   author = {Zhihao Jia and Sina Lin and Charles R. Qi and Alex Aiken},
   isbn = {9781510867963},
   journal = {35th International Conference on Machine Learning, ICML 2018},
   pages = {3560-3571},
   title = {Exploring hidden dimensions in parallelizing convolutional neural networks},
   volume = {5},
   year = {2018},
}
@article{DauphinYannN.2021,
   abstract = {Batch normalization (BatchNorm) has become a standard technique in deep learn- ing. Its popularity is in no small part due to its often positive effect on generalization. Despite this success, the regularization effect of the technique is still poorly under- stood. This study aims to decompose BatchNorm into separate mechanisms that are much simpler. We identify three effects of BatchNorm and assess their impact directly with ablations and interventions. Our experiments show that preventing explosive growth at the final layer at initialization and during training can recover a large part of BatchNormâs generalization boost. This regularization mechanism can lift accuracy by 2.9% for Resnet-50 on Imagenet without BatchNorm. We show it is linked to other methods like Dropout and recent initializations like Fixup. Surprisingly, this simple mechanism matches the improvement of 0.9% of the more complex Dropout regularization for the state-of-the-art Efficientnet-B8 model on Imagenet. This demonstrates the underrated effectiveness of simple regularizations and sheds light on directions to further improve generalization for deep nets.},
   author = {Ekin D. Cubuk Dauphin, Yann N.},
   pages = {1-11},
   title = {Deconstructing the Regularization of Batch-Norm},
   year = {2021},
}
@article{Jia2018,
   abstract = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
   author = {Zhe Jia and Marco Maggioni and Benjamin Staiger and Daniele P. Scarpazza},
   title = {Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking},
   url = {http://arxiv.org/abs/1804.06826},
   year = {2018},
}
@article{Infrastructure2020,
   author = {A I Infrastructure},
   issue = {July},
   title = {NVIDIA DGX A100 System Architecture},
   year = {2020},
}
@article{Raghavan2014,
   author = {Gopal Raghavan},
   pages = {1-8},
   title = {5 Emerging Dram Interfaces You Should Know for Your Next Design},
   volume = {5279},
   year = {2014},
}
@article{,
   author = {On-node Switch},
   keywords = {NVSwitch Technology
NVLink Fabric
GPU Technology},
   title = {NVIDIA NVSwitch: The World's Highest-Bandwidth On-Node Switch},
}
@article{Spirkl2019,
   author = {Wolfgang Spirkl},
   title = {GDDR Memory Enabling AI and High Performance Compute âª The Demand for faster Memory and storage},
   year = {2019},
}
@article{Xilinx2015,
   abstract = {This white paper describes how DDR memory technology is evolving into serial memory schemes like Hybrid Memory Cube (HMC) and others still in the development pipeline. XilinxÂ® UltraScaleâ¢ devices support them all.},
   author = {Xilinx},
   keywords = {Bandwidth Engine,DDR3,DDR4,HBM,HMC,LPDDR,MoSys,serial memory,wp456},
   pages = {1-9},
   title = {The Rise of Serial Memory and the Future of DDR},
   volume = {456},
   url = {http://www.xilinx.com/support/documentation/white_papers/wp456-DDR-serial-mem.pdf},
   year = {2015},
}
@article{,
   author = {Denis Foley Alex Ishii},
   journal = {Hc20(2018)},
   title = {NVSwitch and DGX-2 â NVIDIAâs NVLink-Switching Chip and Scale-Up GPU-Compute Server},
   url = {https://www.hotchips.org/hc30/2conf/2.01_Nvidia_NVswitch_HotChips2018_DGX2NVS_Final.pdf},
}
@article{Nvidia2020,
   abstract = {The new NVIDIAÂ® A100 Tensor Core GPU builds upon the capabilities of the prior NVIDIA Tesla V100 GPU, adding many new features while delivering significantly faster performance for HPC, AI, and data analytics workloads. Powered by the NVIDIA Ampere architecture-based GA100 GPU, the A100 provides very strong scaling for GPU compute and deep learning applications running in single- and multi-GPU workstations, servers, clusters, cloud data centers, systems at the edge, and supercomputers. The A100 GPU enables building elastic, versatile, and high throughput data centers.},
   author = {Nvidia},
   journal = {Data Sheet},
   pages = {20-21},
   title = {NVIDIA A100 Tensor Core GPU},
   year = {2020},
}
@article{Godse2019,
   abstract = {Artificial intelligence (AI), specifically Deep Learning (DL) techniques are used for real-time analytics, fraud detection, autonomous driving, and speech recognition etc. These power and data hungry DL applications on cloud and at edge has increased Deep Neural Network (DNN) complexity. Multi-tiered Compute, Memory and Storage arrangements can help push AI applications by providing faster access to high volume of data and optimizing cost. AI memory needs are quite different from traditional workloads, requiring faster access to data. DRAM manufacturers struggle with challenges like density growth, cost and bit errors. High Bandwidth Memory (HBM) and GDDR help achieve almost real time access to the memory. Each of these memories have range of system trade-offs such as density, power efficiency and bandwidth. Unlike traditional memory, Persistent memory like MRAM, Phase change memory (PCM), Resistive RAM (ReRAM), Carbon Nanotube RAM (NRAM) etc. provide non-volatility. Persistent memory has a potential to reduce the latency and cost gap between DRAM and Storage. Persistent Memory is a promising technology for driving AI but face challenges of cost, scaling and reliability. Bigger the training data set, better the inference drawn by DNN. This comes with a huge storage demand. With increase in layer count of 3D NAND and innovations in circuit design and process technology, flash enables multi-bit TLC and QLC densities. PCIe bus with SSD provides low latency and high throughput, making flash the most optimal solution for AI storage. High aspect ratio channel etch, staircase contacts, defect control etc. are some of the challenges with upcoming flash generations.},
   author = {Ranjana Godse and Adam McPadden and Vipin Patel and Jung Yoon},
   doi = {10.1109/NANOTECH.2018.8653569},
   journal = {2018 IEEE Nanotechnology Symposium, ANTS 2018},
   keywords = {Artificial Intelligence,flash,latency,memory,storage,throughput},
   pages = {2018-2021},
   publisher = {IEEE},
   title = {Memory Technology enabling the next Artificial Intelligence revolution},
   year = {2019},
}
@article{Paper2013,
   author = {White Paper},
   title = {Which DDR SDRAM Memory to Use and When},
   year = {2013},
}
@article{Li2020,
   abstract = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
   author = {Ang Li and Shuaiwen Leon Song and Jieyang Chen and Jiajia Li and Xu Liu and Nathan R. Tallent and Kevin J. Barker},
   doi = {10.1109/TPDS.2019.2928289},
   issn = {15582183},
   issue = {1},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   keywords = {GPU,GPUDirect,NCCL,NUMA,NVLink,NVSwitch,PCIe,Performance evaluation,RDMA,SLI,interconnect},
   pages = {94-110},
   title = {Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect},
   volume = {31},
   year = {2020},
}
@article{Jun2017,
   abstract = {HBM (High Bandwidth Memory) is an emerging standard DRAM solution that can achieve breakthrough bandwidth of higher than 256GBps while reducing the power consumption as well. It has stacked DRAM architecture with core DRAM dies on top of a base logic die, based on the TSV and die stacking technologies. In this paper, the HBM architecture is introduced and a comparison of its generations is provided. Also, the packaging technology and challenges to address reliability, thermal dissipation capability, maximum allowable package sizes, and high throughput stacking solutions are described. Test technology and testability features are discussed for KGSD and 2.5D SiP.},
   author = {Hongshin Jun and Jinhee Cho and Kangseol Lee and Ho Young Son and Kwiwook Kim and Hanho Jin and Keith Kim},
   doi = {10.1109/IMW.2017.7939084},
   journal = {2017 IEEE 9th International Memory Workshop, IMW 2017},
   keywords = {2.5D SiP,3D IC,HNM,High bandwidth DRAM,Low power DRAM 1500,Micro-bump,Stacked DRAM,TSV},
   pages = {3-6},
   publisher = {IEEE},
   title = {HBM (High bandwidth memory) DRAM technology and architecture},
   year = {2017},
}
@article{Liu2015,
   abstract = {Spatial architectures are more efficient than traditional Out-of-Order (OOO) processors for computationally intensive programs. However, spatial architectures require mapping a program, either statically or dynamically, onto the spatial fabric. Static methods can generate efficient mappings, but they cannot adapt to changing workloads and are not compatible across hardware generations. Current dynamic methods are adaptive and compatible, but do not optimize as well due to their limited use of speculation and small mapping scopes. To overcome the limitations of existing dynamic mapping methods for spatial architectures, while minimizing the inefficiencies inherent in OOO superscalar processors, this paper presents DynaSpAM (Dynamic Spatial Architecture Mapping), a framework that tightly couples a spatial fabric with an OOO pipeline. DynaSpAM coaxes the OOO processor into producing an optimized mapping with a simple modification to the processor's scheduler. The insight behind DynaSpAM is that today's powerful OOO processors do for themselves most of the work necessary to produce a highly optimized mapping for a spatial architecture, including aggressively speculating control and memory dependences, and scheduling instructions using a large window. Evaluation of DynaSpAM shows a geomean speedup of 1.42x for 11 benchmarks from the Rodinia benchmark suite with a geomean 23.9% reduction in energy consumption compared to an 8-issue OOO pipeline.},
   author = {Feng Liu and Heejin Ahn and Stephen R. Beard and Taewook Oh and David I. August},
   doi = {10.1145/2749469.2750414},
   isbn = {9781450334020},
   issn = {10636897},
   journal = {Proceedings - International Symposium on Computer Architecture},
   pages = {541-553},
   title = {DynaSpAM: Dynamic spatial architecture mapping using out of order instruction schedules},
   volume = {13-17-June},
   year = {2015},
}
@article{Keutzer2014,
   author = {Kurt Keutzer},
   doi = {10.1007/0-306-47823-4},
   isbn = {0306478234},
   issue = {January 2004},
   title = {Improving Performance through Microarchitecture},
   year = {2014},
}
@article{Balasubramonian2005,
   abstract = {Future high-performance billion-transistor processors are likely to employ partitioned architectures to achieve high clock speeds, high parallelism, low design complexity, and low power. In such architectures, inter-partition communication over global wires has a significant impact on overall processor performance and power consumption. VLSI techniques allow a variety of wire implementations, but these wire properties have previously never been exposed to the microarchitecture. This paper advocates global wire management at the microarchitecture level and proposes a heterogeneous interconnect that is comprised of wires with varying latency, bandwidth, and energy characteristics. We propose and evaluate microarchitectural techniques that can exploit such a heterogeneous interconnect to improve performance and reduce energy consumption. These techniques include a novel cache pipeline design, the identification of narrow bit-width operands, the classification of non-critical data, and the detection of interconnect load imbalance. For a dynamically scheduled partitioned architecture, our results demonstrate that the proposed innovations result in up to II% reductions in overall processor ED 2, compared to a baseline processor that employs a homogeneous interconnect. Â© 2005 IEEE.},
   author = {Rajeev Balasubramonian and Naveen Muralimanohar and Karthik Ramani and Venkatanand Venkatachalapathy},
   doi = {10.1109/HPCA.2005.21},
   isbn = {0769522750},
   issn = {15300897},
   journal = {Proceedings - International Symposium on High-Performance Computer Architecture},
   pages = {28-39},
   title = {Microarchitectural wire management for performance and power in partitioned architectures},
   year = {2005},
}
@article{,
   author = {John Black and Phillip Rogaway and C B C Mac},
   pages = {1-3},
   title = {Comments to NIST concerning AES Modes of Operations : A Suggestion for Handling Arbitrary-Length Messages with the CBC MAC 1 Introduction},
}
@article{Hough2006,
   abstract = {We present the design, implementation, and eval- uation of a circuit we call the Statistics Module that captures cycle-accurate performance data at (or above) the microarchitecture layer. The circuit is deployed introspectivelyâin the architecture itselfâ using an FPGA in the context of a soft-core imple- mentation of a SPARC architecture (LEON). Accessible over the Internet, the circuit can be dynamically configured (without resynthesis) to capture program-level, function-level, and instruction-level statistics on any subset of predefined VHDL signals. The circuit is deployed outside the actual soft core, so that its operation does not interfere with a programâs execution at any level. In contrast with simulations, StatsMod monitors actual real-time program executions, including runtime artifacts such as multithreading, operating system support, and external interrupts. Furthermore, unlike software-introduced instrumentation, the measurements do not affect the statistics, and microarchitecture characteristics are easily captured. Our design avoids the otherwise combinatorial size of circuitry that would be required to accommodate all methods and events, scaling well with the number of artifacts that are actually measured. We have used this circuit to measure cycle-accurate cache-RAM statistics, such as cache hits and misses, RAM reads and writes, using both write-through and write-back policies. In this paper, we show the scalabilty of our design as it accommodates more methods and events.},
   author = {Richard Hough and Phillip Jones and Scott Friedman and Roger Chamberlain and Jason Fritts and John Lockwood and Ron Cytron},
   journal = {Workshop on Introspective Architecture},
   title = {Cycle-Accurate Microarchitecture Performance Evaluation â},
   year = {2006},
}
@article{,
   author = {Varad Deshmukh and Nishchay Mhatre and SK Karandikar},
   journal = {Imap.Hipc.Org},
   pages = {1-5},
   title = {Techniques for Benchmarking of CPU Micro-Architecture for Performance Evaluation},
   url = {http://imap.hipc.org/hipc2011/studsym-papers/1569512125.pdf},
}
@article{Moshovos2001,
   abstract = {Semiconductor technology scaling provides faster and more plentiful transistors to build microprocessors, and applications continue to drive the demand for more powerful microprocessors. Weaving the raw semiconductor material into a microprocessor that offers the performance needed by modern and future applications is the role of computer architecture. This paper overviews some of the microarchitectural techniques that empower modern high-performance microprocessors. The techniques are classified Into: 1) techniques meant to increase the concurrency in instruction processing, while maintaining the appearance of sequential processing and 2) techniques that exploit program behavior. The first category includes pipelining, superscalar execution, out-of-order execution, register renaming, and techniques to overlap memory-accessing instructions. The second category includes memory hierarchies, branch predictors, trace caches, and memory-dependence predictors. The paper also discusses microarchitectural, techniques likely to be used in future microprocessors, including data value speculation and instruction reuse, microarchitectures with multiple sequencers and thread-level speculation, and microarchitectural techniques for tackling the problems of power consumption and reliability. copy; 2001 IEEE Publisher Item Identifier S 0018-9219(01)09682-7.},
   author = {Andreas Moshovos and Gurindar S. Sohi},
   doi = {10.1109/5.964438},
   issn = {00189219},
   issue = {11},
   journal = {Proceedings of the IEEE},
   keywords = {Branch prediction,High-performance microprocessors,Memory dependence speculation,Microarchitecture,Out-oforder execution,Speculative execution,Thread-level speculation},
   pages = {1560-1575},
   title = {Microarchitectural innovations: Boosting microprocessor performance beyond semiconductor technology scaling},
   volume = {89},
   year = {2001},
}
@article{Pentecost2019,
   abstract = {Performance analysis and optimization are essential tasks for hardware and software engineers. In the age of datacenter-scale computing, it is particularly important to conduct comparative performance analysis to understand discrepancies and limitations among different hardware systems and applications. However, there is a distinct lack of productive visualization tools for these comparisons. We present CHAMPVis, a web-based, interactive visualization tool that leverages the hierarchical organization of hardware systems to enable productive performance analysis. With CHAMPVis, users can make definitive performance comparisons across applications or hardware platforms. In addition, CHAMPVis provides methods to rank and cluster based on performance metrics to identify common optimization opportunities. Our thorough task analysis reveals three types of datacenter-scale performance analysis tasks: summarization, detailed comparative analysis, and interactive performance bottleneck identification. We propose techniques for each class of tasks including (1) 1-D feature space projection for similarity analysis; (2) Hierarchical parallel co-ordinates for comparative analysis; and (3) User interactions for rapid diagnostic queries to identify optimization targets. We evaluate CHAMPVis by analyzing standard datacenter applications and machine learning benchmarks in two different case studies.},
   author = {Lillian Pentecost and Udit Gupta and Elisa Ngan and Johanna Beyer and Gu Yeon Wei and David Brooks and Michael Behrisch},
   doi = {10.1109/ProTools49597.2019.00013},
   isbn = {9781728160269},
   journal = {Proceedings of ProTools 2019: Workshop on Programming and Performance Visualization Tools - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis},
   pages = {55-61},
   title = {CHAMPVis: Comparative hierarchical analysis of microarchitectural performance},
   year = {2019},
}
@article{Dukhan2020,
   abstract = {Neural network frameworks today commonly implement Deconvolution and closely related Convolution operator via a combination of GEMM (dense matrix-matrix multiplication) and a memory transformation. The recently proposed Indirect Convolution algorithm suggests a more efficient implementation of Convolution via the Indirect GEMM primitive - a modification of GEMM where pointers to rows are loaded from a buffer rather than being computed assuming constant stride. However, the algorithm is inefficient for Deconvolution with non-unit stride, which is typical in computer vision models. We describe a novel Indirect Deconvolution algorithm for efficient evaluation of the Deconvolution operator with nonunit stride by splitting Deconvolution with a large kernel into multiple subconvolutions with smaller, variable-size kernels, which can be efficiently implemented on top of the Indirect GEMM primitive.},
   author = {Marat Dukhan},
   doi = {10.1109/IPDPSW50202.2020.00154},
   isbn = {9781728174457},
   journal = {Proceedings - 2020 IEEE 34th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2020},
   keywords = {Deconvolution,Segmentation,Transposed convolution},
   pages = {922-926},
   title = {Indirect deconvolution algorithm},
   year = {2020},
}
@article{Chellapilla2006,
   abstract = {Convolutional neural networks (CNNs) are well known for producing\nstate-of-the-art recognizers for document processing [1]. However,\nthey can be difficult to implement and are usually slower than traditional\nmulti-layer perceptrons (MLPs). We present three novel approaches\nto speeding up CNNs: a) unrolling convolution, b) using BLAS (basic\nlinear algebra subroutines), and c) using GPUs (graphic processing\nunits). Unrolled convolution converts the processing in each convolutional\nlayer (both forward-propagation and back-propagation) into a matrix-matrix\nproduct. The matrix-matrix product representation of CNNs makes their\nimplementation as easy as MLPs. BLAS is used to efficiently compute\nmatrix products on the CPU. We also present a pixel shader based\nGPU implementation of CNNs. Results on character recognition problems\nindicate that unrolled convolution with BLAS produces a dramatic\n2.4X-3.0X speedup. The GPU implementation is even faster and produces\na 3.1X-4.1X speedup.},
   author = {Kumar Chellapilla and Sidd Puri and Patrice Simard and Kumar Chellapilla and Sidd Puri and Patrice Simard and High Performance and Convolutional Neural and Patrice Simard},
   keywords = {blas,convolutional neural networks},
   title = {High Performance Convolutional Neural Networks for Document Processing To cite this version : High Performance Convolutional Neural Networks for Document Processing},
   year = {2006},
}
@article{Kalamkar2020,
   abstract = {During the last two years, the goal of many researchers has been to squeeze the last bit of performance out of HPC system for AI tasks. Often this discussion is held in the context of how fast ResNet50 can be trained. Unfortunately, ResNet50 is no longer a representative workload in 2020. Thus, we focus on Recommender Systems which account for most of the AI cycles in cloud computing centers. More specifically, we focus on Facebook's DLRM benchmark. By enabling it to run on latest CPU hardware and software tailored for HPC, we are able to achieve up to two-orders of magnitude improvement in performance on a single socket compared to the reference CPU implementation, and high scaling efficiency up to 64 sockets, while fitting ultra-large datasets which cannot be held in single node's memory. Therefore, this paper discusses and analyzes novel optimization and parallelization techniques for the various operators in DLRM. Several optimizations (e.g. tensor-contraction accelerated MLPs, framework MPI progression, BFLOAT16 training with up to ;1.8 Ã speed-up) are general and transferable to many other deep learning topologies.},
   author = {Dhiraj Kalamkar and Evangelos Georganas and Sudarshan Srinivasan and Jianping Chen and Mikhail Shiryaev and Alexander Heinecke},
   doi = {10.1109/SC41405.2020.00047},
   isbn = {9781728199986},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   title = {Optimizing deep learning recommender systems training on CPU cluster architectures},
   volume = {2020-Novem},
   year = {2020},
}
@article{Shi2020,
   abstract = {Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.},
   author = {Hao Jun Michael Shi and Dheevatsa Mudigere and Maxim Naumov and Jiyan Yang},
   doi = {10.1145/3394486.3403059},
   isbn = {9781450379984},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {embeddings,model compression,recommendation systems},
   pages = {165-175},
   title = {Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems},
   year = {2020},
}
@article{Naumov2019,
   abstract = {In this note we discuss a common misconception, namely that embeddings are always used to reduce the dimensionality of the item space. We show that when we measure dimensionality in terms of information entropy then the embedding of sparse probability distributions, that can be used to represent sparse features or data, may or not reduce the dimensionality of the item space. However, the embeddings do provide a different and often more meaningful representation of the items for a particular task at hand. Also, we give upper bounds and more precise guidelines for choosing the embedding dimension.},
   author = {Maxim Naumov},
   pages = {1-8},
   title = {On the Dimensionality of Embeddings for Sparse Features and Data},
   url = {http://arxiv.org/abs/1901.02103},
   year = {2019},
}
@book{Mudigere2021,
   abstract = {Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable software stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, optimal topology and efficient transport (ii) implementing an optimized PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support optimizers with fully deterministic updates (v) leveraging reduced precision communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.},
   author = {Dheevatsa Mudigere and Yuchen Hao and Jianyu Huang and Andrew Tulloch and Srinivas Sridharan and Xing Liu and Mustafa Ozdal and Jade Nie and Jongsoo Park and Liang Luo and Jie Amy Yang and Leon Gao and Dmytro Ivchenko and Aarti Basant and Yuxi Hu and Jiyan Yang and Ehsan K. Ardestani and Xiaodong Wang and Rakesh Komuravelli and Ching-Hsiang Chu and Serhat Yilmaz and Huayu Li and Jiyuan Qian and Zhuobo Feng and Yinbin Ma and Junjie Yang and Ellie Wen and Hong Li and Lin Yang and Chonglin Sun and Whitney Zhao and Dimitry Melts and Krishna Dhulipala and KR Kishore and Tyler Graf and Assaf Eisenman and Kiran Kumar Matam and Adi Gangidi and Guoqiang Jerry Chen and Manoj Krishnan and Avinash Nayak and Krishnakumar Nair and Bharath Muthiah and Mahmoud khorashadi and Pallab Bhattacharya and Petr Lapukhov and Maxim Naumov and Lin Qiao and Mikhail Smelyanskiy and Bill Jia and Vijay Rao},
   issue = {1},
   journal = {Proceedings of ACM Conference (Conference'17)},
   publisher = {Association for Computing Machinery},
   title = {High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models},
   volume = {1},
   url = {http://arxiv.org/abs/2104.05158},
   year = {2021},
}
@article{,
   abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
   author = {Tal Ben-Nun and Torsten Hoefler},
   doi = {10.1145/3320060},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys},
   keywords = {Deep learning,Distributed computing,Parallel algorithms},
   title = {Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
   volume = {52},
   year = {2019},
}
@article{Leszczynski2020,
   abstract = {Many industrial machine learning (ML) systems require frequent retraining to keep up-to-date with constantly changing data. This retraining exacerbates a large challenge facing ML systems today: model training is unstable, i.e., small changes in training data can cause significant changes in the model's predictions. In this paper, we work on developing a deeper understanding of this instability, with a focus on how a core building block of modern natural language processing (NLP) pipelines---pre-trained word embeddings---affects the instability of downstream NLP models. We first empirically reveal a tradeoff between stability and memory: increasing the embedding memory 2x can reduce the disagreement in predictions due to small changes in training data by 5% to 37% (relative). To theoretically explain this tradeoff, we introduce a new measure of embedding instability---the eigenspace instability measure---which we prove bounds the disagreement in downstream predictions introduced by the change in word embeddings. Practically, we show that the eigenspace instability measure can be a cost-effective way to choose embedding parameters to minimize instability without training downstream models, outperforming other embedding distance measures and performing competitively with a nearest neighbor-based measure. Finally, we demonstrate that the observed stability-memory tradeoffs extend to other types of embeddings as well, including knowledge graph and contextual word embeddings.},
   author = {Megan Leszczynski and Avner May and Jian Zhang and Sen Wu and Christopher R. Aberger and Christopher RÃ©},
   title = {Understanding the Downstream Instability of Word Embeddings},
   url = {http://arxiv.org/abs/2003.04983},
   year = {2020},
}
@article{Golovin2017,
   abstract = {Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Googleâs Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.},
   author = {Daniel Golovin and Benjamin Solnik and Subhodeep Moitra and Greg Kochanski and John Karro and D. Sculley},
   doi = {10.1145/3097983.3098043},
   isbn = {9781450348874},
   keywords = {automated stopping,bayesian optimization,black-box optimization,cesses,gaussian pro-,hyperparameters,transfer learning},
   pages = {1487-1495},
   title = {Google Vizier: A Service for Black-Box Optimization},
   year = {2017},
}
@article{Rotem2018,
   abstract = {This paper presents the design of Glow, a machine learning compiler for heterogeneous hardware. It is a pragmatic approach to compilation that enables the generation of highly optimized code for multiple targets. Glow lowers the traditional neural network dataflow graph into a two-phase strongly-typed intermediate representation. The high-level intermediate representation allows the optimizer to perform domain-specific optimizations. The lower-level instruction-based address-only intermediate representation allows the compiler to perform memory-related optimizations, such as instruction scheduling, static memory allocation and copy elimination. At the lowest level, the optimizer performs machine-specific code generation to take advantage of specialized hardware features. Glow features a lowering phase which enables the compiler to support a high number of input operators as well as a large number of hardware targets by eliminating the need to implement all operators on all targets. The lowering phase is designed to reduce the input space and allow new hardware backends to focus on a small number of linear algebra primitives.},
   author = {Nadav Rotem and Jordan Fix and Saleem Abdulrasool and Garret Catron and Summer Deng and Roman Dzhabarov and Nick Gibson and James Hegeman and Meghan Lele and Roman Levenstein and Jack Montgomery and Bert Maher and Satish Nadathur and Jakob Olesen and Jongsoo Park and Artem Rakhov and Misha Smelyanskiy and Man Wang},
   title = {Glow: Graph Lowering Compiler Techniques for Neural Networks},
   url = {http://arxiv.org/abs/1805.00907},
   year = {2018},
}
@article{Jeon2019,
   abstract = {With widespread advances in machine learning, a number of large enterprises are beginning to incorporate machine learning models across a number of products. These models are typically trained on shared, multi-tenant GPU clusters. Similar to existing cluster computing workloads, scheduling frameworks aim to provide features like high efficiency, resource isolation, fair sharing across users, etc. However Deep Neural Network (DNN) based workloads, predominantly trained on GPUs, differ in two significant ways from traditional big data analytics workloads. First, from a cluster utilization perspective, GPUs represent a monolithic resource that cannot be shared at a fine granularity across users. Second, from a workload perspective, deep learning frameworks require gang scheduling reducing the flexibility of scheduling and making the jobs themselves inelastic to failures at runtime. In this paper we present a detailed workload characterization of a two-month long trace from a multi-tenant GPU cluster in Microsoft. By correlating scheduler logs with logs from individual jobs, we study three distinct issues that affect cluster utilization for DNN training workloads on multi-tenant clusters: (1) the effect of gang scheduling and locality constraints on queuing, (2) the effect of locality on GPU utilization, and (3) failures during training. Based on our experience running a large-scale operation, we provide design guidelines pertaining to next-generation cluster schedulers for DNN training workloads.},
   author = {Myeongjae Jeon and Shivaram Venkataraman and Amar Phanishayee and Junjie Qian and Wencong Xiao and Fan Yang},
   isbn = {9781939133038},
   journal = {Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019},
   pages = {947-960},
   title = {Analysis of large-scale multi-tenant GPU clusters for DNN training workloads},
   year = {2019},
}
@article{Li2016,
   abstract = {Despite its more than 100-year history in experimental and clinical use, photodynamic therapy (PDT) is only starting to be appreciated for its full potential. PDT combines a photosensitizer (PS) and light in the presence of oxygen to treat cancer and other disorders. This manuscript reviews molecular mechanisms that have been evaluated over the past years for the effects of PDT at the cellular level as well as in therapeutic settings in vivo. The availability of multiple PS with different structures and functional properties makes PDT an extremely versatile and, conversely, a challenging approach to cancer therapy. The advancing understanding of molecular pathways helps to design improved regimens. As most cancers are being treated with combination therapies, PDT is being integrated into rationally designed combined regimens that exploit molecular responses to PDT for improved efficacy.},
   author = {Mu Li},
   issue = {February},
   pages = {1-81},
   title = {Scaling Distributed Machine Learning with System and Algorithm Co-design},
   year = {2016},
}
@article{Goyal2017,
   abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
   author = {Priya Goyal and Piotr DollÃ¡r and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
   title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
   url = {http://arxiv.org/abs/1706.02677},
   year = {2017},
}
@article{Coleman2019,
   abstract = {Researchers have proposed hardware, software, and algorithmic optimizations to improve the computational performance of deep learning. While some of these optimizations perform the same operations faster (e.g., increasing GPU clock speed), many others modify the semantics of the training procedure (e.g., reduced precision), and can impact the final model's accuracy on unseen data. Due to a lack of standard evaluation criteria that considers these trade-offs, it is difficult to directly compare these optimizations. To address this problem, we recently introduced DAWNBENCH, a benchmark competition focused on end-to-end training time to achieve near-state-of-the-art accuracy on an unseen dataset-a combined metric called time-to-accuracy (TTA). In this work, we analyze the entries from DAWNBENCH, which received optimized submissions from multiple industrial groups, to investigate the behavior of TTA as a metric as well as trends in the best-performing entries. We show that TTA has a low coefficient of variation and that models optimized for TTA generalize nearly as well as those trained using standard methods. Additionally, even though DAWNBENCH entries were able to train ImageNet models in under 3 minutes, we find they still underutilize hardware capabilities such as Tensor Cores. Furthermore, we find that distributed entries can spend more than half of their time on communication. We show similar findings with entries to the MLPERF v0.5 benchmark.},
   author = {Cody Coleman and Daniel Kang and Deepak Narayanan and Luigi Nardi and Tian Zhao and Jian Zhang and Peter Bailis and Kunle Olukotun and Chris RÃ© and Matei Zaharia},
   doi = {10.1145/3352020.3352024},
   issn = {01635980},
   issue = {1},
   journal = {Operating Systems Review (ACM)},
   pages = {14-25},
   title = {Analysis of dawnbench, a time-to-accuracy machine learning performance benchmark},
   volume = {53},
   year = {2019},
}
@article{Wu2021,
   abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-The-Art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-Temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
   author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
   doi = {10.1109/TNNLS.2020.2978386},
   issn = {21622388},
   issue = {1},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Deep learning,graph autoencoder (GAE),graph convolutional networks (GCNs),graph neural networks (GNNs),graph representation learning,network embedding},
   pages = {4-24},
   pmid = {32217482},
   title = {A Comprehensive Survey on Graph Neural Networks},
   volume = {32},
   year = {2021},
}
@article{Chen2019,
   abstract = {Data, models, and computing are the three pillars that enable machine learning to solve real-world problems at scale. Making progress on these three domains requires not only disruptive algorithmic advances but also systems innovations that can continue to squeeze more efficiency out of modern hardware. Learning systems are in the center of every intelligent application nowadays. This thesis discusses aspects of learning systems under the context of three real-world systems â XGBoost, MXNet, and TVM. The first half of the thesis focuses on scalable learning systems that learn parameters for com- plex models using large-scale data. We introduce XGBoost, a scalable tree boosting system that scales to billions of examples in distributed or memory-limited settings. We then bring a system- atic approach under the context of MXNet to reduce the memory consumption of training to scale up real-world deep learning workloads using a minimal amount of resources. The second half of the thesis brings intelligence to learning systems themselves. We introduce TVM, a system for deploying learning to diverse hardware platforms. TVM exposes graph-level and operator-level optimization knobs to provide performance portability to deep learning work- loads across diverse hardware back-ends. We propose transfer learning methods to automate TVM and deliver performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.},
   author = {Tianqi Chen},
   title = {Scalable and Intelligent Learning Systems},
   year = {2019},
}
@article{Khudia2021,
   abstract = {Deep learning models typically use single-precision (FP32) floating point data types for representing activations and weights, but a slew of recent research work has shown that computations with reduced-precision data types (FP16, 16-bit integers, 8-bit integers or even 4- or 2-bit integers) are enough to achieve same accuracy as FP32 and are much more efficient. Therefore, we designed fbgemm, a high-performance kernel library, from ground up to perform high-performance quantized inference on current generation CPUs. fbgemm achieves efficiency by fusing common quantization operations with a high-performance gemm implementation and by shape- and size-specific kernel code generation at runtime. The library has been deployed at Facebook, where it delivers greater than 2x performance gains with respect to our current production baseline.},
   author = {Daya Khudia and Jianyu Huang and Protonu Basu and Summer Deng and Haixin Liu and Jongsoo Park and Mikhail Smelyanskiy},
   title = {FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference},
   year = {2021},
}
@article{Maulana2017,
   abstract = {Di Indonesia, banyak ditemukan pelajar yang setelah lulus dari SMA atau sederajat melanjutkan sekolah di luar kota, bahkan ada yang di luar provinsi atau juga pulau. Pilihan untuk melanjutkan studi di luar daerah berdasarkan beberapa alasan, antara lain karena fakultas atau jurusan yang diinginkan sesuai dengan minat dan bakat yang tidak terdapat di daerah asal, atau pun karena beasiswa yang diterima mengharuskan untuk melanjutkan studi di universitas yang ditentukan oleh penyedia beasiswa. Indekos sendiri memiliki fungsi utama, yaitu sebagai rumah sementara, tempat belajar, dan tempat beristirahat, namun dari data yang ditemukan banyak kasus mahasiswa melakukan pindahan indekos selama mereka menjadi perantau karena berbagai alasan. Tujuan dari penelitian ini untuk membantu penyewa dan pemilik indekos memahami proses serta faktor pemilihan indekos di daerah seikitar UMS, dari penelitian ini diharapkan para perantau yang ingin menyewa indekos akan lebih memahami keinginan mereka dan pemilik indekos juga memahami faktor-faktor yang mempengaruhi agar indekos mereka diminati. Penelitian ini menggunakan metode penelitian kualitatif deskriptif. Informan dalam penelitian ini berjumlah 10 orang dengan kriteria : (a) Mahasiswa Universitas Muhammadiyah Surakarta minimal angkatan tahun 2016, (b) Tinggal di indekos berdomisili Solo. Metode pengumpulan data yang digunakan pada penelitian ini adalah wawancara. Data dianalisis secara tematik. Berdasarkan hasil penelitian, diketahui proses pengambilan keputusan indekos yaitu, mencari pilihan indekos, mengevaluasi semua pilihan yang ada lalu memilih pilihan dan merencanakan pilihan yang diambil, sedangkan untuk pengambilan keputusan mahasiswa dalam memilih indekos dipengaruhi oleh beberapa faktor, dan peneliti mengurutkan faktor tersebut menjadi empat urutan yaitu, kenyamanan, fasilitas, harga, dan jarak. Pada penelitian selanjutnya bisa digunakan teknik purposive sampling, agar data yang didapat lebih merata terkait proses pengambilan keputusan dan faktor yang mempengaruhi, lalu bisa juga difokuskan},
   author = {Muhammad Sahidin Rizal Maulana},
   issue = {3},
   journal = {Ekp},
   keywords = {ÐÐ¸Ð°ÑÐ¸},
   pages = {1576-1580},
   title = {System Design for Large Scale Machine Learning},
   volume = {13},
   year = {2017},
}
@article{Gu2019,
   abstract = {Deep learning (DL) training jobs bring some unique challenges to existing cluster managers, such as unpredictable training times, an all-or-nothing execution model, and inflexibility in GPU sharing. Our analysis of a large GPU cluster in production shows that existing big data schedulers cause long queueing delays and low overall performance. We present Tiresias, a GPU cluster manager tailored for distributed DL training jobs, which efficiently schedules and places DL jobs to reduce their job completion times (JCTs). Given that a DL job's execution time is often unpredictable, we propose two scheduling algorithms - Discretized Two-Dimensional Gittins index relies on partial information and Discretized Two-Dimensional LAS is information-agnostic - that aim to minimize the average JCT. Additionally, we describe when the consolidated placement constraint can be relaxed, and present a placement algorithm to leverage these observations without any user input. Experiments on the Michigan ConFlux cluster with 60 P100 GPUs and large-scale trace-driven simulations show that Tiresias improves the average JCT by up to 5.5Ã over an Apache YARN-based resource manager used in production. More importantly, Tiresias's performance is comparable to that of solutions assuming perfect knowledge.},
   author = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Liu and Chuanxiong Guo},
   journal = {Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2019},
   pages = {485-500},
   title = {Tiresias: A GPU cluster manager for distributed deep learning},
   year = {2019},
}
@article{Zhao2019,
   abstract = {Recurrent Neural Network (RNN) applications form a major class of AI-powered, low-latency data center workloads. Most execution models for RNN acceleration break computation graphs into BLAS kernels, which lead to significant inter-kernel data movement and resource underutilization. We show that by supporting more general loop constructs that capture design parameters in accelerators, it is possible to improve resource utilization using cross-kernel optimization without sacrificing programmability. Such abstraction level enables a design space search that can lead to efficient usage of on-chip resources on a spatial architecture across a range of problem sizes. We evaluate our optimization strategy on such abstraction with DeepBench using a configurable spatial accelerator. We demonstrate that this implementation provides a geometric speedup of 30x in performance, 1.6x in area, and 2x in power efficiency compared to a Tesla V100 GPU, and a geometric speedup of 2x compared to Microsoft Brainwave implementation on a Stratix 10 FPGA.},
   author = {Tian Zhao and Yaqi Zhang and Kunle Olukotun},
   title = {Serving Recurrent Neural Networks Efficiently with a Spatial Accelerator},
   year = {2019},
}
@book{Xu2021,
   abstract = {We present GSPMD, an automatic, compiler-based parallelization system for common machine learning computation graphs. It allows users to write programs in the same way as for a single device, then give hints through a few annotations on how to distribute tensors, based on which GSPMD will parallelize the computation. Its representation of partitioning is simple yet general, allowing it to express different or mixed paradigms of parallelism on a wide variety of models. GSPMD infers the partitioning for every operator in the graph based on limited user annotations, making it convenient to scale up existing single-device programs. It solves several technical challenges for production usage, such as static shape constraints, uneven partitioning, exchange of halo data, and nested operator partitioning. These techniques allow GSPMD to achieve 50% to 62% compute utilization on 128 to 2048 Cloud TPUv3 cores for models with up to one trillion parameters. GSPMD produces a single program for all devices, which adjusts its behavior based on a run-time partition ID, and uses collective operators for cross-device communication. This property allows the system itself to be scalable: the compilation time stays constant with increasing number of devices.},
   author = {Yuanzhong Xu and HyoukJoong Lee and Dehao Chen and Blake Hechtman and Yanping Huang and Rahul Joshi and Maxim Krikun and Dmitry Lepikhin and Andy Ly and Marcello Maggioni and Ruoming Pang and Noam Shazeer and Shibo Wang and Tao Wang and Yonghui Wu and Zhifeng Chen},
   issue = {1},
   journal = {Proceedings of ACM Conference (Conference'17)},
   publisher = {Association for Computing Machinery},
   title = {GSPMD: General and Scalable Parallelization for ML Computation Graphs},
   volume = {1},
   url = {http://arxiv.org/abs/2105.04663},
   year = {2021},
}
@article{Che2011,
   abstract = {Graphics processors (GPUs) have emerged as an important platform for general purpose computing. GPUs offer a large number of parallel cores and have access to high memory bandwidth; however, data structure layouts in GPU memory often lead to suboptimal performance for programs designed with a CPU memory interface-or no particular memory interface at all!-in mind. This implies that application performance is highly sensitive irregularity in memory access patterns. This issue is all the more important due to the growing disparity between core and DRAM clocks; memory interfaces have increasingly become bottlenecks in computer systems. In this paper, we propose a simple API, Dymaxion1, that allows programmers to optimize memory mappings to improve the efficiency of memory accesses on heterogeneous platforms. Use of Dymaxion requires only minimal modifications to existing CUDA programs. Our current framework extends NVIDIA's CUDA API with the addition of memory layout remapping and index transformation. We consider the overhead of layout remapping and effectively hide it through chunking and overlapping with PCI-E transfer. We present the implementation of Dymaxion and its optimizations and evaluate a variety of important memory access patterns. Using four case studies, we are able to achieve 3.3Ã speedup on GPU kernels and 20% overall performance improvement, including the PCI-E transfer, over the original CUDA implementations on an NVIDIA GTX 480 GPU. We also explore the importance of maintaining per-device data layouts and cross-device data mappings with a case study of concurrent CPU-GPU execution. Copyright 2011 ACM.},
   author = {Shuai Che and Jeremy W. Sheaffer and Kevin Skadron},
   doi = {10.1145/2063384.2063401},
   isbn = {9781450307710},
   journal = {Proceedings of 2011 SC - International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {GPGPU,Heterogeneous computer architectures,Latency hiding,Memory access and data layout},
   title = {Dymaxion: Optimizing memory access patterns for heterogeneous systems},
   year = {2011},
}
@article{,
   abstract = {The Von Neumann bottleneck is a persistent problem in computer architecture, causing stalls and wasted CPU cycles. The Van Neumann bottleneck is particularly relevant for memory-intensive workloads whose working set does not fit into the microprocessor's cache and hence memory accesses suffer the high access latency of DRAM. One technique to address this bottleneck is to prefetch data from memory into on-chip caches. While prefetching has proven successful, for simple access patterns such as strides, existing prefetchers are incapable of providing benefit for applications with complex, irregular access patterns. A neural network-based prefetcher shows promise for these challenging workloads. We provide a better understanding of what type of memory access patterns an LSTM neural network can learn by training individual models on microbenchmarks with well-characterized memory access patterns. We explore a range of model parameters and provide a better understanding of what model is ideal to use. We achieve over 95% accuracy on the microbenchmarks and find a strong relationship between lookback (history window) size and the ability of the model to learn the pattern. We find also an upper limit on the number of concurrent distinct memory access streams that can be learned by a model of a given size.},
   author = {Peter Braun and Heiner Litz},
   journal = {Eecs.Oregonstate.Edu},
   title = {Understanding Memory Access Patterns for Prefetching},
   url = {https://eecs.oregonstate.edu/aidarc/paper/MAP.pdf},
}
@article{Ayers2020,
   abstract = {Prefetching is a well-studied technique for addressing the memory access stall time of contemporary microprocessors. However, despite a large body of related work, the memory access behavior of applications is not well understood, and it remains difficult to predict whether a particular application will benefit from a given prefetcher technique. In this work we propose a novel methodology to classify the memory access patterns of applications, enabling well-informed reasoning about the applicability of a certain prefetcher. Our approach leverages instruction dataflow information to uncover a wide range of access patterns, including arbitrary combinations of offsets and indirection. These combinations-or prefetch kernels-represent reuse, strides, reference locality, and complex address generation. By determining the complexity and frequency of these access patterns, we enable reasoning about prefetcher timeliness and criticality, exposing the limitations of existing prefetchers today. Moreover, using these kernels, we are able to compute the next address for the majority of top-missing instructions, and we propose a software prefetch injection methodology that is able to outperform state-of-the-art hardware prefetchers.},
   author = {Grant Ayers and Heiner Litz and Christos Kozyrakis and Parthasarathy Ranganathan},
   doi = {10.1145/3373376.3378498},
   isbn = {9781450371025},
   journal = {International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
   pages = {513-526},
   title = {Classifying memory access patterns for prefetching},
   year = {2020},
}
@article{Hashemi2018,
   abstract = {The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.},
   author = {Milad Hashemi and Kevin Swersky and Jamie A. Smith and Grant Ayers and Heiner Litz and Jichuan Chang and Christos Kozyrakis and Parthasarathy Ranganathan},
   isbn = {9781510867963},
   journal = {35th International Conference on Machine Learning, ICML 2018},
   pages = {3062-3076},
   title = {Learning memory access patterns},
   volume = {5},
   year = {2018},
}
@article{Bendersky2015,
   abstract = {When working with multi-dimensional arrays, one important decision programmers have to make fairly early on in the project is what memory layout to use for storing the data, and how to access such data in the most efficient manner. Since computer memory is inherently linear - a one- dimensional structure, mapping multi-dimensional data on it can be done in several ways. In this article I want to examine this topic in detail, talking about the various memory layouts available and their effect on the performance of the code.},
   author = {Eli Bendersky},
   pages = {1-12},
   title = {Memory layout of multi-dimensional arrays},
   year = {2015},
}
@article{Library2021,
   author = {Math Kernel Library and Deep Neural Networks and D Spatial},
   pages = {1-7},
   title = {Understanding Memory Formats Introduction Data formats},
   year = {2021},
}
@article{,
   author = {Mahmoud Eljammaly and Lars Karlsson},
   keywords = {canonical storage format,dense tensors,format conversion,in-place conversion,out-of-place conversion,tensor matricization,tensor storage},
   pages = {1-21},
   title = {A Library for Storing and Manipulating Dense Tensors},
}
@article{Yu2019,
   abstract = {GPU computing is becoming increasingly more popular with the proliferation of deep learning (DL) applications. However, unlike traditional resources such as CPU or the network, modern GPUs do not natively support fine-grained sharing primitives. Consequently, implementing common policies such as time sharing and preemption are expensive. Worse, when a DL application cannot completely use a GPU's resources, the GPU cannot be efficiently shared between multiple applications, leading to GPU underutilization. We present Salus to enable two GPU sharing primitives: fast job switching and memory sharing, in order to achieve fine-grained GPU sharing among multiple DL applications. Salus implements an efficient, consolidated execution service that exposes the GPU to different DL applications, and enforces fine-grained sharing by performing iteration scheduling and addressing associated memory management issues. We show that these primitives can then be used to implement flexible sharing policies such as fairness, prioritization, and packing for various use cases. Our integration of Salus with TensorFlow and evaluation on popular DL jobs show that Salus can improve the average completion time of DL training jobs by $3.19\times$, GPU utilization for hyper-parameter tuning by $2.38\times$, and GPU utilization of DL inference applications by $42\times$ over not sharing the GPU and $7\times$ over NVIDIA MPS with small overhead.},
   author = {Peifeng Yu and Mosharaf Chowdhury},
   title = {Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications},
   year = {2019},
}
@article{Jin2017,
   abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.},
   author = {Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   pages = {2727-2752},
   title = {How to escape saddle points efficiently},
   volume = {4},
   year = {2017},
}
@article{Smith2018,
   abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B â . Finally, one can increase the momentum coefficient m and scale B â 1/(1 â m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1% validation accuracy in under 30 minutes.},
   author = {Samuel L. Smith and Pieter Jan Kindermans and Chris Ying and Quoc V. Le},
   issue = {2017},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   pages = {1-11},
   title = {Donât decay the learning rate, increase the batch size},
   year = {2018},
}
@article{Xie2020,
   abstract = {Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.},
   author = {Zeke Xie and Issei Sato and Masashi Sugiyama},
   issue = {2019},
   pages = {1-28},
   title = {A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
   url = {http://arxiv.org/abs/2002.03495},
   year = {2020},
}
@article{Wu2020,
   abstract = {The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.},
   author = {Jingfeng Wu and Wenqing Hu and Haoyi Xiong and Jun Huan and Vladimir Braverman and Zhanxing Zhu},
   isbn = {9781713821120},
   journal = {37th International Conference on Machine Learning, ICML 2020},
   pages = {10298-10307},
   title = {On the noisy gradient descent that generalizes as SGD},
   volume = {PartF16814},
   year = {2020},
}
@article{Bottou2018,
   abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
   author = {LÃ©on Bottou and Frank E. Curtis and Jorge Nocedal},
   doi = {10.1137/16M1080173},
   issn = {00361445},
   issue = {2},
   journal = {SIAM Review},
   keywords = {Algorithm complexity analysis,Machine learning,Noise reduction methods,Numerical optimization,Second-order methods,Stochastic gradient methods},
   pages = {223-311},
   title = {Optimization methods for large-scale machine learning},
   volume = {60},
   year = {2018},
}
@article{He2019,
   abstract = {Deep neural networks have received dramatic success based on the optimization method of stochastic gradient descent (SGD). However, it is still not clear how to tune hyper-parameters, especially batch size and learning rate, to ensure good generalization. This paper reports both theoretical and empirical evidence of a training strategy that we should control the ratio of batch size to learning rate not too large to achieve a good generalization ability. Specifically, we prove a PAC-Bayes generalization bound for neural networks trained by SGD, which has a positive correlation with the ratio of batch size to learning rate. This correlation builds the theoretical foundation of the training strategy. Furthermore, we conduct a large-scale experiment to verify the correlation and training strategy. We trained 1,600 models based on architectures ResNet-110, and VGG-19 with datasets CIFAR-10 and CIFAR-100 while strictly control unrelated variables. Accuracies on the test sets are collected for the evaluation. Spearman's rank-order correlation coefficients and the corresponding p values on 164 groups of the collected data demonstrate that the correlation is statistically significant, which fully supports the training strategy.},
   author = {Fengxiang He and Tongliang Liu and Dacheng Tao},
   issn = {10495258},
   issue = {NeurIPS},
   journal = {Advances in Neural Information Processing Systems},
   title = {Control batch size and learning rate to generalize well: Theoretical and empirical evidence},
   volume = {32},
   year = {2019},
}
@article{Chen2021,
   abstract = {Realizing the potential of quantum computing will require achieving sufficiently low logical error rates. Many applications call for error rates in the $10^\{-15\}$ regime, but state-of-the-art quantum platforms typically have physical error rates near $10^\{-3\}$. Quantum error correction (QEC) promises to bridge this divide by distributing quantum logical information across many physical qubits so that errors can be detected and corrected. Logical errors are then exponentially suppressed as the number of physical qubits grows, provided that the physical error rates are below a certain threshold. QEC also requires that the errors are local and that performance is maintained over many rounds of error correction, two major outstanding experimental challenges. Here, we implement 1D repetition codes embedded in a 2D grid of superconducting qubits which demonstrate exponential suppression of bit or phase-flip errors, reducing logical error per round by more than $100\times$ when increasing the number of qubits from 5 to 21. Crucially, this error suppression is stable over 50 rounds of error correction. We also introduce a method for analyzing error correlations with high precision, and characterize the locality of errors in a device performing QEC for the first time. Finally, we perform error detection using a small 2D surface code logical qubit on the same device, and show that the results from both 1D and 2D codes agree with numerical simulations using a simple depolarizing error model. These findings demonstrate that superconducting qubits are on a viable path towards fault tolerant quantum computing.},
   author = {Zijun Chen and Kevin J. Satzinger and Juan Atalaya and Alexander N. Korotkov and Andrew Dunsworth and Daniel Sank and Chris Quintana and Matt McEwen and Rami Barends and Paul V. Klimov and Sabrina Hong and Cody Jones and Andre Petukhov and Dvir Kafri and Sean Demura and Brian Burkett and Craig Gidney and Austin G. Fowler and Harald Putterman and Igor Aleiner and Frank Arute and Kunal Arya and Ryan Babbush and Joseph C. Bardin and Andreas Bengtsson and Alexandre Bourassa and Michael Broughton and Bob B. Buckley and David A. Buell and Nicholas Bushnell and Benjamin Chiaro and Roberto Collins and William Courtney and Alan R. Derk and Daniel Eppens and Catherine Erickson and Edward Farhi and Brooks Foxen and Marissa Giustina and Jonathan A. Gross and Matthew P. Harrigan and Sean D. Harrington and Jeremy Hilton and Alan Ho and Trent Huang and William J. Huggins and L. B. Ioffe and Sergei V. Isakov and Evan Jeffrey and Zhang Jiang and Kostyantyn Kechedzhi and Seon Kim and Fedor Kostritsa and David Landhuis and Pavel Laptev and Erik Lucero and Orion Martin and Jarrod R. McClean and Trevor McCourt and Xiao Mi and Kevin C. Miao and Masoud Mohseni and Wojciech Mruczkiewicz and Josh Mutus and Ofer Naaman and Matthew Neeley and Charles Neill and Michael Newman and Murphy Yuezhen Niu and Thomas E. O'Brien and Alex Opremcak and Eric Ostby and BÃ¡lint PatÃ³ and Nicholas Redd and Pedram Roushan and Nicholas C. Rubin and Vladimir Shvarts and Doug Strain and Marco Szalay and Matthew D. Trevithick and Benjamin Villalonga and Theodore White and Z. Jamie Yao and Ping Yeh and Adam Zalcman and Hartmut Neven and Sergio Boixo and Vadim Smelyanskiy and Yu Chen and Anthony Megrant and Julian Kelly},
   doi = {10.1038/s41586-021-03588-y},
   issue = {January},
   pages = {383-388},
   title = {Exponential suppression of bit or phase flip errors with repetitive error correction},
   volume = {595},
   url = {http://arxiv.org/abs/2102.06132},
   year = {2021},
}
@article{Krizhevsky2014,
   abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
   author = {Alex Krizhevsky},
   title = {One weird trick for parallelizing convolutional neural networks},
   url = {http://arxiv.org/abs/1404.5997},
   year = {2014},
}
@article{Ferry2020,
   abstract = {Quantum Computing is a new and exciting field at the intersection of mathematics, computer science and physics. It concerns a utilization of quantum mechanics to improve the efficiency of computation. Here we present a gentle introduction to some of the ideas in quantum computing. The paper begins by motivating the central ideas of quantum mechanics and quantum computation with simple toy models. From there we move on to a formal presentation of the small fraction of (finite dimensional) quantum mechanics that we will need for basic quantum computation. Central notions of quantum architecture (qubits and quantum gates) are described. The paper ends with a presentation of one of the simplest quantum algorithms: Deutsch's algorithm. Our presentation demands neither advanced mathematics nor advanced physics.},
   author = {David Ferry},
   doi = {10.4324/9781003031949-11},
   journal = {Quantum Mechanics},
   pages = {267-293},
   title = {An Introduction to Quantum Computing},
   year = {2020},
}
@book{Rieffel2010,
   author = {Eleanor Rieffel and Wolfgang Polak},
   title = {QUANTUM COMPUTING A Gentle Introduction},
   year = {2010},
}
@article{Farhi2014,
   abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
   author = {Edward Farhi and Jeffrey Goldstone and Sam Gutmann},
   pages = {1-16},
   title = {A Quantum Approximate Optimization Algorithm},
   url = {http://arxiv.org/abs/1411.4028},
   year = {2014},
}
@article{Brandao2017,
   abstract = {We give a quantum algorithm for solving semidefinite programs (SDPs). It has worst-case running time n^\{1\}\{2\}\} m^\{1\}\{2\}\} s^2 \poly(\log(n), \log(m), R, r, 1/Î), with n and s the dimension and row-sparsity of the input matrices, respectively, m the number of constraints, Î the accuracy of the solution, and R, r upper bounds on the size of the optimal primal and dual solutions, respectively. This gives a square-root unconditional speed-up over any classical method for solving SDPs both in n and m. We prove the algorithm cannot be substantially improved (in terms of n and m) giving a (n^\{1\}\{2\}\}+m^\{1\}\{2\}\}) quantum lower bound for solving semidefinite programs with constant s, R, r and Î. The quantum algorithm is constructed by a combination of quantum Gibbs sampling and the multiplicative weight method. In particular it is based on a classical algorithm of Arora and Kale for approximately solving SDPs. We present a modification of their algorithm to eliminate the need for solving an inner linear program which may be of independent interest.},
   author = {Fernando G.S.L. Brandao and Krysta M. Svore},
   doi = {10.1109/FOCS.2017.45},
   isbn = {9781538634646},
   issn = {02725428},
   journal = {Annual Symposium on Foundations of Computer Science - Proceedings},
   keywords = {Gibbs sampling,quantum algorithms,semidefinite programs},
   pages = {415-426},
   title = {Quantum speed-ups for solving semidefinite programs},
   volume = {2017-Octob},
   year = {2017},
}
@article{Montanaro2016,
   author = {Ashley Montanaro},
   doi = {10.1038/npjqi.2015.23},
   journal = {Nature Publishing Group},
   pages = {1-8},
   publisher = {Nature Publishing Group},
   title = {Quantum algorithms : an overview},
   url = {http://dx.doi.org/10.1038/npjqi.2015.23},
   year = {2016},
}
@article{Rebentrost2019,
   abstract = {Optimization problems in disciplines such as machine learning are commonly solved with iterative methods. Gradient descent algorithms find local minima by moving along the direction of steepest descent while Newton's method takes into account curvature information and thereby often improves convergence. Here, we develop quantum versions of these iterative optimization algorithms and apply them to polynomial optimization with a unit norm constraint. In each step, multiple copies of the current candidate are used to improve the candidate using quantum phase estimation, an adapted quantum state exponentiation scheme, as well as quantum matrix multiplications and inversions. The required operations perform polylogarithmically in the dimension of the solution vector and exponentially in the number of iterations. Therefore, the quantum algorithm can be useful for high-dimensional problems where a small number of iterations is sufficient.},
   author = {Patrick Rebentrost and Maria Schuld and Leonard Wossnig and Francesco Petruccione and Seth Lloyd},
   doi = {10.1088/1367-2630/ab2a9e},
   issn = {13672630},
   issue = {7},
   journal = {New Journal of Physics},
   keywords = {density matrix exponentiation,quantum computing,quantum optimization},
   title = {Quantum gradient descent and Newton's method for constrained polynomial optimization},
   volume = {21},
   year = {2019},
}
@article{,
   author = {Andrew M Childs and Dmitri Maslov and Yunseong Nam},
   title = {Toward the first quantum simulation with quantum speedup},
}
@article{,
   author = {Patrick Rebentrost and Masoud Mohseni and Seth Lloyd},
   issue = {2},
   pages = {1-5},
   title = {Quantum support vector machine for big data classification},
   volume = {1},
}
@article{Childs2021,
   author = {Andrew M Childs},
   issue = {April},
   title = {Lecture Notes on Quantum Algorithms},
   year = {2021},
}
@article{,
   author = {C Lu and J Pan},
   issue = {2},
   pages = {3-8},
   title = {Entanglement-Based Machine Learning on a Quantum Computer},
}
@article{Brandl2017,
   abstract = {As the size of quantum systems becomes bigger, more complicated hardware is required to control these systems. In order to reduce the complexity, I discuss the amount of parallelism required for a fault-tolerant quantum computer and what computation speed can be achieved in different architectures. To build a large-scale quantum computer, one can use architectural principles, from classical computer architecture, like multiplexing or pipelining. In this document, a Quantum von Neumann architecture is introduced which uses specialized hardware for the different tasks of a quantum computer, like computation or storage. Furthermore, it requires long qubit coherence and the capability to move quantum information between the different parts of the quantum computer. As an example, a Quantum von Neumann architecture for trapped ions is presented which incorporates multiplexing in the memory region for large-scale quantum computation. To illustrate the capability of this architecture, a model trapped ion quantum computer based on Quantum von Neumann architecture, the Quantum 4004, is introduced. Its hardware is optimized for simplicity and uses the classical Intel 4004 CPU from 1971 as a blueprint. The Quantum 4004 has only a single processing zone and is structured in 4 qubit packages. Its quantum memory can store up to 32768 qubit ions and its computation speed is 10 $\mu$s for single qubit operations and 20 $\mu$s for two-qubit operations.},
   author = {Matthias F. Brandl},
   pages = {1-44},
   title = {A Quantum von Neumann Architecture for Large-Scale Quantum Computing},
   url = {http://arxiv.org/abs/1702.02583},
   year = {2017},
}
@article{Blacoe2013,
   author = {William Blacoe and Elham Kashefi and Mirella Lapata},
   issue = {June},
   pages = {847-857},
   title = {A Quantum-Theoretic Approach to Distributional Semantics},
   year = {2013},
}
@article{Oskin2002,
   author = {Mark Oskin and Frederic T. Chong and Isaac L. Chuang},
   doi = {10.1109/2.976922},
   issn = {00189162},
   issue = {1},
   journal = {Computer},
   pages = {79-87},
   title = {A practical architecture for reliable quantum computers},
   volume = {35},
   year = {2002},
}
@article{Schuld2015,
   abstract = {Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.},
   author = {Maria Schuld and Ilya Sinayskiy and Francesco Petruccione},
   doi = {10.1080/00107514.2014.964942},
   issn = {13665812},
   issue = {2},
   journal = {Contemporary Physics},
   keywords = {artificial intelligence,machine learning,quantum computing,quantum machine learning},
   pages = {172-185},
   title = {An introduction to quantum machine learning},
   volume = {56},
   year = {2015},
}
@article{Biamonte2017,
   abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.},
   author = {Jacob Biamonte and Peter Wittek and Nicola Pancotti and Patrick Rebentrost and Nathan Wiebe and Seth Lloyd},
   doi = {10.1038/nature23474},
   issn = {14764687},
   issue = {7671},
   journal = {Nature},
   pages = {195-202},
   pmid = {28905917},
   title = {Quantum machine learning},
   volume = {549},
   year = {2017},
}
@article{Lloyd2014,
   abstract = {The usual way to reveal properties of an unknown quantum state, given many copies of a system in that state, is to perform measurements of dierent observables and to analyse the results statistically 1,2 . For non-sparse but low-rank quantum states, revealing eigenvectors and corresponding eigenvalues in classical form scales super-linearly with the system dimension 3â6 . Here we show that multiple copies of a quantum system with density matrix Ï can be used to construct the unitary transformation e âiÏt . As a result, one can perform quantum principal component analysis of an unknown low-rank density matrix, revealing in quantum form the eigenvectors corresponding to the large eigenvalues in time exponentially faster than any existing algorithm. We discuss applications to data analysis, process tomography and state discrimination.},
   author = {Seth Lloyd and Masoud Mohseni and Patrick Rebentrost},
   doi = {10.1038/NPHYS3029},
   issn = {17452481},
   issue = {9},
   journal = {Nature Physics},
   pages = {631-633},
   title = {Quantum principal component analysis},
   volume = {10},
   year = {2014},
}
@article{,
   abstract = {Machine Learning (ML) is becoming a more and more popular field of knowledge, being a term known not only in the academic field due to its successful applications to many real-world problems. The advent of Deep Learning and Big Data in the last decade has contributed to make it even more popular. Many companies, both large ones and SMEs, have created specific departments for ML and data analysis, being in fact their main activity in many cases. This current exploitation of ML should not mislead us; while it is a mature field of knowledge, there is still room for many novel contributions, namely, a better understanding of the underlying Mathematics, proposal and tuning of algorithms suitable for new problems (e.g., Natural Language Processing), automation and optimization of the search of parameters, etc. Within this framework of new contributions to ML, Quantum Machine Learning (QML) has emerged strongly lately, speeding up ML calculations and providing alternative representations to existing approaches. This special session includes six high-quality papers dealing with some of the most relevant aspects of QML, including analysis of learning in quantum computing and quantum annealers, quantum versions of classical ML models "like neural networks or learning vector quantization", and quantum learning approaches for measurement and control.},
   author = {Jose D. Martin-Guerrero and Lucas Lamata},
   doi = {10.1016/b978-0-12-821982-9.00007-1},
   isbn = {9782875870742},
   issue = {October},
   journal = {ESANN 2020 - Proceedings, 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
   pages = {257-266},
   title = {Quantum machine learning},
   year = {2020},
}
@article{Wiebe2016,
   abstract = {In recent years, deep learning has had a profound impact on machine learning and artificial intelligence. At the same time, algorithms for quantum computers have been shown to efficiently solve some problems that are intractable on conventional, classical computers. We show that quantum computing not only reduces the time required to train a deep restricted Boltzmann machine, but also provides a richer and more comprehensive framework for deep learning than classical computing and leads to significant improvements in the optimization of the underlying objective function. Our quantum methods also permit efficient training of multilayer and fully connected models.},
   author = {Nathan Wiebe and Ashish Kapoor and Krysta M. Svore},
   doi = {10.26421/qic16.7-8-1},
   issn = {15337146},
   issue = {7-8},
   journal = {Quantum Information and Computation},
   keywords = {Quantum algorithms,Quantum computing,Quantum machine learning},
   pages = {541-587},
   title = {Quantum Deep Learning},
   volume = {16},
   year = {2016},
}
@article{Garg2020,
   abstract = {The last few decades have seen significant breakthroughs in the fields of deep learning and quantum computing. Research at the junction of the two fields has garnered an increasing amount of interest, which has led to the development of quantum deep learning and quantum-inspired deep learning techniques in recent times. In this work, we present an overview of advances in the intersection of quantum computing and deep learning by discussing the technical contributions, strengths and similarities of various research works in this domain. To this end, we review and summarise the different schemes proposed to model quantum neural networks (QNNs) and other variants like quantum convolutional networks (QCNNs). We also briefly describe the recent progress in quantum inspired classic deep learning algorithms and their applications to natural language processing.},
   author = {Siddhant Garg and Goutham Ramakrishnan},
   title = {Advances in Quantum Deep Learning: An Overview},
   url = {http://arxiv.org/abs/2005.04316},
   year = {2020},
}
@article{Luo2020,
   abstract = { We introduce   Yao   , an extensible, efficient open-source framework for quantum algorithm design.   Yao   features generic and differentiable programming of quantum circuits. It achieves state-of-the-art performance in simulating small to intermediate-sized quantum circuits that are relevant to near-term applications. We introduce the design principles and critical techniques behind   Yao   . These include the quantum block intermediate representation of quantum circuits, a builtin automatic differentiation engine optimized for reversible computing, and batched quantum registers with GPU acceleration. The extensibility and efficiency of   Yao   help boost innovation in quantum algorithm design. },
   author = {Xiu-Zhe Luo and Jin-Guo Liu and Pan Zhang and Lei Wang},
   doi = {10.22331/q-2020-10-11-341},
   issn = {2521-327X},
   journal = {Quantum},
   pages = {341},
   title = {Yao.jl: Extensible, Efficient Framework for Quantum Algorithm Design},
   volume = {4},
   year = {2020},
}
@article{Fu2016,
   author = {X Fu and L Lao and C G Almudever and F Sebastiano and R Versluis and E Charbon and K Bertels},
   doi = {10.1109/nsyss.2016.7400683},
   isbn = {9781450341288},
   keywords = {architecture,micro-,quantum computer},
   pages = {1-1},
   title = {A Heterogeneous Quantum Computer Architecture},
   year = {2016},
}
@article{Aram2009,
   author = {W Aram and Avinatan Hassidim and Seth Lloyd Quantum},
   doi = {10.1103/PhysRevLett.103.150502},
   title = {Quantum Algorithm for Linear Systems of Equations},
   year = {2009},
}
@article{,
   abstract = {As quantum computers become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classical computer programs for most of their careers. While currently available quantum computers have less than 100 qubits, quantum computing hardware is widely expected to grow in terms of qubit count, quality, and connectivity. This review aims to explain the principles of quantum programming, which are quite different from classical programming, with straightforward algebra that makes understanding of the underlying fascinating quantum mechanical principles optional. We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succinct and self-contained fashion. We show how these algorithms can be implemented on IBM's quantum computer, and in each case, we discuss the results of the implementation with respect to differences between the simulator and the actual hardware runs. This article introduces computer scientists, physicists, and engineers to quantum algorithms and provides a blueprint for their implementations.},
   author = {Abhijith J. and Adetokunbo Adedoyin and John Ambrosiano and Petr Anisimov and Andreas BÃ¤rtschi and William Casper and Gopinath Chennupati and Carleton Coffrin and Hristo Djidjev and David Gunter and Satish Karra and Nathan Lemons and Shizeng Lin and Alexander Malyzhenkov and David Mascarenas and Susan Mniszewski and Balu Nadiga and Daniel O'Malley and Diane Oyen and Scott Pakin and Lakshman Prasad and Randy Roberts and Phillip Romero and Nandakishore Santhi and Nikolai Sinitsyn and Pieter J. Swart and James G. Wendelberger and Boram Yoon and Richard Zamora and Wei Zhu and Stephan Eidenbenz and Patrick J. Coles and Marc Vuffray and Andrey Y. Lokhov},
   title = {Quantum Algorithm Implementations for Beginners},
   url = {http://arxiv.org/abs/1804.03719},
   year = {2018},
}
@article{Karamlou2020,
   abstract = {In the near-term, hybrid quantum-classical algorithms hold great potential for outperforming classical approaches. Understanding how these two computing paradigms work in tandem is critical for identifying areas where such hybrid algorithms could provide a quantum advantage. In this work, we study a QAOA-based quantum optimization algorithm by implementing the Variational Quantum Factoring (VQF) algorithm. We execute experimental demonstrations using a superconducting quantum processor and investigate the trade-off between quantum resources (number of qubits and circuit depth) and the probability that a given biprime is successfully factored. In our experiments, the integers 1099551473989, 3127, and 6557 are factored with 3, 4, and 5 qubits, respectively, using a QAOA ansatz with up to 8 layers and we are able to identify the optimal number of circuit layers for a given instance to maximize success probability. Furthermore, we demonstrate the impact of different noise sources on the performance of QAOA and reveal the coherent error caused by the residual ZZ-coupling between qubits as a dominant source of error in the superconducting quantum processor.},
   author = {Amir H. Karamlou and William A. Simon and Amara Katabarwa and Travis L. Scholten and Borja Peropadre and Yudong Cao},
   pages = {1-11},
   title = {Analyzing the Performance of Variational Quantum Factoring on a Superconducting Quantum Processor},
   url = {http://arxiv.org/abs/2012.07825},
   year = {2020},
}
@article{Verdon2019,
   abstract = {We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.},
   author = {Guillaume Verdon and Trevor McCourt and Enxhell Luzhnica and Vikash Singh and Stefan Leichenauer and Jack Hidary},
   title = {Quantum Graph Neural Networks},
   url = {http://arxiv.org/abs/1909.12264},
   year = {2019},
}
@article{Sack2021,
   abstract = {The quantum approximate optimization algorithm (QAOA) is a prospective near-term quantum algorithm due to its modest circuit depth and promising benchmarks. However, an external parameter optimization required in QAOA could become a performance bottleneck. This motivates studies of the optimization landscape and search for heuristic ways of parameter initialization. In this work we visualize the optimization landscape of the QAOA applied to the MaxCut problem on random graphs, demonstrating that random initialization of the QAOA is prone to converging to local minima with sub-optimal performance. We introduce the initialization of QAOA parameters based on the Trotterized quantum annealing (TQA) protocol, parameterized by the Trotter time step. We find that the TQA initialization allows to circumvent the issue of false minima for a broad range of time steps, yielding the same performance as the best result out of an exponentially scaling number of random initializations. Moreover, we demonstrate that the optimal value of the time step coincides with the point of proliferation of Trotter errors in quantum annealing. Our results suggest practical ways of initializing QAOA protocols on near-term quantum devices and reveals new connections between QAOA and quantum annealing.},
   author = {Stefan H. Sack and Maksym Serbyn},
   pages = {1-11},
   title = {Quantum annealing initialization of the quantum approximate optimization algorithm},
   url = {http://arxiv.org/abs/2101.05742},
   year = {2021},
}
@article{Grossi2021,
   abstract = {Starting from the idea of Quantum Computing which is a concept that dates back to 80s, we come to the present day where we can perform calculations on real quantum computers. This sudden development of technology opens up new scenarios that quickly lead to the desire and the real possibility of integrating this technology into current software architectures. The usage of frameworks that allow computation to be performed directly on quantum hardware poses a series of challenges. This document describes a an architectural framework that addresses the problems of integrating an API exposed Quantum provider in an existing Enterprise architecture and it provides a minimum viable product (MVP) solution that really merges classical quantum computers on a basic scenario with reusable code on GitHub repository. The solution leverages a web-based frontend where user can build and select applications/use cases and simply execute it without any further complication. Every triggered run leverages on multiple backend options, that include a scheduler managing the queuing mechanism to correctly schedule jobs and final results retrieval. The proposed solution uses the up-to-date cloud native technologies (e.g. Cloud Functions, Containers, Microservices) and serves as a general framework to develop multiple applications on the same infrastructure.},
   author = {M. Grossi and L. Crippa and A. Aita and G. Bartoli and V. Sammarco and E. Picca and N. Said and F. Tramonto and F. Mattei},
   issue = {Mi},
   pages = {1-8},
   title = {A Serverless Cloud Integration For Quantum Computing},
   url = {http://arxiv.org/abs/2107.02007},
   year = {2021},
}
@article{Gorard2021,
   abstract = {This article presents a novel algorithmic methodology for performing automated diagrammatic deductions over combinatorial structures, using a combination of modified equational theorem-proving techniques and the extended Wolfram model hypergraph rewriting formalism developed by the authors in previous work. We focus especially upon the application of this new algorithm to the problem of automated circuit simplification in quantum information theory, using Wolfram model multiway operator systems combined with the ZX-calculus formalism for enacting fast diagrammatic reasoning over linear transformations between qubits. We show how to construct a generalization of the deductive inference rules for Knuth-Bendix completion in which equation matches are selected on the basis of causal edge density in the associated multiway system, before proceeding to demonstrate how to embed the higher-order logic of the ZX-calculus rules within this first-order equational framework. After showing explicitly how the (hyper)graph rewritings of both Wolfram model systems and the ZX-calculus can be effectively realized within this formalism, we proceed to exhibit comparisons of time complexity vs. proof complexity for this new algorithmic approach when simplifying randomly-generated Clifford circuits down to pseudo-normal form, as well as when reducing the number of T-gates in randomly-generated non-Clifford circuits, with circuit sizes ranging up to 3000 gates, illustrating that the method performs favorably in comparison with existing circuit simplification frameworks, and also exhibiting the approximately quadratic speedup obtained by employing the causal edge density optimization. Finally, we present a worked example of an automated proof of correctness for a simple quantum teleportation protocol, in order to demonstrate more clearly the internal operations of the theorem-proving procedure.},
   author = {Jonathan Gorard and Manojna Namuduri and Xerxes D. Arsiwalla},
   title = {ZX-Calculus and Extended Wolfram Model Systems II: Fast Diagrammatic Reasoning with an Application to Quantum Circuit Simplification},
   url = {http://arxiv.org/abs/2103.15820},
   year = {2021},
}
@article{Leontica2021,
   abstract = {Simulating the behaviour of complex quantum systems is impossible on classical supercomputers due to the exponential scaling of the number of quantum states with the number of particles in the simulated system. Quantum computers aim to break through this limit by using one quantum system to simulate another quantum system. Although in their infancy, they are a promising tool for applied fields seeking to simulate quantum interactions in complex atomic and molecular structures. Here, we show an efficient technique for transpiling the unitary evolution of quantum systems into the language of universal quantum computation using the IBM quantum computer and show that it is a viable tool for compiling near-term quantum simulation algorithms. We develop code that decomposes arbitrary 3-qubit gates and implement it in a quantum simulation first for a linear ordered chain to highlight the generality of the approach, and second, for a complex molecule. We choose the Fenna-Matthews-Olsen (FMO) photosynthetic protein because it has a well characterised Hamiltonian and presents a complex dissipative system coupled to a noisy environment that helps to improve the efficiency of energy transport. The method can be implemented in a broad range of molecular and other simulation settings.},
   author = {S. Leontica and F. Tennie and T. Farrow},
   doi = {10.1038/s42005-021-00616-1},
   isbn = {4200502100},
   issn = {23993650},
   issue = {1},
   journal = {Communications Physics},
   pages = {1-7},
   publisher = {Springer US},
   title = {Simulating molecules on a cloud-based 5-qubit IBM-Q universal quantum computer},
   volume = {4},
   url = {http://dx.doi.org/10.1038/s42005-021-00616-1},
   year = {2021},
}
@article{,
   abstract = {It has been hypothesized that quantum computers may lend themselves well to applications in machine learning. In the present work, we analyze function classes defined via quantum kernels. Quantum computers offer the possibility to efficiently compute inner products of exponentially large density operators that are classically hard to compute. However, having an exponentially large feature space renders the problem of generalization hard. Furthermore, being able to evaluate inner products in high dimensional spaces efficiently by itself does not guarantee a quantum advantage, as already classically tractable kernels can correspond to high- or infinite-dimensional reproducing kernel Hilbert spaces (RKHS). We analyze the spectral properties of quantum kernels and find that we can expect an advantage if their RKHS is low dimensional and contains functions that are hard to compute classically. If the target function is known to lie in this class, this implies a quantum advantage, as the quantum computer can encode this inductive bias, whereas there is no classically efficient way to constrain the function class in the same way. However, we show that finding suitable quantum kernels is not easy because the kernel evaluation might require exponentially many measurements. In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode knowledge about the problem at hand into quantum circuits, while encoding the same bias into a classical model would be hard. These situations may plausibly occur when learning on data generated by a quantum process, however, they appear to be harder to come by for classical datasets.},
   author = {Jonas M. KÃ¼bler and Simon Buchholz and Bernhard SchÃ¶lkopf},
   pages = {1-27},
   title = {The Inductive Bias of Quantum Kernels},
   url = {http://arxiv.org/abs/2106.03747},
   year = {2021},
}
@article{Preskill2021,
   abstract = {Forty years ago, Richard Feynman proposed harnessing quantum physics to build a more powerful kind of computer. Realizing Feynman's vision is one of the grand challenges facing 21st century science and technology. In this article, we'll recall Feynman's contribution that launched the quest for a quantum computer, and assess where the field stands 40 years later.},
   author = {John Preskill},
   pages = {1-49},
   title = {Quantum computing 40 years later},
   url = {http://arxiv.org/abs/2106.10522},
   year = {2021},
}
@article{Kumar2013,
   abstract = {We propose a different methodology towards approaching a Maze problem. We convert the problem into a Quantum Search Problem (QSP), and its solutions are sought for using the iterative Grover's Search Algorithm. Though the category of mazes we are looking at are of the NP complete class, we have redirected such a NP complete problem into a QSP. Our solution deals with two dimensional perfect mazes with no closed loops. We encode all possible individual paths from the starting point of the maze into a quantum register. A quantum fitness operator applied on the register encodes each individual with its fitness value. We propose an oracle design which marks all the individuals above a certain fitness value and use the Grover search algorithm to find one of the marked states. Iterating over this method, we approach towards the optimum solution.},
   author = {Niraj Kumar and Debabrata Goswami},
   title = {Quantum Algorithm to Solve a Maze: Converting the Maze Problem into a Search Problem},
   url = {http://arxiv.org/abs/1312.4116},
   year = {2013},
}
@article{Smith2019,
   author = {Adam Smith},
   doi = {10.1038/s41534-019-0217-0},
   issn = {2056-6387},
   journal = {npj Quantum Information},
   pages = {1-13},
   publisher = {Springer US},
   title = {Simulating quantum many-body dynamics on a current digital quantum computer},
   url = {http://dx.doi.org/10.1038/s41534-019-0217-0},
   year = {2019},
}
@article{Wilen2021,
   abstract = {The central challenge in building a quantum computer is error correction. Unlike classical bits, which are susceptible to only one type of error, quantum bits (qubits) are susceptible to two types of error, corresponding to flips of the qubit state about the X and ZÂ directions. Although the Heisenberg uncertainty principle precludes simultaneous monitoring of X- and Z-flips on a single qubit, it is possible to encode quantum information in large arrays of entangled qubits that enable accurate monitoring of all errors in the system, provided that the error rate is low1. Another crucial requirement is that errors cannot be correlated. Here we characterize a superconducting multiqubit circuit and find that charge noise in the chip is highly correlated on a length scale over 600Â micrometres; moreover, discrete charge jumps are accompanied by a strong transient reduction of qubit energy relaxation time across the millimetre-scale chip. The resulting correlated errors are explained in terms of the charging event and phonon-mediated quasiparticle generation associated with absorption of Î³-rays and cosmic-ray muons in the qubit substrate. Robust quantum error correction will require the development of mitigation strategies to protect multiqubit arrays from correlated errors due to particle impacts.},
   author = {C. D. Wilen and S. Abdullah and N. A. Kurinsky and C. Stanford and L. Cardani and G. DâImperio and C. Tomei and L. Faoro and L. B. Ioffe and C. H. Liu and A. Opremcak and B. G. Christensen and J. L. DuBois and R. McDermott},
   doi = {10.1038/s41586-021-03557-5},
   issn = {14764687},
   issue = {7863},
   journal = {Nature},
   pages = {369-373},
   pmid = {34135523},
   title = {Correlated charge noise and relaxation errors in superconducting qubits},
   volume = {594},
   year = {2021},
}
@article{Cong2019,
   abstract = {Neural network-based machine learning has recently proven successful for many complex applications ranging from image recognition to precision medicine. However, its direct application to problems in quantum physics is challenging due to the exponential complexity of many-body systems. Motivated by recent advances in realizing quantum information processors, we introduce and analyse a quantum circuit-based algorithm inspired by convolutional neural networks, a highly effective model in machine learning. Our quantum convolutional neural network (QCNN) uses only O(log(N)) variational parameters for input sizes of N qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. To explicitly illustrate its capabilities, we show that QCNNs can accurately recognize quantum states associated with a one-dimensional symmetry-protected topological phase, with performance surpassing existing approaches. We further demonstrate that QCNNs can be used to devise a quantum error correction scheme optimized for a given, unknown error model that substantially outperforms known quantum codes of comparable complexity. The potential experimental realizations and generalizations of QCNNs are also discussed.},
   author = {Iris Cong and Soonwon Choi and Mikhail D. Lukin},
   doi = {10.1038/s41567-019-0648-8},
   issn = {17452481},
   issue = {12},
   journal = {Nature Physics},
   pages = {1273-1278},
   title = {Quantum convolutional neural networks},
   volume = {15},
   year = {2019},
}
@article{Wootton2020,
   abstract = {Quantum computation is an emerging technology that promises to be a powerful tool in many areas. Though some years likely still remain until significant quantum advantage is demonstrated, the development of the technology has led to a range of valuable resources. These include publicly available prototype quantum hardware, advanced simulators for small quantum programs and programming frameworks to test and develop quantum software. In this provocation paper we seek to demonstrate that these resources are sufficient to provide the first useful results in the field of procedural generation. This is done by introducing a proof-of-principle method: a quantum generalization of a blurring process, in which quantum interference is used to provide a unique effect. Through this we hope to show that further developments in the technology are not required before it becomes useful for procedural generation. Rather, fruitful experimentation with this new technology can begin now.},
   author = {James R. Wootton},
   doi = {10.1145/3402942.3409600},
   isbn = {9781450388078},
   journal = {ACM International Conference Proceeding Series},
   keywords = {procedural generation,quantum computing},
   title = {Procedural generation using quantum computation},
   year = {2020},
}
@article{Aizman2019,
   abstract = {Training deep learning (DL) models on petascale datasets is essential for achieving competitive and state-of-the-art performance in applications such as speech, video analytics, and object recognition. However, existing distributed filesystems were not developed for the access patterns and usability requirements of DL jobs. In this paper, we describe AIStore, a highly scalable, easy-to-deploy storage system, and WebDataset, a standards-based storage format and library that permits efficient access to very large datasets. We compare system performance experimentally using image classification workloads and storing training data on a variety of backends, including local SSDs, single-node NFS, and two identical bare-metal clusters: HDFS and AIStore.},
   author = {Alex Aizman and Gavin Maltby and Thomas Breuel},
   doi = {10.1109/BigData47090.2019.9005703},
   isbn = {9781728108582},
   journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
   keywords = {deep learning,performance,petascale,scale-out},
   pages = {5965-5967},
   title = {High Performance I/O for Large Scale Deep Learning},
   year = {2019},
}
@article{Anderson2021,
   abstract = {In this paper, we provide a deep dive into the deployment of inference accelerators at Facebook. Many of our ML workloads have unique characteristics, such as sparse memory accesses, large model sizes, as well as high compute, memory and network bandwidth requirements. We co-designed a high-performance, energy-efficient inference accelerator platform based on these requirements. We describe the inference accelerator platform ecosystem we developed and deployed at Facebook: both hardware, through Open Compute Platform (OCP), and software framework and tooling, through Pytorch/Caffe2/Glow. A characteristic of this ecosystem from the start is its openness to enable a variety of AI accelerators from different vendors. This platform, with six low-power accelerator cards alongside a single-socket host CPU, allows us to serve models of high complexity that cannot be easily or efficiently run on CPUs. We describe various performance optimizations, at both platform and accelerator level, which enables this platform to serve production traffic at Facebook. We also share deployment challenges, lessons learned during performance optimization, as well as provide guidance for future inference hardware co-design.},
   author = {Michael Anderson and Benny Chen and Stephen Chen and Summer Deng and Jordan Fix and Michael Gschwind and Aravind Kalaiah and Changkyu Kim and Jaewon Lee and Jason Liang and Haixin Liu and Yinghai Lu and Jack Montgomery and Arun Moorthy and Satish Nadathur and Sam Naghshineh and Avinash Nayak and Jongsoo Park and Chris Petersen and Martin Schatz and Narayanan Sundaram and Bangsheng Tang and Peter Tang and Amy Yang and Jiecao Yu and Hector Yuen and Ying Zhang and Aravind Anbudurai and Vandana Balan and Harsha Bojja and Joe Boyd and Matthew Breitbach and Claudio Caldato and Anna Calvo and Garret Catron and Sneh Chandwani and Panos Christeas and Brad Cottel and Brian Coutinho and Arun Dalli and Abhishek Dhanotia and Oniel Duncan and Roman Dzhabarov and Simon Elmir and Chunli Fu and Wenyin Fu and Michael Fulthorp and Adi Gangidi and Nick Gibson and Sean Gordon and Beatriz Padilla Hernandez and Daniel Ho and Yu-Cheng Huang and Olof Johansson and Shishir Juluri and Shobhit Kanaujia and Manali Kesarkar and Jonathan Killinger and Ben Kim and Rohan Kulkarni and Meghan Lele and Huayu Li and Huamin Li and Yueming Li and Cynthia Liu and Jerry Liu and Bert Maher and Chandra Mallipedi and Seema Mangla and Kiran Kumar Matam and Jubin Mehta and Shobhit Mehta and Christopher Mitchell and Bharath Muthiah and Nitin Nagarkatte and Ashwin Narasimha and Bernard Nguyen and Thiara Ortiz and Soumya Padmanabha and Deng Pan and Ashwin Poojary and Ye and Qi and Olivier Raginel and Dwarak Rajagopal and Tristan Rice and Craig Ross and Nadav Rotem and Scott Russ and Kushal Shah and Baohua Shan and Hao Shen and Pavan Shetty and Krish Skandakumaran and Kutta Srinivasan and Roshan Sumbaly and Michael Tauberg and Mor Tzur and Hao Wang and Man Wang and Ben Wei and Alex Xia and Chenyu Xu and Martin Yang and Kai Zhang and Ruoxi Zhang and Ming Zhao and Whitney Zhao and Rui Zhu and Lin Qiao and Misha Smelyanskiy and Bill Jia and Vijay Rao},
   title = {First-Generation Inference Accelerator Deployment at Facebook},
   url = {http://arxiv.org/abs/2107.04140},
   year = {2021},
}
@article{Jacot2018,
   abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit (12; 9), thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function fÎ¸ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function fÎ¸ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
   author = {Arthur Jacot and Franck Gabriel and ClÃ©ment Hongler},
   issn = {10495258},
   issue = {5},
   journal = {Advances in Neural Information Processing Systems},
   pages = {8571-8580},
   title = {Neural tangent kernel: Convergence and generalization in neural networks},
   volume = {2018-Decem},
   year = {2018},
}
@article{,
   abstract = {In many-particle problems involving interacting fermions or bosons, the most natural language for expressing the Hamiltonian, the observables, and the basis states is the language of the second-quantization operators. It thus appears advantageous to write numerical computer codes which allow the user to define the problem and the quantities of interest directly in terms of operator strings, rather than in some low-level programming language. Here I describe a Mathematica package which provides a flexible framework for performing the required translations between several different representations of operator expressions: condensed notation using pure ASCII character strings, traditional notation ("pretty printing"), internal Mathematica representation using nested lists (used for automatic symbolic manipulations), and various higher-level ("macro") expressions. The package consists of a collection of transformation rules that define the algebra of operators and a comprehensive library of utility functions. While the emphasis is given on the problems from solid-state and atomic physics, the package can be easily adapted to any given problem involving non-commuting operators. It can be used for educational and demonstration purposes, but also for direct calculations of problems of moderate size. Program summary: Program title: SNEG Catalogue identifier: AEJL-vl-0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ AEJL-vl-0.html Program obtainable from: CPC Program Library, Queens University, Belfast, N. Ireland Licensing provisions: GNU General Public License No. of lines in distributed program, including test data, etc.: 319 808 No. of bytes in distributed program, including test data, etc.: 1 081 247 Distribution format: tar.gz Programming language: Mathematica Computer: Any computer which runs Mathematica Operating system: Any OS which runs Mathematica RAM: Problem dependent Classification: 2.9, 5, 6.2 Nature of problem: Manipulation of expressions involving second-quantization operators and other non-commuting objects. Calculation of commutators, anticommutators, expectation values. Generation of matrix representations of the Hamiltonians expressed in the second-quantization language. Solution method: Automatic reordering of operator strings in some well specified canonical order; (anti)commutation rules are used where needed. States may be represented in occupation-number representation. Dirac bra-ket notation may be intermixed with non-commuting operator expressions. Restrictions: For very long operator strings, the brute-force automatic reordering becomes slow, but it can be turned off. In such cases, the expectation values may still be evaluated using Wicks theorem. Unusual features: SNEG provides the natural notation of second-quantization operators (dagger for creation operators, etc.) when used interactively using the Mathematica notebook interface. Running time: Problem dependent Â© 2011 Elsevier B.V. All rights reserved.},
   author = {Rok Å½itko},
   doi = {10.1016/j.cpc.2011.05.013},
   issn = {00104655},
   issue = {10},
   journal = {Computer Physics Communications},
   keywords = {Bra-ket notation,Occupation-number representation,Second-quantization operators,Symbolic manipulation,Wicks theorem},
   pages = {2259-2264},
   title = {SNEG - Mathematica package for symbolic calculations with second-quantization-operator expressions},
   volume = {182},
   year = {2011},
}
@article{Yang2017,
   abstract = {We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the "edge of chaos" hypothesis, these subexponential and polynomial laws allow residual networks to "hover over the boundary between stability and chaos," thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.},
   author = {Greg Yang and Samuel S. Schoenholz},
   issue = {Nips},
   title = {Mean Field Residual Networks: On the Edge of Chaos},
   url = {http://arxiv.org/abs/1712.08969},
   year = {2017},
}
@article{Gibney2019,
   author = {Elizabeth Gibney},
   doi = {10.1038/d41586-019-02935-4},
   issn = {14764687},
   issue = {7776},
   journal = {Nature},
   keywords = {Mathematics and computing,Quantum information,Quantum physics,Software},
   pages = {22-24},
   pmid = {31578480},
   title = {Quantum gold rush: the private funding pouring into quantum start-ups},
   volume = {574},
   year = {2019},
}
@article{Kratzer2019,
   abstract = {When density functional theory is used to describe the electronic structure of periodic systems, the application of Bloch's theorem to the Kohn-Sham wavefunctions greatly facilitates the calculations. In this paper of the series, the concepts needed to model infinite systems are introduced. These comprise the unit cell in real space, as well as its counterpart in reciprocal space, the Brillouin zone. Grids for sampling the Brillouin zone and finite k-point sets are discussed. For metallic systems, these tools need to be complemented by methods to determine the Fermi energy and the Fermi surface. Various schemes for broadening the distribution function around the Fermi energy are presented and the approximations involved are discussed. In order to obtain an interpretation of electronic structure calculations in terms of physics, the concepts of bandstructures and atom-projected and/or orbital-projected density of states are useful. Aspects of convergence with the number of basis functions and the number of k-points need to be addressed specifically for each physical property. The importance of this issue will be exemplified for force constant calculations and simulations of finite-temperature properties of materials. The methods developed for periodic systems carry over, with some reservations, to less symmetric situations by working with a supercell. The chapter closes with an outlook to the use of supercell calculations for surfaces and interfaces of crystals.},
   author = {Peter Kratzer and JÃ¶rg Neugebauer},
   doi = {10.3389/fchem.2019.00106},
   issn = {22962646},
   issue = {MAR},
   journal = {Frontiers in Chemistry},
   keywords = {Brillouin zone sampling,Convergence tests,Density functional theory,High-throughput calculations,Solid-state chemistry techniques,Supercell approach},
   pages = {1-18},
   title = {The basics of electronic structure theory for periodic systems},
   volume = {7},
   year = {2019},
}
@article{Yang2006,
   abstract = {A new direct constrained optimization algorithm for minimizing the Kohn-Sham (KS) total energy functional is presented in this paper. The key ingredients of this algorithm involve projecting the total energy functional into a sequence of subspaces of small dimensions and seeking the minimizer of total energy functional within each subspace. The minimizer of a subspace energy functional not only provides a search direction along which the KS total energy functional decreases but also gives an optimal "step-length" to move along this search direction. Numerical examples are provided to demonstrate that this new direct constrained optimization algorithm can be more efficient than the self-consistent field (SCF) iteration. Â© 2006 Elsevier Inc. All rights reserved.},
   author = {Chao Yang and Juan C. Meza and Lin Wang Wang},
   doi = {10.1016/j.jcp.2006.01.030},
   issn = {10902716},
   issue = {2},
   journal = {Journal of Computational Physics},
   keywords = {Constrained optimization,Electronic structure calculation,Nonlinear eigenvalue problems,Total energy minimization},
   pages = {709-721},
   title = {A constrained optimization algorithm for total energy minimization in electronic structure calculations},
   volume = {217},
   year = {2006},
}
@article{Luo2020,
   abstract = {Linear-scaling density functional theory (DFT) is an efficient method to describe the electronic structures of molecules, semiconductors, and insulators to avoid the high cubic-scaling cost in conventional DFT calculations. Here, we present a parallel implementation of linear-scaling density matrix trace correcting (TC) purification algorithm to solve the KohnâSham (KS) equations with the numerical atomic orbitals in the HONPAS package. Such a linear-scaling density matrix purification algorithm is based on the Kohn's nearsightedness principle, resulting in a sparse Hamiltonian matrix with localized basis sets in the DFT calculations. Therefore, sparse matrix multiplication is the most time-consuming step in the density matrix purification algorithm for linear-scaling DFT calculations. We propose to use the MPI_Allgather function for parallel programming to deal with the sparse matrix multiplication within the compressed sparse row (CSR) format, which can scale up to hundreds of processing cores on modern heterogeneous supercomputers. We demonstrate the computational accuracy and efficiency of this parallel density matrix purification algorithm by performing large-scale DFT calculations on boron nitrogen nanotubes containing tens of thousands of atoms.},
   author = {Zhaolong Luo and Xinming Qin and Lingyun Wan and Wei Hu and Jinlong Yang},
   doi = {10.3389/fchem.2020.589910},
   issn = {22962646},
   issue = {November},
   journal = {Frontiers in Chemistry},
   keywords = {density matrix purification algorithm,linear-scaling density functional theory,parallel implementation,sparse matrix multiplication,tens of thousands of atoms},
   pages = {1-10},
   title = {Parallel Implementation of Large-Scale Linear Scaling Density Functional Theory Calculations With Numerical Atomic Orbitals in HONPAS},
   volume = {8},
   year = {2020},
}
@article{Lin2019,
   abstract = {Kohn-Sham density functional theory (DFT) is the most widely used electronic structure theory. Despite significant progress in the past few decades, the numerical solution of Kohn-Sham DFT problems remains challenging, especially for large-scale systems. In this paper we review the basics as well as state-of-the-art numerical methods, and focus on the unique numerical challenges of DFT.},
   author = {Lin Lin and Jianfeng Lu and Lexing Ying},
   doi = {10.1017/S0962492919000047},
   issn = {14740508},
   journal = {Acta Numerica},
   pages = {405-539},
   title = {Numerical methods for Kohn-Sham density functional theory},
   volume = {28},
   year = {2019},
}
@article{Parkkinen2018,
   abstract = {Density functional theory within the Kohn-Sham density functional theory (KS-DFT) ansatz has been implemented into our bubbles and cube real-space molecular electronic structure framework, where functions containing steep cusps in the vicinity of the nuclei are expanded in atom-centered one-dimensional (1D) numerical grids multiplied with spherical harmonics (bubbles). The remainder, i.e., the cube, which is the cusp-free and smooth difference between the atomic one-center contributions and the exact molecular function, is represented on a three-dimensional (3D) equidistant grid by using a tractable number of grid points. The implementation of the methods is demonstrated by performing 3D numerical KS-DFT calculations on light atoms and small molecules. The accuracy is assessed by comparing the obtained energies with the best available reference energies.},
   author = {Pauli Parkkinen and Wen Hua Xu and Eelis Solala and Dage Sundholm},
   doi = {10.1021/acs.jctc.8b00456},
   issn = {15499626},
   issue = {8},
   journal = {Journal of Chemical Theory and Computation},
   pages = {4237-4245},
   pmid = {29944363},
   title = {Density Functional Theory under the Bubbles and Cube Numerical Framework},
   volume = {14},
   year = {2018},
}
@article{Jensen2018,
   abstract = {The inverse problem of KohnâSham density functional theory (DFT) is often solved in an effort to benchmark and design approximate exchange-correlation potentials. The forward and inverse problems of DFT rely on the same equations but the numerical methods for solving each problem are substantially different. We examine both problems in this tutorial with a special emphasis on the algorithms and error analysis needed for solving the inverse problem. Two inversion methods based on partial differential equation constrained optimization and constrained variational ideas are introduced. We compare and contrast several different inversion methods applied to one-dimensional finite and periodic model systems.},
   author = {Daniel S. Jensen and Adam Wasserman},
   doi = {10.1002/qua.25425},
   issn = {1097461X},
   issue = {1},
   journal = {International Journal of Quantum Chemistry},
   keywords = {PDE-constrained optimization,density functional theory,inverse problems},
   pages = {1-29},
   title = {Numerical methods for the inverse problem of density functional theory},
   volume = {118},
   year = {2018},
}
@article{Uhlir2017,
   author = {Adam Uhlir},
   title = {daGui : A DataFlow Graphical User Interface},
   year = {2017},
}
@article{,
   abstract = {This chapter starts with a quick overview to the Kohn-Sham ansatz outlining the general aspects of the first-principles methodology followed by an introduction to the relevant choice of the ge- ometrical models to simulate surfaces, and the choice of the appropriate electronic structure method. As an example, two electronic structure methods are introduced at a greater depth, which are the full-potential linearized augmented plane wave (FLAPW)-like methods to solve the Kohn-Sham equation for a periodic solid and surfaces, and the Korringa, Kohn and Ros- tocker (KKR) Green-function method as an example ofan Green-function method to cope with the surface geometry.},
   author = {Stefan Blugel},
   title = {Density Functional Theory in Practice},
   url = {https://www.fz-juelich.de/SharedDocs/Downloads/PGI/PGI-1/DE/sb_pdf.pdf?__blob=publicationFile},
}
@article{,
   title = {Visualizing Dataflow Graphs of Deep Learning Models in Tensorflow},
}
@article{Gidney2019,
   abstract = {We improve the space complexity of Karatsuba multiplication on a quantum computer from $O(n^\{1.427\})$ to $O(n)$ while maintaining $O(n^\{\lg 3\})$ gate complexity. We achieve this by ensuring recursive calls can add their outputs directly into subsections of the output register. This avoids the need to store, and uncompute, intermediate results. This optimization, which is analogous to classical tail-call optimization, should be applicable to a wide range of recursive quantum algorithms.},
   author = {Craig Gidney},
   pages = {1-11},
   title = {Asymptotically Efficient Quantum Karatsuba Multiplication},
   url = {http://arxiv.org/abs/1904.07356},
   year = {2019},
}
@article{Tuggener2020,
   abstract = {We present an extensive evaluation of a wide variety of promising design patterns for automated deep-learning (AutoDL) methods, organized according to the problem categories of the 2019 AutoDL challenges, which set the task of optimizing both model accuracy and search efficiency under tight time and computing constraints. We propose structured empirical evaluations as the most promising avenue to obtain design principles for deep-learning systems due to the absence of strong theoretical support. From these evaluations, we distill relevant patterns which give rise to neural network design recommendations. In particular, we establish (a) that very wide fully connected layers learn meaningful features faster; we illustrate (b) how the lack of pretraining in audio processing can be compensated by architecture search; we show (c) that in text processing deep-learning-based methods only pull ahead of traditional methods for short text lengths with less than a thousand characters under tight resource limitations; and lastly we present (d) evidence that in very data- and computing-constrained settings, hyperparameter tuning of more traditional machine-learning methods outperforms deep-learning systems.},
   author = {Lukas Tuggener and Mohammadreza Amirian and Fernando Benites and Pius von DÃ¤niken and Prakhar Gupta and Frank-Peter Schilling and Thilo Stadelmann},
   doi = {10.3390/ai1040031},
   issn = {0004-3702},
   issue = {4},
   journal = {Ai},
   keywords = {architecture design,audio processing,automated machine learning,computer vision,natural language processing,weakly supervised learning},
   pages = {510-538},
   title = {Design Patterns for Resource-Constrained Automated Deep-Learning Methods},
   volume = {1},
   year = {2020},
}
@article{Washizaki2020,
   abstract = {Researchers and practitioners studying best practices strive to design Machine Learning (ML) application systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. In this paper, software-engineering architecture and design (anti-)patterns for ML application systems are analyzed to bridge the gap between traditional software systems and ML application systems with respect to architecture and design. Specifically, a systematic literature review confirms that ML application systems are popular due to the promotion of artificial intelligence. We identified 32 scholarly documents and 48 gray documents out of which 38 documents discuss 33 patterns: 12 architecture patterns, 13 design patterns, and 8 anti-patterns. Additionally, a survey of developers reveals that there are 7 major architecture patterns and 5 major design patterns. Then the relationships among patterns are identified in a pattern map.},
   author = {Hironori Washizaki and Hiromu Uchida and Foutse Khomh and Yann-GaÃ«l GuÃ©hÃ©neuc},
   journal = {Arxiv Preprint},
   pages = {1-8},
   title = {Machine Learning Architecture and Design Patterns},
   url = {http://www.washi.cs.waseda.ac.jp/wp-content/uploads/2019/12/IEEE_Software_19__ML_Patterns.pdf},
   year = {2020},
}
@article{Vayghan2019,
   abstract = {The move towards the microservice based architecture is well underway. In this architectural style, small and loosely coupled modules are developed, deployed, and scaled independently to compose cloud-native applications. However, for carrier-grade service providers to migrate to the microservices architectural style, availability remains a concern. Kubernetes is an open source platform that defines a set of building blocks which collectively provide mechanisms for deploying, maintaining, scaling, and healing containerized microservices. Thus, Kubernetes hides the complexity of microservice orchestration while managing their availability. In a preliminary work we evaluated Kubernetes, using its default configuration, from the availability perspective in a private cloud settings. In this paper, we investigate more architectures and conduct more experiments to evaluate the availability that Kubernetes delivers for its managed microservices. We present different architectures for public and private clouds. We evaluate the availability achievable through the healing capability of Kubernetes. We investigate the impact of adding redundancy on the availability of microservice based applications. We conduct experiments under the default configuration of Kubernetes as well as under its most responsive one. We also perform a comparative evaluation with the Availability Management Framework (AMF), which is a proven solution as a middleware service for managing high-availability. The results of our investigations show that in certain cases, the service outage for applications managed with Kubernetes is significantly high.},
   author = {Leila Abdollahi Vayghan and Mohamed Aymen Saied and Maria Toeroe and Ferhat Khendek},
   keywords = {automated deployment machinery,availability,by fully,containers,deployed and scaled independently,docker,failure,kubernetes,microservices,orchestration,services that can be,with minimum centralized},
   title = {Kubernetes as an Availability Manager for Microservice Applications},
   url = {http://arxiv.org/abs/1901.04946},
   year = {2019},
}
@article{Schultz2018,
   abstract = {Distributed databases offer high availability but can impose high costs on client applications in order to maintain strong consistency at all times. MongoDB is a document oriented database whose replication system provides a variety of consistency levels allowing client applications to select the trade-offs they want to make when it comes to consistency and latency, at a per operation level. In this paper we discuss the tunable consistency models in MongoDB replication and their utility for application developers. We discuss how the MongoDB replication system's speculative execution model and data rollback protocol help make this spectrum of consistency levels possible. We also present case studies of how these consistency levels are used in real world applications, along with a characterization of their performance beneffts and trade-offs.},
   author = {William Schultz and Tess Avitabile and Alyson Cabral},
   doi = {10.14778/3352063.3352125},
   issn = {21508097},
   issue = {12},
   journal = {Proceedings of the VLDB Endowment},
   pages = {2071-2081},
   title = {Tunable consistency in MongoDB},
   volume = {12},
   year = {2018},
}
@article{,
   author = {An Analysis and Mongodb Security and Matthew Trudeau and Joshua Kolodny},
   title = {An ââAnalysis ââand ââOverview ââof ââMongoDB Security},
}
@article{DeCandia2007,
   abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an "always-on" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use. Copyright 2007 ACM.},
   author = {Giuseppe DeCandia and Deniz Hastorun and Madan Jampani and Gunavardhan Kakulapati and Avinash Lakshman and Alex Pilchin and Swaminathan Sivasubramanian and Peter Vosshall and Werner Vogels},
   doi = {10.1145/1294261.1294281},
   issn = {01635980},
   journal = {Operating Systems Review (ACM)},
   keywords = {Performance,Reliability,Scalability},
   pages = {205-220},
   title = {Dynamo: Amazon's highly available key-value store},
   year = {2007},
}
@article{Paksula2010,
   abstract = {In this paper an approach to persist objects in Redis key-value database is provided. Pattern focuses on the consistency of objects under race and failure conditions. Proposed pattern is useful for cloud storage implementations that require speed and scalability. This approach can optionally be implemented with schemaless data model. Implementation can be done with any language, although the examples are presented with Ruby. Short overview on cloud architecture design is given as setting the context for using this persistence pattern},
   author = {Matti Paksula},
   journal = {Science},
   keywords = {-nosql,acid,architectures,base,cloud,is evaluated and examples,key-value databases,object peristance,of,persistence,ruby on rails,ruby on rails object,scalability,the last part implementation,usage with activeredis},
   pages = {6},
   title = {Persisting Objects in Redis Key-Value Database},
   year = {2010},
}
@article{Services2017,
   author = {Amazon Web Services},
   issue = {May},
   title = {Database Caching Strategies Using Redis},
   year = {2017},
}
@article{Chang2006,
   abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
   author = {Fay Chang and Jeffrey Dean and Sanjay Ghemawat and Wilson C. Hsieh and Deborah A. Wallach and Mike Burrows and Tushar Chandra and Andrew Fikes and Robert E. Gruber},
   journal = {OSDI 2006 - 7th USENIX Symposium on Operating Systems Design and Implementation},
   pages = {205-218},
   title = {BigTable: A distributed storage system for structured data},
   year = {2006},
}
@article{Bommasani2021,
   abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on conventional deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
   author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher RÃ© and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian TramÃ¨r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
   pages = {1-212},
   title = {On the Opportunities and Risks of Foundation Models},
   url = {http://arxiv.org/abs/2108.07258},
   year = {2021},
}
@book{,
   issue = {1},
   journal = {Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03Ã¢â¢ï¬05, 2018, Woodstock, NY},
   publisher = {Association for Computing Machinery},
   title = {OneFlow : Redesign the Distributed Deep Learning Framework from Scratch},
   volume = {1},
}
@book{Nalchigar2019,
   abstract = {Despite the hype around machine learning (ML), many organizations are struggling to derive business value from ML capabilities. Design patterns have long been used in software engineering to enhance design effectiveness and to speed up the development process. The contribution of this paper is two-fold. First, it introduces solution patterns as an explicit way of representing generic and well-proven ML designs for commonly-known and recurring business analytics problems. Second, it reports on the feasibility, expressiveness, and usefulness of solution patterns for ML, in collaboration with an industry partner. It provides a prototype architecture for supporting the use of solution patterns in real world scenarios. It presents a proof-of-concept implementation of the architecture and illustrates its feasibility. Findings from the collaboration suggest that solution patterns can have a positive impact on ML design and development efforts.},
   author = {Soroosh Nalchigar and Eric Yu and Yazan Obeidi and Sebastian Carbajales and John Green and Allen Chan},
   doi = {10.1007/978-3-030-21290-2_39},
   isbn = {9783030212896},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Advanced analytics,Business analytics,Conceptual modeling,Design patterns,Machine learning},
   pages = {627-642},
   publisher = {Springer International Publishing},
   title = {Solution Patterns for Machine Learning},
   volume = {11483 LNCS},
   url = {http://dx.doi.org/10.1007/978-3-030-21290-2_39},
   year = {2019},
}
@article{Rucci2005,
   abstract = {Under natural viewing conditions, small movements of the eye, head and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several seconds. However, it is unclear whether the physiological motion of the retinal image serves a visual purpose during the brief periods of natural visual fixation. This study examines the impact of fixational instability on the statistics of the visual input to the retina and on the structure of neural activity in the early visual system. We show that fixational instability introduces a component in the retinal input signals that, in the presence of natural images, lacks spatial correlations. This component strongly influences neural activity in a model of the LGN. It decorrelates cell responses even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity at the early stages of the visual system has been proposed to be beneficial for discarding statistical redundancies in the input signals. The results of this study suggest that fixational instability might contribute to the establishment of efficient representations of natural stimuli. Â© 2005 Taylor & Francis.},
   author = {Michele Rucci and Antonino Casile},
   doi = {10.1080/09548980500300507},
   issn = {0954898X},
   issue = {2-3},
   journal = {Network: Computation in Neural Systems},
   keywords = {Drift,Eye movement,Lateral geniculate nucleus,Redundancy reduction,Retina,Saccade,Visual fixation},
   pages = {121-138},
   pmid = {16411492},
   title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
   volume = {16},
   year = {2005},
}
@article{Agrawal2019,
   abstract = {TensorFlow Eager is a multi-stage, Python-embedded domain-specific language for hardware-accelerated machine learning, suitable for both interactive research and production. TensorFlow, which TensorFlow Eager extends, requires users to represent computations as dataflow graphs; this permits compiler optimizations and simplifies deployment but hinders rapid prototyping and run-time dynamism. TensorFlow Eager eliminates these usability costs without sacrificing the benefits furnished by graphs: It provides an imperative front-end to TensorFlow that executes operations immediately and a JIT tracer that translates Python functions composed of TensorFlow operations into executable dataflow graphs. TensorFlow Eager thus offers a multi-stage programming model that makes it easy to interpolate between imperative and staged execution in a single package.},
   author = {Akshay Agrawal and Akshay Naresh Modi and Alexandre Passos and Allen Lavoie and Ashish Agarwal and Asim Shankar and Igor Ganichev and Josh Levenberg and Mingsheng Hong and Rajat Monga and Shanqing Cai},
   title = {TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning},
   url = {http://arxiv.org/abs/1903.01855},
   year = {2019},
}
@article{Frostig2018,
   abstract = {We describe JAX, a domain-specific tracing JIT compiler for gen-erating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subrou-tines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward-and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured con-trol flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily pro-grammable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
   author = {Roy Frostig and Matthew James Johnson and Chris Leary},
   title = {Compiling machine learning programs via high-level tracing: JAX},
   year = {2018},
}
@article{,
   author = {Heng-tze Cheng and Levent Koc and Jeremiah Harmsen and Tal Shaked and Tushar Chandra and Hrishi Aradhye and Glen Anderson and Greg Corrado and Wei Chai and Mustafa Ispir and Rohan Anil and Zakaria Haque and Lichan Hong and Vihan Jain and Xiaobing Liu and Hemal Shah},
   keywords = {deep learning,recommender systems,wide},
   title = {Wide & Deep Learning for Recommender Systems},
}
@article{Covington2016,
   abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
   author = {Paul Covington and Jay Adams and Emre Sargin},
   doi = {10.1145/2959100.2959190},
   journal = {RecSys 2016 - Proceedings of the 10th ACM Conference on Recommender Systems},
   keywords = {Deep learning,Recommender system,Scalability},
   pages = {191-198},
   title = {Deep neural networks for youtube recommendations},
   year = {2016},
}
@article{Zhao2019,
   abstract = {In this paper, we introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking objectives, as well as implicit selection biases in user feedback. To tackle these challenges, we explored a variety of soft-parameter sharing techniques such as Multi-gate Mixture-of-Experts so as to efciently optimize for multiple ranking objectives. Additionally, we mitigated the selection biases by adopting a Wide & Deep framework. We demonstrated that our proposed techniques can lead to substantial improvements on recommendation quality on one of the world's largest video sharing platforms.},
   author = {Zhe Zhao and Lichan Hong and Li Wei and Jilin Chen and Aniruddh Nath and Shawn Andrews and Aditee Kumthekar and Maheswaran Sathiamoorthy and Xinyang Yi and Ed Chi},
   doi = {10.1145/3298689.3346997},
   journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
   keywords = {Multitask Learning,Recommendation and Ranking,Selection Bias},
   pages = {43-51},
   title = {Recommending what video to watch next: A multitask ranking system},
   year = {2019},
}
@article{Yi2019,
   abstract = {Many recommendation systems retrieve and score items from a very large corpus. A common recipe to handle data sparsity and power-law item distribution is to learn item representations from its content features. Apart from many content-aware systems based on matrix factorization, we consider a modeling framework using two-tower neural net, with one of the towers (item tower) encoding a wide variety of item content features. A general recipe of training such two-tower models is to optimize loss functions calculated from in-batch negatives, which are items sampled from a random mini-batch. However, in-batch loss is subject to sampling biases, potentially hurting model performance, particularly in the case of highly skewed distribution. In this paper, we present a novel algorithm for estimating item frequency from streaming data. Through theoretical analysis and simulation, we show that the proposed algorithm can work without requiring fxed item vocabulary, and is capable of producing unbiased estimation and being adaptive to item distribution change. We then apply the sampling-bias-corrected modeling approach to build a large scale neural retrieval system for YouTube recommendations. The system is deployed to retrieve personalized suggestions from a corpus with tens of millions of videos. We demonstrate the efectiveness of sampling-bias correction through ofine experiments on two real-world datasets. We also conduct live A/B testings to show that the neural retrieval system leads to improved recommendation quality for YouTube.},
   author = {Xinyang Yi and Ji Yang and Lichan Hong and Derek Zhiyuan Cheng and Lukasz Heldt and Aditee Kumthekar and Zhe Zhao and Li Wei and Ed Chi},
   doi = {10.1145/3298689.3346996},
   journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
   keywords = {Information Retrieval,Neural Networks,Recommender systems},
   pages = {269-277},
   title = {Sampling-bias-corrected neural modeling for large corpus item recommendations},
   year = {2019},
}
@article{Bengio2008,
   abstract = {Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems. Â© 2008 IEEE.},
   author = {Yoshua Bengio and Jean SÃ©bastien SenÃ©cal},
   doi = {10.1109/TNN.2007.912312},
   issn = {10459227},
   issue = {4},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Energy-based models,Fast training,Importance sampling,Language modeling,Monte Carlo methods,Probabilistic neural networks},
   pages = {713-722},
   pmid = {18390314},
   title = {Adaptive importance sampling to accelerate training of a neural probabilistic language model},
   volume = {19},
   year = {2008},
}
@article{Zaharia2018,
   abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
   author = {Matei Zaharia and Andrew Chen and Aaron Davidson and Ali Ghodsi and Sue Ann Hong and Andy Konwinski and Siddharth Murching and Tomas Nykodym and Paul Ogilvie and Mani Parkhe and Fen Xie and Corey Zumar},
   journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
   pages = {39-45},
   title = {Accelerating the Machine Learning Lifecycle with MLflow},
   year = {2018},
}
@article{Chen2020,
   abstract = {MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments.},
   author = {Andrew Chen and Andy Chow and Aaron Davidson and Arjun DCunha and Ali Ghodsi and Sue Ann Hong and Andy Konwinski and Clemens Mewald and Siddharth Murching and Tomas Nykodym and Paul Ogilvie and Mani Parkhe and Avesh Singh and Fen Xie and Matei Zaharia and Richard Zang and Juntai Zheng and Corey Zumar},
   doi = {10.1145/3399579.3399867},
   pages = {1-4},
   title = {Developments in MLflow},
   year = {2020},
}
@article{Databricks2020,
   author = {Databricks},
   title = {Standardizing the Machine Learning Lifecycle From experimentation to production with MLflow},
   year = {2020},
}
@article{Molino2019,
   abstract = {In this work we present Ludwig, a flexible, extensible and easy to use toolbox which allows users to train deep learning models and use them for obtaining predictions without writing code. Ludwig implements a novel approach to deep learning model building based on two main abstractions: data types and declarative configuration files. The data type abstraction allows for easier code and sub-model reuse, and the standardized interfaces imposed by this abstraction allow for encapsulation and make the code easy to extend. Declarative model definition configuration files enable inexperienced users to obtain effective models and increase the productivity of expert users. Alongside these two innovations, Ludwig introduces a general modularized deep learning architecture called Encoder-Combiner-Decoder that can be instantiated to perform a vast amount of machine learning tasks. These innovations make it possible for engineers, scientists from other fields and, in general, a much broader audience to adopt deep learning models for their tasks, concretely helping in its democratization.},
   author = {Piero Molino and Yaroslav Dudin and Sai Sumanth Miryala},
   title = {Ludwig: a type-based declarative deep learning toolbox},
   url = {http://arxiv.org/abs/1909.07930},
   year = {2019},
}
@article{Wang2020,
   abstract = {We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of 443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.},
   author = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
   pages = {13029-13038},
   title = {Scaled-YOLOv4: Scaling Cross Stage Partial Network},
   url = {http://arxiv.org/abs/2011.08036},
   year = {2020},
}
@article{Reuther2018,
   abstract = {In the rapidly expanding field of parallel processing, job schedulers are the âoperating systemsâ of modern big data architectures and supercomputing systems. Job schedulers allocate computing resources and control the execution of processes on those resources. Historically, job schedulers were the domain of supercomputers, and job schedulers were designed to run massive, long-running computations over days and weeks. More recently, big data workloads have created a need for a new class of computations consisting of many short computations taking seconds or minutes that process enormous quantities of data. For both supercomputers and big data systems, the efficiency of the job scheduler represents a fundamental limit on the efficiency of the system. Detailed measurement and modeling of the performance of schedulers are critical for maximizing the performance of a large-scale computing system. This paper presents a detailed feature analysis of 15 supercomputing and big data schedulers. For big data workloads, the scheduler latency is the most important performance characteristic of the scheduler. A theoretical model of the latency of these schedulers is developed and used to design experiments targeted at measuring scheduler latency. Detailed benchmarking of four of the most popular schedulers (Slurm, Son of Grid Engine, Mesos, and Hadoop YARN) is conducted. The theoretical model is compared with data and demonstrates that scheduler performance can be characterized by two key parameters: the marginal latency of the scheduler ts and a nonlinear exponent Î±s. For all four schedulers, the utilization of the computing system decreases to <10% for computations lasting only a few seconds. Multi-level schedulers (such as LLMapReduce) that transparently aggregate short computations can improve utilization for these short computations to >90% for all four of the schedulers that were tested.},
   author = {Albert Reuther and Chansup Byun and William Arcand and David Bestor and Bill Bergeron and Matthew Hubbell and Michael Jones and Peter Michaleas and Andrew Prout and Antonio Rosa and Jeremy Kepner},
   doi = {10.1016/j.jpdc.2017.06.009},
   issn = {07437315},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Data analytics,High performance computing,Job scheduler,Resource manager,Scheduler},
   pages = {76-92},
   title = {Scalable system scheduling for HPC and big data},
   volume = {111},
   year = {2018},
}
@article{,
   title = {Kubeflow++ Building an Open Source Data Science Platform},
}
@article{Golubovic2021,
   abstract = {Machine Learning (ML) has been growing in popularity in multiple areas and groups at CERN, covering fast simulation, tracking, anomaly detection, among many others. We describe a new service available at CERN, based on Kubeflow and managing the full ML lifecycle: data preparation and interactive analysis, large scale distributed model training and model serving. We cover specific features available for hyper-parameter tuning and model metadata management, as well as infrastructure details to integrate accelerators and external resources. We also present results and a cost evaluation from scaling out a popular ML use case using public cloud resources, achieving close to linear scaling when using a large number of GPUs.},
   author = {Dejan Golubovic and Ricardo Rocha},
   doi = {10.1051/epjconf/202125102067},
   journal = {EPJ Web of Conferences},
   pages = {02067},
   title = {Training and Serving ML workloads with Kubeflow at CERN},
   volume = {251},
   year = {2021},
}
@article{,
   author = {Vangelis Koukis},
   title = {Scalable ML Workflows with Advanced Data Management on Kubeflow},
}
@article{,
   author = {Sina Chavoshi},
   title = {Hybrid Machine Learning with the Kubeflow Pipelines and RAPIDS},
}
@article{,
   author = {Kaz Sato and Developer Advocate and Google Cloud},
   title = {ML Ops and Kubeflow Pipelines Kaz Sato},
}
@article{Mainali2020,
   abstract = {Data collection and analysis approaches have changed drastically in the past few years. The reason behind adopting different approach is improved data availability and continuous change in analysis requirements. Data have been always there, but data management is vital nowadays due to rapid generation and availability of various formats. Big data has opened the possibility of dealing with potentially infinite amounts of data with numerous formats in a short time. The data analytics is becoming complex due to data characteristics, sophisticated tools and technologies, changing business needs, varied interests among stakeholders, and lack of a standardized process. DataOps is an emerging approach advocated by data practitioners to cater to the challenges in data analytics projects. Data analytics projects differ from software engineering in many aspects. DevOps is proven to be an efficient and practical approach to deliver the project in the Software Industry. However, DataOps is still in its infancy, being recognized as an independent and essential task data analytics. In this thesis paper, we uncover DataOps as a methodology to implement data pipelines by conducting a systematic search of research papers. As a result, we define DataOps outlining ambiguities and challenges. We also explore the coverage of DataOps to different stages of the data lifecycle. We created comparison matrixes of different tools and technologies categorizing them in different functional groups to demonstrate their usage in data lifecycle management. We followed DataOps implementation guidelines to implement data pipeline using Apache Airflow as workflow orchestrator inside Docker and compared with simple manual execution of a data analytics project. As per evaluation, the data pipeline with DataOps provided automation in task execution, orchestration in execution environment, testing and monitoring, communication and collaboration, and reduced end-to-end product delivery cycle time along with the reduction in pipeline execution time.},
   author = {Kiran Mainali},
   journal = {Trita-Eecs-Ex Nv - 2020:895},
   keywords = {Computer and Information Sciences,Data analytics,Data lifecycle,Data pipeline,Data- och informationsvetenskap,DataOps,DataOps pipeline,DataOps tools and technologies,DataOps tools and technology},
   pages = {97},
   title = {DataOps : Towards Understanding and Defining Data Analytics Approach},
   year = {2020},
}
@article{Bochkovskiy2020,
   abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
   author = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
   issn = {2331-8422},
   title = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
   url = {http://arxiv.org/abs/2004.10934},
   year = {2020},
}
@article{Atwal2020,
   abstract = {Technology is deliberately left until the final chapter because while it is essential, it is less critical than people, culture, and processes. If tools were all it took to be successful, then Silicon Valley giants would not open-source their crown jewels such as Kubernetes, TensorFlow, Apache Kafka, and Apache Airflow.},
   author = {Harvinder Atwal},
   doi = {10.1007/978-1-4842-5104-1},
   isbn = {9781484251034},
   journal = {Practical DataOps},
   title = {Practical DataOps},
   year = {2020},
}
@article{,
   abstract = {We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.},
   author = {Christopher RÃ© and Feng Niu and Pallavi Gudipati and Charles Srisuwananukorn},
   pages = {1-13},
   title = {Overton: A Data System for Monitoring and Improving Machine-Learned Products},
   url = {http://arxiv.org/abs/1909.05372},
   year = {2019},
}
@article{Molino2021,
   abstract = {The people training and using ML models now are typically experienced developers with years of study working within large organizations, but the next wave of ML systems should allow a substantially larger number of people, potentially without any coding skills, to perform the same tasks. These new ML systems will not require users to fully understand all the details of how models are trained and used for obtaining predictions, but will provide them a more abstract interface that is less demanding and more familiar. Declarative interfaces are well-suited for this goal, by hiding complexity and favoring separation of interest, and ultimately leading to increased productivity.},
   author = {Piero Molino and Christopher RÃ©},
   doi = {10.1145/3475965.3479315},
   issn = {1542-7730},
   issue = {3},
   journal = {Queue},
   pages = {46-76},
   title = {Declarative Machine Learning Systems},
   volume = {19},
   year = {2021},
}
@article{Gog2016,
   abstract = {Centralized datacenter schedulers can make high-quality placement decisions when scheduling tasks in a cluster. Today, however, high-quality placements come at the cost of high latency at scale, which degrades response time for interactive tasks and reduces cluster utilization. This paper describes Firmament, a centralized scheduler that scales to over ten thousand machines at subsecond placement latency even though it continuously reschedules all tasks via a min-cost max-flow (MCMF) optimization. Firmament achieves low latency by using multiple MCMF algorithms, by solving the problem incrementally, and via problem-specific optimizations. Experiments with a Google workload trace from a 12,500-machine cluster show that Firmament improves placement latency by 20Ã over Quincy [22], a prior centralized scheduler using the same MCMF optimization. Moreover, even though Firmament is centralized, it matches the placement latency of distributed schedulers for workloads of short tasks. Finally, Firmament exceeds the placement quality of four widely-used centralized and distributed schedulers on a real-world cluster, and hence improves batch task response time by 6Ã.},
   author = {Ionel Gog and Malte Schwarzkop and Adam Gleave and Robert N.M. Watson and Steven Hand},
   isbn = {9781931971331},
   journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
   pages = {99-115},
   title = {Firmament: Fast, centralized cluster scheduling at scale},
   year = {2016},
}
@article{Lagunas2021,
   abstract = {Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size.},
   author = {FranÃ§ois Lagunas and Ella Charlaix and Victor Sanh and Alexander M. Rush},
   title = {Block Pruning For Faster Transformers},
   url = {http://arxiv.org/abs/2109.04838},
   year = {2021},
}
@article{Chang2021,
   author = {Jonathan Chang and Matteo Manica and Rachel Bawden and Antoine Chaffin},
   title = {MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION},
   year = {2021},
}
@article{Rubanova2019,
   abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
   author = {Yulia Rubanova and Ricky T.Q. Chen and David Duvenaud},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {Latent ODEs for irregularly-sampled time series},
   volume = {32},
   year = {2019},
}
@article{Hu2020,
   abstract = {To derive the hidden dynamics from observed data is one of the fundamental but also challenging problems in many different fields. In this study, we propose a new type of interpretable network called the ordinary differential equation network (ODENet), in which the numerical integration of explicit ordinary differential equations (ODEs) are embedded into the machine learning scheme to build a general framework for revealing the hidden dynamics buried in massive time-series data efficiently and reliably. ODENet takes full advantage of both machine learning algorithms and ODE modeling. On one hand, the embedding of ODEs makes the framework more interpretable benefiting from the mature theories of ODEs. On the other hand, the schemes of machine learning enable data handling, paralleling, and optimization to be easily and efficiently implemented. From classical Lotka-Volterra equations to chaotic Lorenz equations, the ODENet exhibits its remarkable capability in handling time-series data even in the presence of large noise. We further apply the ODENet to real actin aggregation data, which shows an impressive performance as well. These results demonstrate the superiority of ODENet in dealing with noisy data, data with either non-equal spacing or large sampling time steps over other traditional machine learning algorithms.},
   author = {Pipi Hu and Wuyue Yang and Yi Zhu and Liu Hong},
   keywords = {odenet,ordinary differential equations,symbolic regression,time-series data},
   pages = {1-17},
   title = {Revealing hidden dynamics from time-series data by ODENet},
   url = {http://arxiv.org/abs/2005.04849},
   year = {2020},
}
@article{Liu2021,
   abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf\{S\}hifted \textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url\{https://github.com/microsoft/Swin-Transformer\}.},
   author = {Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
   title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
   url = {http://arxiv.org/abs/2103.14030},
   year = {2021},
}
@article{Liu2021,
   abstract = {The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.},
   author = {Ze Liu and Jia Ning and Yue Cao and Yixuan Wei and Zheng Zhang and Stephen Lin and Han Hu},
   pages = {1-12},
   title = {Video Swin Transformer},
   url = {http://arxiv.org/abs/2106.13230},
   year = {2021},
}
@article{,
   author = {K R Jayaram and Vinod Muthusamy and Parijat Dube and Vatche Ishakian and Chen Wang and Benjamin Herta and Scott Boag and Diana Arroyo and Asser Tantawi and Archit Verma and Falk Pollok and Rania Khalaf},
   keywords = {acm reference format,cluster,deep learning platform,fault tolerance,gang scheduling,jayaram,k,parijat dube,r,scheduling,vatche ishakian,vinod muthusamy},
   title = {FfDL : A Flexible Multi-tenant Deep Learning Platform},
}
@article{Delimitrou2014,
   abstract = {With the ongoing rise and phenomenal success of machine learning (ML), particu-larly deep learning, efficient training of large neural network models in scalable cloud infrastructures becomes a priority. ML workloads have traditionally been run in high-performance computing (HPC) environments, where users log in to dedi-cated machines and utilize the attached GPUs to run jobs that train models on huge datasets. Providing a similar user experience in a multi-tenant cloud environment comes with its own unique challenges regarding fault tolerance, performance, and security. We tackle these challenges and present a deep learning stack specifically designed for on-demand cloud environments. Based on a detailed discussion of the system architecture, we examine real usage data from internal users, and discuss performance experiments that illustrate the scalability of the system.},
   author = {Christina Delimitrou and Christos Kozyrakis and Michael Isard and Vijayan Prabhakaran and Jon Currey and Udi Wieder and Kunal Talwar and Andrew Goldberg and Marcelo Amaral and JordÃ  Polo and David Carrera and Seetharami Seelam and Malgorzata Steinder and Tobias Domhan and Jost Tobias Springenberg and Frank Hutter and Robert Grandl and Ganesh Ananthanarayanan and Srikanth Kandula and Sriram Rao and Aditya Akella and Christina Delimitrou and Christos Kozyrakis and Malte Schwarzkopf and Andy Konwinski and Daniel Golovin and Benjamin Solnik and Subhodeep Moitra and Greg Kochanski and John Karro and D Sculley and Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar and Scott Boag and Parijat Dube and Benjamin Herta and Waldemar Hummer and Vatche Ishakian and Jayaram K. R. and Michael Kalantar and Vinod Muthusamy and Priya Nagpurkar and Florian Rosenberg},
   isbn = {9781450348874},
   issn = {0163-5964},
   issue = {1},
   journal = {Proceedings of the 2014 ACM conference on SIGCOMM - SIGCOMM '14},
   keywords = {-  Computer systems organization  ->  Cloud comput,-  Theory of computation  ->  Scheduling algorithm,Automated Stopping,Bayesian Optimization,Black-Box Optimization,Gaussian Processes,Graph algorithms analysis,Hyperparameters,Machine learning theory,Transfer Learning,acm reference format,algorithms,arms,bandits with infinitely many,cloud computing,cluster management,cluster schedulers,cluster scheduling,comple-,datacenter,datacenters,deep learning,design,gpu,heterogeneity,hyperparameter optimization,interference,learning,makespan,model selection,multi-dimensional packing,multi-gpu,online optimization,optimistic concurrency con-,performance,performance analysis,placement,process management,qos,quality of service,resource allocation and assignment,resource contention,resource efficiency,scheduling,workload interference and deep},
   pages = {261-276},
   pmid = {1951794},
   title = {Scalable Multi-Framework Multi-Tenant Lifecycle Management of Deep Learning Training Jobs},
   volume = {42},
   url = {http://arxiv.org/abs/1603.06560%0Ahttps://static.googleusercontent.com/media/research.google.com/iw//pubs/archive/46180.pdf%0Ahttp://dl.acm.org/citation.cfm?id=2465386%0Ahttp://doi.acm.org/10.1145/2451116.2451125%0Ahttp://dl.acm.org/citation.cfm?doid=2619},
   year = {2014},
}
@article{,
   abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
   author = {Martin Zinkevich This and Style Guide},
   title = {Rules of Machine Learning : Best Practices for ML Engineering},
}
@article{Qiao2021,
   author = {Aurick Qiao and Sang Keun Choe and Suhas Jayaram Subramanya and Willie Neiswanger and U C Berkeley and Gregory R Ganger and Eric P Xing and Gregory R Ganger},
   journal = {Osdi'21},
   title = {Pollux : Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning},
   year = {2021},
}
@article{Ratner2017,
   abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train stateof- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8 Ã faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8Ã speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.},
   author = {Alexander Ratner and Stephen H. Bach and Henry Ehrenberg and Jason Fries and Sen Wu and Christopher RÃ©},
   doi = {10.14778/3157794.3157797},
   issn = {21508097},
   issue = {3},
   journal = {Proceedings of the VLDB Endowment},
   pages = {269-282},
   title = {Snorkel: Rapid training data creation with weak supervision},
   volume = {11},
   year = {2017},
}
@article{Chen2018,
   abstract = {Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing tech-niques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hy- perparameter a. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.},
   author = {Zhao Chen and Vijay Badrinarayanan and Chen Yu Lee and Andrew Rabinovich},
   journal = {35th International Conference on Machine Learning, ICML 2018},
   pages = {1240-1251},
   title = {GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
   volume = {2},
   year = {2018},
}
@article{Huang2015,
   abstract = {Existing ABR algorithms face a significant challenge in estimating future capacity: capacity can vary widely over time, a phenomenon commonly observed in commercial services. In this work, we suggest an alternative approach: rather than presuming that capacity estimation is required, it is perhaps better to begin by using only the buffer, and then ask when capacity estimation is needed. We test the viability of this approach through a series of experiments spanning millions of real users in a commercial service. We start with a simple design which directly chooses the video rate based on the current buffer occupancy. Our own investigation reveals that capacity estimation is unnecessary in steady state; however using simple capacity estimation (based on immediate past throughput) is important during the startup phase, when the buffer itself is growing from empty. This approach allows us to reduce the rebuffer rate by 10-20% compared to Netflix's then-default ABR algorithm, while delivering a similar average video rate, and a higher video rate in steady state.},
   author = {Te Yuan Huang and Ramesh Johari and Nick McKeown and Matthew Trunnell and Mark Watson},
   doi = {10.1145/2619239.2626296},
   issn = {19435819},
   issue = {4},
   journal = {Computer Communication Review},
   keywords = {HTTP-based video streaming,Video rate adaptation algorithm},
   pages = {187-198},
   title = {A buffer-based approach to rate adaptation: Evidence from a large video streaming service},
   volume = {44},
   year = {2015},
}
@article{Ratner2019,
   abstract = {Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.},
   author = {Alexander Ratner and Dan Alistarh and Gustavo Alonso and David G. Andersen and Peter Bailis and Sarah Bird and Nicholas Carlini and Bryan Catanzaro and Jennifer Chayes and Eric Chung and Bill Dally and Jeff Dean and Inderjit S. Dhillon and Alexandros Dimakis and Pradeep Dubey and Charles Elkan and Grigori Fursin and Gregory R. Ganger and Lise Getoor and Phillip B. Gibbons and Garth A. Gibson and Joseph E. Gonzalez and Justin Gottschlich and Song Han and Kim Hazelwood and Furong Huang and Martin Jaggi and Kevin Jamieson and Michael I. Jordan and Gauri Joshi and Rania Khalaf and Jason Knight and Jakub KoneÄnÃ½ and Tim Kraska and Arun Kumar and Anastasios Kyrillidis and Aparna Lakshmiratan and Jing Li and Samuel Madden and H. Brendan McMahan and Erik Meijer and Ioannis Mitliagkas and Rajat Monga and Derek Murray and Kunle Olukotun and Dimitris Papailiopoulos and Gennady Pekhimenko and Theodoros Rekatsinas and Afshin Rostamizadeh and Christopher RÃ© and Christopher De Sa and Hanie Sedghi and Siddhartha Sen and Virginia Smith and Alex Smola and Dawn Song and Evan Sparks and Ion Stoica and Vivienne Sze and Madeleine Udell and Joaquin Vanschoren and Shivaram Venkataraman and Rashmi Vinayak and Markus Weimer and Andrew Gordon Wilson and Eric Xing and Matei Zaharia and Ce Zhang and Ameet Talwalkar},
   title = {MLSys: The New Frontier of Machine Learning Systems},
   year = {2019},
}
@article{Stoica2017,
   abstract = {With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, AI (Artificial Intelligence) has moved from research labs to production. These changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems software and architectures, and by the broad accessibility of these technologies. The next generation of AI systems promises to accelerate these developments and increasingly impact our lives via frequent interactions and making (often mission-critical) decisions on our behalf, often in highly personalized contexts. Realizing this promise, however, raises daunting challenges. In particular, we need AI systems that make timely and safe decisions in unpredictable environments, that are robust against sophisticated adversaries, and that can process ever increasing amounts of data across organizations and individuals without compromising confidentiality. These challenges will be exacerbated by the end of the Moore's Law, which will constrain the amount of data these technologies can store and process. In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society.},
   author = {Ion Stoica and Dawn Song and Raluca Ada Popa and David Patterson and Michael W. Mahoney and Randy Katz and Anthony D. Joseph and Michael Jordan and Joseph M. Hellerstein and Joseph E. Gonzalez and Ken Goldberg and Ali Ghodsi and David Culler and Pieter Abbeel},
   title = {A Berkeley View of Systems Challenges for AI},
   year = {2017},
}
@article{Mitchell2019,
   abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
   author = {Margaret Mitchell and Simone Wu and Andrew Zaldivar and Parker Barnes and Lucy Vasserman and Ben Hutchinson and Elena Spitzer and Inioluwa Deborah Raji and Timnit Gebru},
   doi = {10.1145/3287560.3287596},
   isbn = {9781450361255},
   issue = {Figure 2},
   journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
   keywords = {Datasheets,Disaggregated evaluation,Documentation,Ethical considerations,Fairness evaluation,ML model evaluation,Model cards},
   pages = {220-229},
   title = {Model cards for model reporting},
   year = {2019},
}
@article{Feder2021,
   abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learningâbased methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1},
   author = {Amir Feder and Nadav Oved and Uri Shalit and Roi Reichart},
   doi = {10.1162/COLI_a_00404},
   issn = {15309312},
   issue = {2},
   journal = {Computational Linguistics},
   pages = {333-386},
   title = {Causalm: Causal model explanation through counterfactual language models},
   volume = {47},
   year = {2021},
}
@article{Radford2021,
   abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
   author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
   note = {CLIP},
   title = {Learning Transferable Visual Models From Natural Language Supervision},
   url = {http://arxiv.org/abs/2103.00020},
   year = {2021},
}
@article{,
   abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
   author = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll and Ross Girshick},
   note = {MAE},
   title = {Masked Autoencoders Are Scalable Vision Learners},
}
@article{Dosovitskiy2020,
   abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
   author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
   note = {ViT},
   title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
   url = {http://arxiv.org/abs/2010.11929},
   year = {2020},
}
@article{Lin2020,
   abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
   author = {Tsung Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollar},
   doi = {10.1109/TPAMI.2018.2858826},
   issn = {19393539},
   issue = {2},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Computer vision,convolutional neural networks,machine learning,object detection},
   note = {RetinaNet},
   pages = {318-327},
   pmid = {30040631},
   title = {Focal Loss for Dense Object Detection},
   volume = {42},
   year = {2020},
}
@article{So2021,
   abstract = {Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.},
   author = {David R. So and Wojciech MaÅke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},
   pages = {1-35},
   title = {Primer: Searching for Efficient Transformers for Language Modeling},
   url = {http://arxiv.org/abs/2109.08668},
   year = {2021},
}
@article{Wei2021,
   abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.},
   author = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
   pages = {1-41},
   title = {Finetuned Language Models Are Zero-Shot Learners},
   url = {http://arxiv.org/abs/2109.01652},
   year = {2021},
}
@article{Liu2021,
   abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
   author = {Hanxiao Liu and Zihang Dai and David R. So and Quoc V. Le},
   issue = {Mlm},
   title = {Pay Attention to MLPs},
   url = {http://arxiv.org/abs/2105.08050},
   year = {2021},
}
@article{Tay2020,
   abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
   author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
   issue = {August 2020},
   keywords = {atten-,deep learning,natural language processing,transformer models},
   pages = {1-28},
   title = {Efficient Transformers: A Survey},
   url = {http://arxiv.org/abs/2009.06732},
   year = {2020},
}
@article{Lin2021,
   abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
   author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
   issue = {1},
   keywords = {Transformer, Self-Attention, Pre-trained Models, D},
   publisher = {Association for Computing Machinery},
   title = {A Survey of Transformers},
   volume = {1},
   url = {http://arxiv.org/abs/2106.04554},
   year = {2021},
}
@article{Han2020,
   abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
   author = {Kai Han and Yunhe Wang and Hanting Chen and Xinghao Chen and Jianyuan Guo and Zhenhua Liu and Yehui Tang and An Xiao and Chunjing Xu and Yixing Xu and Zhaohui Yang and Yiman Zhang and Dacheng Tao},
   pages = {1-25},
   title = {A Survey on Vision Transformer},
   url = {http://arxiv.org/abs/2012.12556},
   year = {2020},
}
@article{Gao2020,
   abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
   author = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
   title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
   url = {http://arxiv.org/abs/2101.00027},
   year = {2020},
}
@article{Nguyen2019,
   abstract = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose $\ell_2$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.},
   author = {Toan Q. Nguyen and Julian Salazar},
   doi = {10.5281/zenodo.3525484},
   title = {Transformers without Tears: Improving the Normalization of Self-Attention},
   year = {2019},
}
@article{,
   abstract = {Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates -- the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback.},
   author = {Angelina McMillan-Major and Salomey Osei and Juan Diego Rodriguez and Pawan Sasanka Ammanamanchi and Sebastian Gehrmann and Yacine Jernite},
   doi = {10.18653/v1/2021.gem-1.11},
   pages = {121-135},
   title = {Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards},
   year = {2021},
}
@article{Gururangan2020,
   abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
   author = {Suchin Gururangan and Ana MarasoviÄ and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
   doi = {10.18653/v1/2020.acl-main.740},
   issn = {2331-8422},
   pages = {8342-8360},
   title = {Donât Stop Pretraining: Adapt Language Models to Domains and Tasks},
   year = {2020},
}
@article{Wolf2020,
   abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url\{https://github.com/huggingface/transformers\}.},
   author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Remi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander Rush},
   doi = {10.18653/v1/2020.emnlp-demos.6},
   pages = {38-45},
   title = {Transformers: State-of-the-Art Natural Language Processing},
   year = {2020},
}
@article{Shao2020,
   abstract = {Normalization operations are widely used to train deep neural networks, and they can improve both convergence and generalization in most tasks. The theories for normalizationâs effectiveness and new forms of normalization have always been hot topics in research. To better understand normalization, one question can be whether normalization is indispensable for training deep neural networks? In this paper, we analyze what would happen when normalization layers are removed from the networks, and show how to train deep neural networks without normalization layers and without performance degradation. Our proposed method can achieve the same or even slightly better performance in a variety of tasks: image classification in ImageNet, object detection and segmentation in MS-COCO, video classification in Kinetics, and machine translation in WMT English-German, etc. Our study may help better understand the role of normalization layers and can be a competitive alternative to normalization layers. Codes are available at https://github.com/hukkai/rescaling.},
   author = {Jie Shao and Kai Hu and Changhu Wang and Xiangyang Xue and Bhiksha Raj},
   issn = {10495258},
   issue = {NeurIPS},
   journal = {Advances in Neural Information Processing Systems},
   note = {RescaleNet},
   pages = {1-11},
   title = {Is normalization indispensable for training deep neural networks?},
   volume = {2020-Decem},
   year = {2020},
}
@article{Qiu2020,
   abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
   author = {Xi Peng Qiu and Tian Xiang Sun and Yi Ge Xu and Yun Fan Shao and Ning Dai and Xuan Jing Huang},
   doi = {10.1007/s11431-020-1647-3},
   issn = {1862281X},
   issue = {10},
   journal = {Science China Technological Sciences},
   keywords = {deep learning,distributed representation,language modelling,natural language processing,neural network,pre-trained model,self-supervised learning,word embedding},
   pages = {1872-1897},
   title = {Pre-trained models for natural language processing: A survey},
   volume = {63},
   year = {2020},
}
@article{McCann2017,
   abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
   author = {Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {6295-6306},
   title = {Learned in translation: Contextualized word vectors},
   volume = {2017-Decem},
   year = {2017},
}
@article{Yuan2021,
   abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
   author = {Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
   title = {Florence: A New Foundation Model for Computer Vision},
   url = {http://arxiv.org/abs/2111.11432},
   year = {2021},
}
@article{Liu2020,
   abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
   author = {Qi Liu and Matt J. Kusner and Phil Blunsom},
   title = {A Survey on Contextual Embeddings},
   url = {http://arxiv.org/abs/2003.07278},
   year = {2020},
}
@article{Kaissis2021,
   abstract = {Using large, multi-national datasets for high-performance medical imaging AI systems requires innovation in privacy-preserving machine learning so models can train on sensitive data without requiring data transfer. Here we present PriMIA (Privacy-preserving Medical Image Analysis), a free, open-source software framework for differentially private, securely aggregated federated learning and encrypted inference on medical imaging data. We test PriMIA using a real-life case study in which an expert-level deep convolutional neural network classifies paediatric chest X-rays; the resulting modelâs classification performance is on par with locally, non-securely trained models. We theoretically and empirically evaluate our frameworkâs performance and privacy guarantees, and demonstrate that the protections provided prevent the reconstruction of usable data by a gradient-based model inversion attack. Finally, we successfully employ the trained model in an end-to-end encrypted remote inference scenario using secure multi-party computation to prevent the disclosure of the data and the model.},
   author = {Georgios Kaissis and Alexander Ziller and Jonathan Passerat-Palmbach and ThÃ©o Ryffel and Dmitrii Usynin and Andrew Trask and IonÃ©sio Lima and Jason Mancuso and Friederike Jungmann and Marc Matthias Steinborn and Andreas Saleh and Marcus Makowski and Daniel Rueckert and Rickmer Braren},
   doi = {10.1038/s42256-021-00337-8},
   issn = {25225839},
   issue = {6},
   journal = {Nature Machine Intelligence},
   pages = {473-484},
   publisher = {Springer US},
   title = {End-to-end privacy preserving deep learning on multi-institutional medical imaging},
   volume = {3},
   url = {http://dx.doi.org/10.1038/s42256-021-00337-8},
   year = {2021},
}
@article{,
   author = {ThÃ©o Ryffel and Jonathan Passerat-palmbach},
   pages = {1-5},
   title = {A generic framework for privacy preserving deep learning},
}
@article{Yang2019,
   abstract = {Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
   author = {Qiang Yang and Yang Liu and Tianjian Chen and Yongxin Tong},
   doi = {10.1145/3298981},
   issn = {21576912},
   issue = {2},
   journal = {ACM Transactions on Intelligent Systems and Technology},
   keywords = {Federated learning,GDPR,transfer learning},
   pages = {1-19},
   title = {Federated machine learning: Concept and applications},
   volume = {10},
   year = {2019},
}
@article{Wyatt2021,
   abstract = {Cognitive simulation (CogSim) is an important and emerging workflow for HPC scientific exploration and scientific machine learning (SciML). One challenging workload for CogSim is the replacement of one component in a complex physical simulation with a fast, learned, surrogate model that is "inside" of the computational loop. The execution of this in-the-loop inference is particularly challenging because it requires frequent inference across multiple possible target models, can be on the simulation's critical path (latency bound), is subject to requests from multiple MPI ranks, and typically contains a small number of samples per request. In this paper we explore the use of large, dedicated Deep Learning / AI accelerators that are disaggregated from compute nodes for this CogSim workload. We compare the trade-offs of using these accelerators versus the node-local GPU accelerators on leadership-class HPC systems.},
   author = {Michael R Wyatt and Valen Yamamoto and Zoe Tosi and Ian Karlin and Brian Van Essen},
   title = {Is Disaggregation possible for HPC Cognitive Simulation?},
   url = {http://arxiv.org/abs/2112.05216},
   year = {2021},
}
@article{Baevski2022,
   abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
   author = {Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},
   journal = {Arxiv},
   title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
   url = {www.github.com/pytorch/fairseq/},
   year = {2022},
}
@article{Gao2020,
   abstract = {This paper presents TorchANI, a PyTorch based software for training/inferenceof ANI (ANAKIN-ME) deep learning models to obtain potential energy surfaces andother physical properties of molecular systems. ANI is an accurate neural networkpotential originally implemented using C++/CUDA in a program called NeuroChem.Compared with NeuroChem, TorchANI has a design emphasis on being light weight,user friendly, cross platform, and easy to read and modify for fast prototyping, whileallowing acceptable sacrifice on running performance. Because the computation ofatomic environmental vectors (AEVs) and atomic neural networks are all implementedusing PyTorch operators, TorchANI is able to use PyTorchâs autograd engine to automatically compute analytical forces and Hessian matrices, as well as do force trainingwithout additional codes required.},
   author = {Xiang Gao and Farhad Ramezanghorbani and Olexander Isayev and Justin S. Smith and Adrian E. Roitberg},
   doi = {10.26434/chemrxiv.12218294.v1},
   issn = {2573-2293},
   issue = {1},
   journal = {ChemRxiv},
   keywords = {Anakin,Machine Learning,Pytorch},
   title = {TorchANI: A Free and Open Source PyTorch Based Deep Learning Implementation of the ANI Neural Network Potentials},
   year = {2020},
}
@article{Lew2019,
   abstract = {Machine learning (ML) has recently emerged as an important application driving future architecture design. Traditionally, architecture research has used detailed simulators to model and measure the impact of proposed changes. However, current open-source, publicly available simulators lack support for running a full ML stack like PyTorch. High-confidence, cycle-accurate simulations are crucial for architecture research and without them, it is difficult to rapidly prototype new ideas. In this paper, we describe changes we made to GPGPU-Sim, a popular, widely used GPU simulator, to run ML applications that use cuDNN and PyTorch, two widely used frameworks for running Deep Neural Networks (DNNs). This work has the potential to enable significant microarchitectural research into GPUs for DNNs. Our results show that the modified simulator, which has been made publicly available with this paper 1Source code available at https://github.com/gpgpu-sim/gpgpu-sim-distribution (dev branch), provides execution time results within 18% of real hardware. We further use it to study other ML workloads and demonstrate how the simulator identifies opportunities for architectural optimization that prior tools are unable to provide.},
   author = {Jonathan Lew and Deval A. Shah and Suchita Pati and Shaylin Cattell and Mengchi Zhang and Amruth Sandhupatla and Christopher Ng and Negar Goli and Matthew D. Sinclair and Timothy G. Rogers and Tor M. Aamodt},
   doi = {10.1109/ISPASS.2019.00028},
   isbn = {9781728107462},
   journal = {Proceedings - 2019 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2019},
   keywords = {CuDNN,GPGPU-Sim,Machine Learning,Py-Torch},
   pages = {151-152},
   title = {Analyzing Machine Learning Workloads Using a Detailed GPU Simulator},
   year = {2019},
}
@article{Miao2017,
   abstract = {Deep learning has improved state-of-The-Art results in many important fields, and has been the subject of much research in recent years, leading to the development of several systems for facilitating deep learning. Current systems, however, mainly focus on model building and training phases, while the issues of data management, model sharing, and lifecycle management are largely ignored. Deep learning modeling lifecycle generates a rich set of data artifacts, e.g., learned parameters and training logs, and it comprises of several frequently conducted tasks, e.g., to understand the model behaviors and to try out new models. Dealing with such artifacts and tasks is cumbersome and largely left to the users. This paper describes our vision and implementation of a data and lifecycle management system for deep learning. First, we generalize model exploration and model enumeration queries from commonly conducted tasks by deep learning modelers, and propose a high-level domain specific language (DSL), inspired by SQL, to raise the abstraction level and thereby accelerate the modeling process. To manage the variety of data artifacts, especially the large amount of checkpointed float parameters, we design a novel model versioning system (dlv), and a read-optimized parameter archival storage system (PAS) that minimizes storage footprint and accelerates query workloads with minimal loss of accuracy. PAS archives versioned models using deltas in a multi-resolution fashion by separately storing the less significant bits, and features a novel progressive query (inference) evaluation algorithm. Third, we develop efficient algorithms for archiving versioned models using deltas under co-retrieval constraints. We conduct extensive experiments over several real datasets from computer vision domain to show the efficiency of the proposed techniques.},
   author = {Hui Miao and Ang Li and Larry S. Davis and Amol Deshpande},
   doi = {10.1109/ICDE.2017.112},
   isbn = {9781509065431},
   issn = {10844627},
   journal = {Proceedings - International Conference on Data Engineering},
   pages = {571-582},
   title = {Towards unified data and lifecycle management for deep learning},
   year = {2017},
}
@article{,
   abstract = {DEAP (Distributed Evolutionary Algorithms in Python) is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black box type of frameworks. It also incorporates easy parallelism where users need not concern themselves with gory implementation details like synchronization and load balancing, only functional decomposition. Several examples illustrate the multiple properties of DEAP. Copyright 2012 ACM.},
   author = {FranÃ§ois Michel De Rainville and FÃ©lix Antoine Fortin and Marc AndrÃ© Gardner and Marc Parizeau and Christian GagnÃ©},
   doi = {10.1145/2330784.2330799},
   isbn = {9781450311786},
   journal = {GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation Companion},
   keywords = {Parallel evolutionary algorithms,Software tools},
   pages = {85-92},
   title = {DEAP: A Python framework for Evolutionary Algorithms},
   year = {2012},
}
@article{Ke2018,
   abstract = {Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.},
   author = {Nan Rosemary Ke and Anirudh Goyal and Olexa Bilaniuk and Jonathan Binas and Michael C. Mozer and Chris Pal and Yoshua Bengio},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {7640-7651},
   title = {Sparse attentive backtracking: Temporal credit assignment through reminding},
   volume = {2018-Decem},
   year = {2018},
}
@article{Anil2020,
   abstract = {Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.},
   author = {Rohan Anil and Vineet Gupta and Tomer Koren and Kevin Regan and Yoram Singer},
   issn = {2331-8422},
   pages = {1-24},
   title = {Scalable Second Order Optimization for Deep Learning},
   url = {http://arxiv.org/abs/2002.09018},
   year = {2020},
}
@article{Han2016,
   abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving, Exploiting sparsity saves 10x, Weight sharing gives 8x, Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88x104 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.},
   author = {Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark A. Horowitz and William J. Dally},
   doi = {10.1109/ISCA.2016.30},
   isbn = {9781467389471},
   journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
   keywords = {ASIC,Algorithm-Hardware co-Design,Deep Learning,Hardware Acceleration,Model Compression},
   pages = {243-254},
   title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
   volume = {16},
   year = {2016},
}
@article{Mendis2019,
   abstract = {Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM-based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.},
   author = {Charith Mendis and Alex Renda and Saman Amarasinghe and Michael Carbin},
   isbn = {9781510886988},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   pages = {7908-7918},
   title = {IThemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks},
   volume = {2019-June},
   year = {2019},
}
@article{Smith2020,
   abstract = {Maximum diversification of data is a central theme in building generalized and accurate machine learning (ML) models. In chemistry, ML has been used to develop models for predicting molecular properties, for example quantum mechanics (QM) calculated potential energy surfaces and atomic charge models. The ANI-1x and ANI-1ccx ML-based general-purpose potentials for organic molecules were developed through active learning; an automated data diversification process. Here, we describe the ANI-1x and ANI-1ccx data sets. To demonstrate data diversity, we visualize it with a dimensionality reduction scheme, and contrast against existing data sets. The ANI-1x data set contains multiple QM properties from 5 M density functional theory calculations, while the ANI-1ccx data set contains 500 k data points obtained with an accurate CCSD(T)/CBS extrapolation. Approximately 14 million CPU core-hours were expended to generate this data. Multiple QM calculated properties for the chemical elements C, H, N, and O are provided: energies, atomic forces, multipole moments, atomic charges, etc. We provide this data to the community to aid research and development of ML models for chemistry.},
   author = {Justin S. Smith and Roman Zubatyuk and Benjamin Nebgen and Nicholas Lubbers and Kipton Barros and Adrian E. Roitberg and Olexandr Isayev and Sergei Tretiak},
   doi = {10.1038/s41597-020-0473-z},
   issn = {20524463},
   issue = {1},
   journal = {Scientific Data},
   pages = {1-10},
   pmid = {32358545},
   title = {The ANI-1ccx and ANI-1x data sets, coupled-cluster and density functional theory properties for molecules},
   volume = {7},
   year = {2020},
}
@article{Zeng2019,
   abstract = {Deep learning has aroused extensive attention due to its great empirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural network (DNN) training. However, theoretical studies on their convergence properties are limited due to the highly nonconvex nature of DNN training. In this paper, we aim at providing a general methodology for provable convergence guarantees for this type of methods. In particular, for most of the commonly used DNN training models involving both two- and three-splitting schemes, we establish the global convergence to a critical point at a rate of O(l/k), where k is the number of iterations. The results extend to general loss functions which have Lipschitz continuous gradients and deep residual networks (ResNets). Our key development adds several new elements to the Kurdyka-Åojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of deep learning.},
   author = {Jinshan Zeng and Tim Tsz Kit Lau and Shao Bo Lin and Yuan Yao},
   isbn = {9781510886988},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   pages = {12685-12711},
   title = {Global convergence of block coordinate descent in deep learning},
   volume = {2019-June},
   year = {2019},
}
@article{Lample2016,
   abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures - one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.1},
   author = {Guillaume Lample and Miguel Ballesteros and Sandeep Subramanian and Kazuya Kawakami and Chris Dyer},
   doi = {10.18653/v1/n16-1030},
   isbn = {9781941643914},
   journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
   pages = {260-270},
   title = {Neural architectures for named entity recognition},
   year = {2016},
}
@article{David2020,
   abstract = {Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100 -- 1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.},
   author = {Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
   title = {TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems},
   url = {http://arxiv.org/abs/2010.08678},
   year = {2020},
}
@article{Martins2016,
   abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
   author = {Andre F.T. Martins and Ramon F. Astudillo},
   isbn = {9781510829008},
   journal = {33rd International Conference on Machine Learning, ICML 2016},
   pages = {2432-2443},
   title = {From softmax to sparsemax: A sparse model of attention and multi-label classification},
   volume = {4},
   year = {2016},
}
@article{Wang2019,
   abstract = {Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. There are plenty of specialized hardware for neural networks, but little research has been done for specialized neural network optimization for a particular hardware architecture. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.},
   author = {Kuan Wang and Zhijian Liu and Yujun Lin and Ji Lin and Song Han},
   doi = {10.1109/CVPR.2019.00881},
   isbn = {9781728132938},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Deep Learning,Vision Applications and Systems},
   pages = {8604-8612},
   title = {HAQ: Hardware-aware automated quantization with mixed precision},
   volume = {2019-June},
   year = {2019},
}
@article{Kwon2019,
   abstract = {The data partitioning and scheduling strategies used by DNN accelerators to leverage reuse and perform staging are known as dataflow, which directly impacts the performance and energy efficiency of DNN accelerators. An accelerator microarchitecture dictates the dataflow(s) that can be employed to execute layers in a DNN. Selecting a dataflow for a layer can have a large impact on utilization and energy efficiency, but there is a lack of understanding on the choices and consequences of dataflows, and of tools and methodologies to help architects explore the co-optimization design space. In this work, we first introduce a set of data-centric directives to concisely specify the DNN dataflow space in a compiler-friendly form. We then show how these directives can be analyzed to infer various forms of reuse and to exploit them using hardware capabilities. We codify this analysis into an analytical cost model, MAESTRO (Modeling Accelerator Efficiency via Spatio-Temporal Reuse and Occupancy), that estimates various cost-benefit tradeoffs of a dataflow including execution time and energy efficiency for a DNN model and hardware configuration. We demonstrate the use of MAESTRO to drive a hardware design space exploration experiment, which searches across 480M designs to identify 2.5M valid designs at an average rate of 0.17M designs per second, including Pareto-optimal throughput- and energy-optimized design points.},
   author = {Hyoukjun Kwon and Prasanth Chatarasi and Michael Pellauer and Angshuman Parashar and Vivek Sarkar and Tushar Krishna},
   doi = {10.1145/3352460.3358252},
   isbn = {9781450369381},
   issn = {10724451},
   journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
   keywords = {Cost modeling,Dataflow,Neural networks},
   pages = {754-768},
   title = {Understanding reuse, performance, and hardware cost of DNN dataflows: A data-centric approach},
   year = {2019},
}
@article{,
   abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.},
   author = {Arild NÃ¸kland},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1045-1053},
   title = {Direct feedback alignment provides learning in deep neural networks},
   year = {2016},
}
@article{Pappas2018,
   abstract = {Tying the weights of the target word embeddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality. Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters. In particular, we propose a structure-aware output layer which captures the semantic structure of the output space of words within a joint input-output embedding. The model is a generalized form of weight tying which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled. In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them. Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder baselines trained with or without weight tying.},
   author = {Nikolaos Pappas and Lesly Miculicich Werlen and James Henderson},
   doi = {10.18653/v1/w18-6308},
   isbn = {9781948087810},
   journal = {WMT 2018 - 3rd Conference on Machine Translation, Proceedings of the Conference},
   pages = {73-83},
   title = {Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation},
   volume = {1},
   year = {2018},
}
@article{Lubbers2018,
   abstract = {We introduce the Hierarchically Interacting Particle Neural Network (HIP-NN) to model molecular properties from datasets of quantum calculations. Inspired by a many-body expansion, HIP-NN decomposes properties, such as energy, as a sum over hierarchical terms. These terms are generated from a neural network - a composition of many nonlinear transformations - acting on a representation of the molecule. HIP-NN achieves the state-of-the-art performance on a dataset of 131k ground state organic molecules and predicts energies with 0.26 kcal/mol mean absolute error. With minimal tuning, our model is also competitive on a dataset of molecular dynamics trajectories. In addition to enabling accurate energy predictions, the hierarchical structure of HIP-NN helps to identify regions of model uncertainty.},
   author = {Nicholas Lubbers and Justin S. Smith and Kipton Barros},
   doi = {10.1063/1.5011181},
   issn = {00219606},
   issue = {24},
   journal = {Journal of Chemical Physics},
   pages = {1-8},
   pmid = {29960311},
   title = {Hierarchical modeling of molecular energies using a deep neural network},
   volume = {148},
   year = {2018},
}
@article{Meissen2012,
   abstract = {Encryption is used to protect data against eavesdroppers who would otherwise intercept private communication. One party encrypts a message and sends the corresponding cipher- text to a second party, who then decrypts the ciphertext to recover the message. To prevent an untrusted third party from eavesdropping, the problem of recovering any information about the message from the ciphertext should be reasonably hard; in addition, the cipher- text should itself reveal no information about the message. Increasingly, data storage and computation is outsourced to these untrusted parties, which gives rise to the need for an encryption scheme that allows computation on the ciphertexts. The homomorphic properties of various encryption schemes have been a fascination of the cryptographic community for decades. With the rise of cloud computing and decentralized processing, the need for security in such applications is increasing. Only recently, however, has the construction of a fully homomorphic encryption scheme been realized. I present a mathematical approach to Craig Gentryâs proposed fully homomorphic scheme. I start with an overview of other homomorphic encryption schemes, followed by an examination of polynomial rings and their relation to lattices. Finally, I explore the scheme itself and provide a foundation from which to understand the challenges faced when constructing a fully homomorphic encryption scheme.},
   author = {Rebecca Meissen},
   journal = {Wpi.Edu},
   title = {A Mathematical Approach to Fully Homomorphic Encryption},
   url = {http://www.wpi.edu/Pubs/E-project/Available/E-project-042612-132350/unrestricted/Meissen_MQP2.pdf},
   year = {2012},
}
@article{Niculae2017,
   abstract = {Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.},
   author = {Vlad Niculae and Mathieu Blondel},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {3339-3349},
   title = {A regularized framework for sparse and structured neural attention},
   volume = {2017-Decem},
   year = {2017},
}
@article{Yao2017,
   abstract = {Machine learning classifiers are basic research tools used in numerous types of network analysis and modeling. To reduce the need for domain expertise and costs of running local ML classifiers, network researchers can instead rely on centralized Machine Learning as a Service (MLaaS) platforms. In this paper, we evaluate the effectiveness of MLaaS systems ranging from fully-automated, turnkey systems to fully-customizable systems, and find that with more user control comes greater risk. Good decisions produce even higher performance, and poor decisions result in harsher performance penalties. We also find that server side optimizations help fully-automated systems outperform default settings on competitors, but still lag far behind well-tuned MLaaS systems which compare favorably to standaloneML libraries. Finally, we find classifier choice is the dominating factor in determining model performance, and that users can approximate the performance of an optimal classifier choice by experimenting with a small subset of random classifiers. While network researchers should approach MLaaS systems with caution, they can achieve results comparable to standalone classifiers if they have sufficient insight into key decisions like classifiers and feature selection.},
   author = {Yuanshun Yao and Bimal Viswanath and Zhujun Xiao and Haitao Zheng and Bolun Wang and Ben Y. Zhao},
   doi = {10.1145/3131365.3131372},
   isbn = {9781450351188},
   issue = {119},
   journal = {Proceedings of the ACM SIGCOMM Internet Measurement Conference, IMC},
   keywords = {Cloud computing,Machine learning},
   pages = {384-397},
   title = {Complexity vs. Performance: Empirical analysis of machine learning as a service},
   volume = {Part F1319},
   year = {2017},
}
@article{Soro2021,
   abstract = {TinyML is a fast-growing multidisciplinary field at the intersection of machine learning, hardware, and software, that focuses on enabling deep learning algorithms on embedded (microcontroller powered) devices operating at extremely low power range (mW range and below). TinyML addresses the challenges in designing power-efficient, compact deep neural network models, supporting software framework, and embedded hardware that will enable a wide range of customized, ubiquitous inference applications on battery-operated, resource-constrained devices. In this report, we discuss the major challenges and technological enablers that direct this field's expansion. TinyML will open the door to the new types of edge services and applications that do not rely on cloud processing but thrive on distributed edge inference and autonomous reasoning.},
   author = {Stanislava Soro},
   issue = {20},
   title = {TinyML for Ubiquitous Edge AI},
   url = {http://arxiv.org/abs/2102.01255},
   year = {2021},
}
@article{Shazeer2018,
   abstract = {Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer [16] sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at https://github.com/tensorflow/mesh .},
   author = {Noam Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and Hyouk Joong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake Hechtman},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {10414-10423},
   title = {Mesh-tensorflow: Deep learning for supercomputers},
   volume = {2018-Decem},
   year = {2018},
}
@article{Dalton2015,
   abstract = {Sparse matrix-matrix multiplication (SpGEMM) is a key operation in numerous areas from information to the physical sciences. Implementing SpGEMM efficiently on throughput-oriented processors, such as the graphics processing unit (GPU), requires the programmer to expose substantial fine-grained parallelism while conserving the limited off-chip memory bandwidth. Balancing these concerns, we decompose the SpGEMM operation into three highly parallel phases: expansion, sorting, and contraction, and introduce a set of complementary bandwidth-saving performance optimizations. Our implementation is fully general and our optimization strategy adaptively processes the SpGEMM workload row-wise to substantially improve performance by decreasing the work complexity and utilizing the memory hierarchy more effectively.},
   author = {Steven Dalton and Luke Olson and Nathan Bell},
   doi = {10.1145/2699470},
   issn = {15577295},
   issue = {4},
   journal = {ACM Transactions on Mathematical Software},
   keywords = {GPU,Matrix-matrix,Parallel,Sparse},
   pages = {1-24},
   title = {Optimizing sparse matrix-matrix multiplication for the GPU},
   volume = {41},
   year = {2015},
}
@article{Parr2018,
   abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
   author = {Terence Parr and Jeremy Howard},
   pages = {1-33},
   title = {The Matrix Calculus You Need For Deep Learning},
   volume = {2018},
   url = {http://arxiv.org/abs/1802.01528},
   year = {2018},
}
@article{Miyashita2016,
   abstract = {Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.},
   author = {Daisuke Miyashita and Edward H. Lee and Boris Murmann},
   title = {Convolutional Neural Networks using Logarithmic Data Representation},
   url = {http://arxiv.org/abs/1603.01025},
   year = {2016},
}
@article{Han2016,
   abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce âdeep compressionâ, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35Ã to 49Ã without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9Ã to 13Ã; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35Ã, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49Ã from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3Ã to 4Ã layerwise speedup and 3Ã to 7Ã better energy efficiency.},
   author = {Song Han and Huizi Mao and William J. Dally},
   journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
   pages = {1-14},
   title = {Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding},
   year = {2016},
}
@article{Gale2020,
   abstract = {Scientific workloads have traditionally exploited high levels of sparsity to accelerate computation and reduce memory requirements. While deep neural networks can be made sparse, achieving practical speedups on GPUs is difficult because these applications have relatively moderate levels of sparsity that are not sufficient for existing sparse kernels to outperform their dense counterparts. In this work, we study sparse matrices from deep learning applications and identify favorable properties that can be exploited to accelerate computation. Based on these insights, we develop high-performance GPU kernels for two sparse matrix operations widely applicable in neural networks: Sparse matrix-dense matrix multiplication and sampled dense- dense matrix multiplication. Our kernels reach 27% of single-precision peak on Nvidia V100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet models that achieve ;1.2-2.1 Ã speedups and up to ;12.8 Ã memory savings without sacrificing accuracy.},
   author = {Trevor Gale and Matei Zaharia and Cliff Young and Erich Elsen},
   doi = {10.1109/SC41405.2020.00021},
   isbn = {9781728199986},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Neural networks,graphics processing units,sparse matrices},
   title = {Sparse gpu kernels for deep learning},
   volume = {2020-Novem},
   year = {2020},
}
@article{Terry2013,
   author = {Doug Terry},
   doi = {10.1145/2500500},
   issn = {00010782},
   issue = {12},
   journal = {Communications of the ACM},
   pages = {82-89},
   title = {Replicated data consistency explained through baseball},
   volume = {56},
   year = {2013},
}
@article{Halevi2017,
   abstract = {Fully homomorphic encryption (FHE) has been called the âSwiss Army knife of cryptographyâ, since it provides a single tool that can be uniformly applied to many cryptographic applications. In this tutorial we study FHE and describe its different properties, relations with other concepts in cryptography, and constructions. We briefly discuss the three generations of FHE constructions since Gentryâs breakthrough result in 2009, and cover in detail the third-generation scheme of Gentry, Sahai, and Waters (GSW).},
   author = {Shai Halevi},
   doi = {10.1007/978-3-319-57048-8_5},
   issn = {2197845X},
   issue = {9783319570471},
   journal = {Information Security and Cryptography},
   pages = {219-276},
   title = {Homomorphic encryption},
   volume = {0},
   year = {2017},
}
@article{Besard2019,
   abstract = {GPUs and other accelerators are popular devices for accelerating compute-intensive, parallelizable applications. However, programming these devices is a difficult task. Writing efficient device code is challenging, and is typically done in a low-level programming language. High-level languages are rarely supported, or do not integrate with the rest of the high-level language ecosystem. To overcome this, we propose compiler infrastructure to efficiently add support for new hardware or environments to an existing programming language. We evaluate our approach by adding support for NVIDIA GPUs to the Julia programming language. By integrating with the existing compiler, we significantly lower the cost to implement and maintain the new compiler, and facilitate reuse of existing application code. Moreover, use of the high-level Julia programming language enables new and dynamic approaches for GPU programming. This greatly improves programmer productivity, while maintaining application performance similar to that of the official NVIDIA CUDA toolkit.},
   author = {Tim Besard and Christophe Foket and Bjorn De Sutter},
   doi = {10.1109/TPDS.2018.2872064},
   issn = {15582183},
   issue = {4},
   journal = {IEEE Transactions on Parallel and Distributed Systems},
   keywords = {Graphics processors,code generation,retargetable compilers,very high-level languages},
   pages = {827-841},
   title = {Effective Extensible Programming: Unleashing Julia on GPUs},
   volume = {30},
   year = {2019},
}
@article{,
   abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply âautodiffâ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each otherâs results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names âdynamic computational graphsâ and âdifferentiable programmingâ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms âautodiffâ, âautomatic differentiationâ, and âsymbolic differentiationâ as these are encountered more and more in machine learning settings.},
   author = {AtÄ±lÄ±m GÃ¼neÅ Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Backpropagation,Differentiable Programming},
   pages = {1-43},
   title = {Automatic differentiation in machine learning: A survey},
   volume = {18},
   year = {2018},
}
@article{Yang2019,
   abstract = {Pipeline parallelism (PP) when training neural networks enables larger models to be partitioned spatially, leading to both lower network communication and overall higher hardware utilization. Unfortunately, to preserve the statistical efficiency of sequential training, existing PP techniques sacrifice hardware efficiency by decreasing pipeline utilization or incurring extra memory costs. In this paper, we investigate to what extent these sacrifices are necessary. We devise PipeMare, a simple yet robust training method that tolerates asynchronous updates during PP execution without sacrificing utilization or memory, which allows efficient use of fine-grained pipeline parallelism. Concretely, when tested on ResNet and Transformer networks, asynchrony enables PipeMare to use up to $2.7\times$ less memory or get $4.3\times$ higher pipeline utilization, with similar model quality, when compared to state-of-the-art synchronous PP training techniques.},
   author = {Bowen Yang and Jian Zhang and Jonathan Li and Christopher RÃ© and Christopher R. Aberger and Christopher De Sa},
   pages = {1-39},
   title = {PipeMare: Asynchronous Pipeline Parallel DNN Training},
   url = {http://arxiv.org/abs/1910.05124},
   year = {2019},
}
@article{Lin2017,
   abstract = {Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: https://github.com/synxlin/deep-gradient-compression.},
   author = {Yujun Lin and Song Han and Huizi Mao and Yu Wang and William J. Dally},
   pages = {1-13},
   title = {Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
   url = {http://arxiv.org/abs/1712.01887},
   year = {2017},
}
@article{Leijen2009,
   abstract = {The Task Parallel Library (TPL) is a library for .NET that makes it easy to take advantage of potential parallelism in a program. The library relies heavily on generics and delegate expressions to provide custom control structures expressing structured parallelism such as map-reduce in user programs. The library implementation is built around the notion of a task as a finite CPU-bound computation. To capture the ubiquitous apply-to-all pattern the library also introduces the novel concept of a replicable task. Tasks and replicable tasks are assigned to threads using work stealing techniques, but unlike traditional implementations based on the THE protocol, the library uses a novel data structure called a 'duplicating queue'. A surprising feature of duplicating queues is that they have sequentially inconsistent behavior on architectures with weak memory models, but capture this non-determinism in a benign way by sometimes duplicating elements. TPL ships as part of the Microsoft Parallel Extensions for the .NET framework 4.0, and forms the foundation of Parallel LINQ queries (however, note that the productized TPL library may differ in significant ways from the basic design described in this article). Copyright Â© 2009 ACM.},
   author = {Daan Leijen and Wolfram Schulte and Sebastian Burckhardt},
   doi = {10.1145/1640089.1640106},
   isbn = {9781605587349},
   journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
   keywords = {Domain specific languages,Duplicating queue,Parallelism,Work stealing},
   pages = {227-241},
   title = {The design of a task parallel library},
   year = {2009},
}
@article{Gordon2018,
   abstract = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
   author = {Ariel Gordon and Elad Eban and Ofir Nachum and Bo Chen and Hao Wu and Tien Ju Yang and Edward Choi},
   doi = {10.1109/CVPR.2018.00171},
   isbn = {9781538664209},
   issn = {10636919},
   issue = {1},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {1586-1595},
   title = {MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks},
   year = {2018},
}
@article{Yang2017,
   abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
   author = {Zichao Yang and Zhiting Hu and Ruslan Salakhutdinov and Taylor Berg-Kirkpatrick},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   pages = {5917-5928},
   title = {Improved variational autoencoders for text modeling using dilated convolutions},
   volume = {8},
   year = {2017},
}
@article{Zaheer2020,
   abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
   author = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1-51},
   title = {Big bird: Transformers for longer sequences},
   volume = {2020-Decem},
   year = {2020},
}
@article{Liang2021,
   abstract = {We propose a new forward-backward stochastic differential equation solver for high-dimensional derivative pricing problems by combining a deep learning solver with a least squares regression technique widely used in the least squares Monte Carlo method for the valuation of American options. Our numerical experiments demonstrate the accuracy of our least squares backward deep neural network solver and its capability to produce accurate prices for complex early exercisable derivatives, such as callable yield notes. Our method can serve as a generic numerical solver for pricing derivatives across various asset groups, in particular, as an accurate means for pricing high-dimensional derivatives with early exercise features.},
   author = {Jian Liang and Zhe Xu and Peter Li},
   doi = {10.1080/14697688.2021.1881149},
   issn = {14697696},
   issue = {8},
   journal = {Quantitative Finance},
   keywords = {Bermudan option,Callable yield note (CYN),Deep neural network (DNN),Forward-backward stochastic differential equation,High-dimensional derivative pricing,Least square regression (LSQ)},
   pages = {1309-1323},
   publisher = {Taylor & Francis},
   title = {Deep learning-based least squares forward-backward stochastic differential equation solver for high-dimensional derivative pricing},
   volume = {21},
   url = {https://doi.org/14697688.2021.1881149},
   year = {2021},
}
@article{Kearns1992,
   abstract = {Computational learning theory can be broadly and imprecisely defined as the mathematical study of effcient learning by machines or computational systems. The demand for effciency is one of the primary characteristics distinguishing computational learning theory from the older but still activ e areas of inductiv e inference and statistical pattern recognition. Thus computational learning theory encompasses a wide v ariet yof in teresting learning environmen ts and formal models too n umerous to detail in an y single volume Our goal here is to simply coney theaor of the recen v v t research by rst sum marizing w ork in v arious learning models and then carefully scrutinizing a single model that is reasonably natural and realisticî¬ and has enjo ed great y popularit y in its infancy î®},
   author = {Michael Kearns},
   isbn = {0262111527},
   pages = {165},
   title = {The Computational Complexity of Machine Learning},
   year = {1992},
}
@article{Kon2012,
   abstract = {We apply information-based complexity analysis to support vector machine (SVM) algorithms, with the goal of a comprehensive continuous algorithmic analysis of such algorithms. This involves complexity measures in which some higher order operations (e.g., certain optimizations) are considered primitive for the purposes of measuring complexity. We consider classes of information operators and of a F algorithms made up of scaled families and , and a a F F 8 5 Â§ Â§ investigate the utility of scaling the complexities and to minimize 8 5 error. We look at the division of statistical learning into information and algorithmic components, at the complexities of each, and at applications to support vector machine (SVM) and more general machine learning algorithms. We give applications to SVM algorithms graded into linear and higher order components, and give an example in biomedical informatics.},
   author = {Mark A Kon},
   journal = {arXiv preprint arXiv:1212.4562},
   pages = {1-36},
   title = {A complexity analysis of statistical learning algorithms},
   year = {2012},
}
@article{Pang2019,
   abstract = {Physics-informed neural networks (PINNs), introduced in [M. Raissi, P. Perdikaris, and G. Karniadakis, J. Comput. Phys., 378 (2019), pp. 686-707], are effective in solving integer-order partial differential equations (PDEs) based on scattered and noisy data. PINNs employ standard feedforward neural networks (NNs) with the PDEs explicitly encoded into the NN using automatic differentiation, while the sum of the mean-squared PDE residuals and the mean-squared error in initial-boundary conditions is minimized with respect to the NN parameters. Here we extend PINNs to fractional PINNs (fPINNs) to solve space-time fractional advection-diffusion equations (fractional ADEs), and we study systematically their convergence, hence explaining both fPINNs and PINNs for the first time. Specifically, we demonstrate their accuracy and effectiveness in solving multidimensional forward and inverse problems with forcing terms whose values are only known at randomly scattered spatio-temporal coordinates (black-box (BB) forcing terms). A novel element of the fPINNs is the hybrid approach that we introduce for constructing the residual in the loss function using both automatic differentiation for the integer-order operators and numerical discretization for the fractional operators. This approach bypasses the difficulties stemming from the fact that automatic differentiation is not applicable to fractional operators because the standard chain rule in integer calculus is not valid in fractional calculus. To discretize the fractional operators, we employ the Gr\" unwald-Letnikov (GL) formula in one-dimensional fractional ADEs and the vector GL formula in conjunction with the directional fractional Laplacian in two- and three-dimensional fractional ADEs. We first consider the one-dimensional fractional Poisson equation and compare the convergence of the fPINNs against the finite difference method (FDM). We present the solution convergence using both the mean L2 error as well as the standard deviation due to sensitivity to NN parameter initializations. Using different GL formulas we observe first-, second-, and third-order convergence rates for small size training sets but the error saturates for larger training sets. We explain these results by analyzing the four sources of numerical errors due to discretization, sampling, NN approximation, and optimization. The total error decays monotonically (below 10 - 5 for a third-order GL formula) but it saturates beyond that point due to the optimization error. We also analyze the relative balance between discretization and sampling errors and observe that the sampling size and the number of discretization points (auxiliary points) should be comparable to achieve the highest accuracy. As we increase the depth of the NN up to certain value, the mean error decreases and the standard deviation increases, whereas the width has essentially no effect unless its value is either too small or too large. We next consider time-dependent fractional ADEs and compare white-box (WB) and BB forcing. We observe that for the WB forcing, our results are similar to the aforementioned cases; however, for the BB forcing fPINNs outperform FDM. Subsequently, we consider multidimensional time-, space-, and space-time-fractional ADEs using the directional fractional Laplacian and we observe relative errors of 10 - 3 \sim 10 - 4. Finally, we solve several inverse problems in one, two, and three dimensions to identify the fractional orders, diffusion coefficients, and transport velocities and obtain accurate results given proper initializations even in the presence of significant noise.},
   author = {Guofei Pang and L. U. Lu and George E.M. Karniadakis},
   doi = {10.1137/18M1229845},
   issn = {10957197},
   issue = {4},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {Fractional advection-diffusion,Fractional inverse problem,Numerical error analysis,Parameter identification,Physics-informed learning machines},
   pages = {A2603-A2626},
   title = {FPinns: Fractional physics-informed neural networks},
   volume = {41},
   year = {2019},
}
@article{Serban2016,
   abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-The-Art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.},
   author = {Iulian V. Serban and Alessandro Sordoni and Yoshua Bengio and Aaron Courville and Joelle Pineau},
   isbn = {9781577357605},
   journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
   pages = {3776-3783},
   title = {Building end-To-end dialogue systems using generative hierarchical neural network models},
   year = {2016},
}
@article{Chen2019,
   abstract = {Compilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchi-tectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions. In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.},
   author = {Yishen Chen and Ajay Brahmakshatriya and Charith Mendis and Alex Renda and Eric Atkinson and Ondrej Sykora and Saman Amarasinghe and Michael Carbin},
   doi = {10.1109/IISWC47752.2019.9042166},
   isbn = {9781728140452},
   journal = {Proceedings of the 2019 IEEE International Symposium on Workload Characterization, IISWC 2019},
   keywords = {Benchmarking,Cost/performance,Measurement techniques,Modeling techniques},
   pages = {167-177},
   title = {BHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models},
   year = {2019},
}
@article{Haidar2019,
   abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16-FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4Ã speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
   author = {Azzam Haidar and Stanimire Tomov and Jack Dongarra and Nicholas J. Higham},
   doi = {10.1109/SC.2018.00050},
   isbn = {9781538683842},
   journal = {Proceedings - International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018},
   keywords = {FP16 Arithmetic,GPU Computing,Half Precision,Iterative Refinement Computation,Linear Algebra,Mixed Precision Solvers},
   pages = {603-613},
   title = {Harnessing GPU Tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers},
   year = {2019},
}
@article{Shen2020,
   abstract = {The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/sIncerass/powernorm.},
   author = {Sheng Shen and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
   isbn = {9781713821120},
   journal = {37th International Conference on Machine Learning, ICML 2020},
   pages = {8700-8710},
   title = {PowerNorm: Rethinking batch normalization in transformers},
   volume = {PartF16814},
   year = {2020},
}
@article{Hu2018,
   abstract = {In this paper, we introduce the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM naturally leads to the formulation of Affine Particle-In-Cell (APIC) [Jiang et al. 2015] and Polynomial Particle-In-Cell [Fu et al. 2017] in a way that is consistent with a Galerkin-style weak form discretization of the governing equations. Additionally, it enables a new stress divergence discretization that effortlessly allows all MPM simulations to run two times faster than before. We also develop a Compatible Particle-In-Cell (CPIC) algorithm on top of MLS-MPM. Utilizing a colored distance field representation and a novel compatibility condition for particles and grid nodes, our framework enables the simulation of various new phenomena that are not previously supported by MPM, including material cutting, dynamic open boundaries, and two-way coupling with rigid bodies. MLS-MPM with CPIC is easy to implement and friendly to performance optimization.},
   author = {Yuanming Hu and Yu Fang and Ziheng Ge and Ziyin Qu and Yixin Zhu and Andre Pradhana and Chenfanfu Jiang},
   doi = {10.1145/3197517.3201293},
   issn = {15577368},
   issue = {4},
   journal = {ACM Transactions on Graphics},
   keywords = {Cutting,Discontinuity,Distance field,Material point method (MPM),Moving least squares,Rigid coupling},
   title = {A moving least squares material point method with displacement discontinuity and two-way rigid body coupling},
   volume = {37},
   year = {2018},
}
@article{Li2020,
   abstract = {While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difficult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-defined structures (edges), the model learns to construct attention edges to improve task-specific performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classification, we demonstrate SAC is competitive with state-of-the-art models while significantly reducing memory cost.},
   author = {Xiaoya Li and Yuxian Meng and Mingxin Zhou and Qinghong Han and Fei Wu and Jiwei Li},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1-13},
   title = {SAC: Accelerating and structuring self-attention via sparse adaptive connection},
   volume = {2020-Decem},
   year = {2020},
}
@article{Lillicrap2016,
   abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
   author = {Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
   doi = {10.1038/ncomms13276},
   issn = {20411723},
   journal = {Nature Communications},
   pages = {1-10},
   pmid = {27824044},
   publisher = {Nature Publishing Group},
   title = {Random synaptic feedback weights support error backpropagation for deep learning},
   volume = {7},
   url = {http://dx.doi.org/10.1038/ncomms13276},
   year = {2016},
}
@article{Ying2018,
   abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
   author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and Jure Leskovec},
   doi = {10.1145/3219819.3219890},
   isbn = {9781450355520},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   pages = {974-983},
   title = {Graph convolutional neural networks for web-scale recommender systems},
   year = {2018},
}
@article{Barham2019,
   abstract = {In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability. Systems researchers are doing an excellent job improving the performance of 5-year-old benchmarks, but gradually making it harder to explore innovative machine learning research ideas. We explain how the evolution of hardware accelerators favors compiler back ends that hyper-optimize large monolithic kernels, show how this reliance on high-performance but inflexible kernels reinforces the dominant style of programming model, and argue these programming abstractions lack expressiveness, maintainability, and modularity; all of which hinders research progress. We conclude by noting promising directions in the field, and advocate steps to advance progress towards high-performance general purpose numerical computing systems on modern accelerators.},
   author = {Paul Barham and Michael Isard},
   doi = {10.1145/3317550.3321441},
   isbn = {9781450367271},
   journal = {Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS 2019},
   pages = {177-183},
   title = {Machine Learning Systems are Stuck in a Rut},
   year = {2019},
}
@article{Sun2019,
   abstract = {The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of Graphics Processing Units (GPUs). As single-GPU platforms struggle to satisfy these performance demands, multi-GPU platforms have started to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabric, runtime libraries, and associated programming models. The research community currently lacks a publicly available and comprehensive multi-GPU simulation framework to evaluate next-generation multi-GPU system designs. In this work, we present MGPUSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. MGPUSim comes with in-built support for multi-threaded execution to enable fast, parallelized, and accurate simulation. In terms of performance accuracy, MGPUSim differs by only 5.5% on average from the actual GPU hardware. We also achieve a 3.5Ã and a 2.5Ã average speedup running functional emulation and detailed timing simulation, respectively, on a 4-core CPU, while delivering the same accuracy as serial simulation. We illustrate the flexibility and capability of the simulator through two concrete design studies. In the first, we propose the Locality API, an API extension that allows the GPU programmer to both avoid the complexity of multi-GPU programming, while precisely controlling data placement in the multi-GPU memory. In the second design study, we propose <u>P</u>rogressive P<u>a</u>ge <u>S</u>plitting M<u>i</u>gration (PASI), a customized multi-GPU memory management system enabling the hardware to progressively improve data placement. For a discrete 4-GPU system, we observe that the Locality API can speed up the system by 1.6Ã (geometric mean), and PASI can improve the system performance by 2.6Ã (geometric mean) across all benchmarks, compared to a unified 4-GPU platform.},
   author = {Yifan Sun and Trinayan Baruah and Saiful A. Mojumder and Shi Dong and Xiang Gong and Shane Treadway and Yuhui Bao and Spencer Hance and Carter McCardwell and Vincent Zhao and Harrison Barclay and Amir Kavyan Ziabari and Zhongliang Chen and Rafael Ubal and JosÃ© L. AbellÃ¡n and John Kim and Ajay Joshi and David Kaeli},
   doi = {10.1145/3307650.3322230},
   isbn = {9781450366694},
   issn = {10636897},
   journal = {Proceedings - International Symposium on Computer Architecture},
   keywords = {Memory management,Multi-GPU systems,Simulation},
   pages = {197-209},
   title = {MGPUSim: Enabling multi-GPU performance modeling and optimization},
   year = {2019},
}
@article{Pal2019,
   abstract = {Deploying deep learning (DL) models across multiple compute devices to train large and complex models continues to grow in importance because of the demand for faster and more frequent training. Data parallelism (DP) is the most widely used parallelization strategy, but as the number of devices in data parallel training grows, so does the communication overhead between devices. Additionally, a larger aggregate batch size per step leads to statistical efficiency loss, i.e., a larger number of epochs are required to converge to a desired accuracy. These factors affect overall training time and beyond a certain number of devices, the speedup from DP scales poorly. This work explores hybrid parallelization, where each data parallel worker comprises more than one device to accelerate each training step by exploiting model parallelism. We show that at scale, hybrid training will be more effective at minimizing end-to-end training time than exploiting DP alone. We project that, for Inception-V3, GNMT, and BigLSTM, the hybrid strategy provides an end-to-end training speedup of at least 26.5%, 8%, and 22%, respectively, compared to what DP alone can achieve at scale.},
   author = {Saptadeep Pal and Eiman Ebrahimi and Arslan Zulfiqar and Yaosheng Fu and Victor Zhang and Szymon Migacz and David Nellans and Puneet Gupta},
   doi = {10.1109/MM.2019.2935967},
   issn = {19374143},
   issue = {5},
   journal = {IEEE Micro},
   pages = {91-101},
   title = {Optimizing Multi-GPU Parallelization Strategies for Deep Learning Training},
   volume = {39},
   year = {2019},
}
@article{Zhang2019,
   abstract = {Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However, current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an efficient communication architecture for distributed DL on GPUs. Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3, a 50% improvement over the open-source TensorFlow (20x speed-up).},
   author = {Hao Zhang and Zeyu Zheng and Shizhen Xu and Wei Dai and Qirong Ho and Xiaodan Liang and Zhiting Hu and Jinliang Wei and Pengtao Xie and Eric P. Xing},
   isbn = {9781931971386},
   journal = {Proceedings of the 2017 USENIX Annual Technical Conference, USENIX ATC 2017},
   pages = {181-193},
   title = {Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters},
   year = {2019},
}
@article{Shallue2019,
   abstract = {Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.},
   author = {Christopher J. Shallue and Jaehoon Lee and Joseph Antognini and Jascha Sohl-Dickstein and Roy Frostig and George E. Dahl},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Batch size,Data parallelism,Deep learning,Neural networks,Stochastic gradient descent},
   pages = {1-49},
   title = {Measuring the effects of data parallelism on neural network training},
   volume = {20},
   year = {2019},
}
@article{Jouppi2017,
   abstract = {Many architects believe that major improvements in cost-energyperformance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile responsetime requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an NVIDIA K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X-30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X-80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
   author = {Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre Luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Daniel Killebrew and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross and Matt Ross and Amir Salek and Emad Samadiani and Chris Severn and Gregory Sizikov and Matthew Snelham and Jed Souter and Dan Steinberg and Andy Swing and Mercedes Tan and Gregory Thorson and Bo Tian and Horia Toma and Erick Tuttle and Vijay Vasudevan and Richard Walter and Walter Wang and Eric Wilcox and Doe Hyun Yoon},
   doi = {10.1145/3079856.3080246},
   issn = {10636897},
   journal = {Proceedings - International Symposium on Computer Architecture},
   keywords = {Accelerator,CNN,DNN,Deep learning,Domain-specific architecture,GPU,LSTM,MLP,Neural network,RNN,TPU,TensorFlow},
   pages = {1-12},
   title = {In-datacenter performance analysis of a tensor processing unit},
   volume = {Part F1286},
   year = {2017},
}
@article{Lian2017,
   abstract = {Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.},
   author = {Xiangru Lian and Ce Zhang and Huan Zhang and Cho Jui Hsieh and Wei Zhang and Ji Liu},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {5331-5341},
   title = {Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent},
   volume = {2017-Decem},
   year = {2017},
}
@article{Niu2011,
   abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve stateof-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performancedestroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
   author = {Feng Niu and Benjamin Recht and Christopher RÃ© and Stephen J. Wright},
   isbn = {9781618395993},
   journal = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
   keywords = {incremental gradient methods,machine learning,multicore,parallel computing},
   pages = {1-23},
   title = {HOGWILD!: A lock-free approach to parallelizing Stochastic Gradient Descent},
   year = {2011},
}
@article{Ma2018,
   abstract = {The interest and demand for training deep neural networks have been experiencing rapid growth, spanning a wide range of applications in both academia and industry. However, training them distributed and at scale remains difficult due to the complex ecosystem of tools and hardware involved. One consequence is that the responsibility of orchestrating these complex components is often left to one-off scripts and glue code customized for specific problems. To address these restrictions, we introduce \emph\{Alchemist\} - an internal service built at Apple from the ground up for \emph\{easy\}, \emph\{fast\}, and \emph\{scalable\} distributed training. We discuss its design, implementation, and examples of running different flavors of distributed training. We also present case studies of its internal adoption in the development of autonomous systems, where training times have been reduced by 10x to keep up with the ever-growing data collection.},
   author = {Minghuang Ma and Hadi Pouransari and Daniel Chao and Saurabh Adya and Santiago Akle Serrano and Yi Qin and Dan Gimnicher and Dominic Walsh},
   issn = {2331-8422},
   title = {Democratizing Production-Scale Distributed Deep Learning},
   url = {http://arxiv.org/abs/1811.00143},
   year = {2018},
}
@article{Xu2020,
   abstract = {In data-parallel synchronous training of deep neural networks, different devices (replicas) run the same program with different partitions of the training batch, but weight update computation is repeated on all replicas, because the weights do not have a batch dimension to partition. This can be a bottleneck for performance and scalability in typical language models with large weights, and models with small per-replica batch size which is typical in large-scale training. This paper presents an approach to automatically shard the weight update computation across replicas with efficient communication primitives and data formatting, using static analysis and transformations on the training computation graph. We show this technique achieves substantial speedups on typical image and language models on Cloud TPUs, requiring no change to model code. This technique helps close the gap between traditionally expensive (ADAM) and cheap (SGD) optimizers, as they will only take a small part of training step time and have similar peak memory usage. It helped us to achieve state-of-the-art training performance in Google's MLPerf 0.6 submission.},
   author = {Yuanzhong Xu and HyoukJoong Lee and Dehao Chen and Hongjun Choi and Blake Hechtman and Shibo Wang},
   issn = {2331-8422},
   title = {Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training},
   url = {http://arxiv.org/abs/2004.13336},
   year = {2020},
}
@article{Tang2018,
   abstract = {Optimizing distributed learning systems is an art of balancing between computation and communication. There have been two lines of research that try to deal with slower networks: communication compression for low bandwidth networks, and decentralization for high latency networks. In this paper, We explore a natural question: can the combination of both techniques lead to a system that is robust to both bandwidth and latency? Although the system implication of such combination is trivial, the underlying theoretical principle and algorithm design is challenging: unlike centralized algorithms, simply compressing exchanged information, even in an unbiased stochastic way, within the decentralized network would accumulate the error and fail to converge. In this paper, we develop a framework of compressed, decentralized training and propose two different strategies, which we call extrapolation compression and difference compression. We analyze both algorithms and prove both converge at the rate of O(1/nT) where n is the number of workers and T is the number of iterations, matching the convergence rate for full precision, centralized training. We validate our algorithms and find that our proposed algorithm outperforms the best of merely decentralized and merely quantized algorithm significantly for networks with both high latency and low bandwidth.},
   author = {Hanlin Tang and Shaoduo Gan and Ce Zhang and Tong Zhang and Ji Liu},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {7652-7662},
   title = {Communication compression for decentralized training},
   volume = {2018-Decem},
   year = {2018},
}
@article{Perozzi2014,
   abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection. Â© 2014 ACM.},
   author = {Bryan Perozzi and Rami Al-Rfou and Steven Skiena},
   doi = {10.1145/2623330.2623732},
   isbn = {9781450329569},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {deep learning,latent representations,learning with partial labels,network classification,online learning,social networks},
   pages = {701-710},
   title = {DeepWalk: Online learning of social representations},
   year = {2014},
}
@article{Rogers2020,
   abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
   author = {Anna Rogers and Olga Kovaleva and Anna Rumshisky},
   doi = {10.1162/tacl_a_00349},
   issn = {2307387X},
   journal = {Transactions of the Association for Computational Linguistics},
   pages = {842-866},
   title = {A primer in bertology: What we know about how bert works},
   volume = {8},
   year = {2020},
}
@article{Shi2015,
   abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
   author = {Xingjian Shi and Zhourong Chen and Hao Wang and Dit Yan Yeung and Wai Kin Wong and Wang Chun Woo},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {802-810},
   title = {Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
   volume = {2015-Janua},
   year = {2015},
}
@article{Hennigh2021,
   abstract = {We present SimNet, an AI-driven multi-physics simulation framework, to accelerate simulations across a wide range of disciplines in science and engineering. Compared to traditional numerical solvers, SimNet addresses a wide range of use cases - coupled forward simulations without any training data, inverse and data assimilation problems. SimNet offers fast turnaround time by enabling parameterized system representation that solves for multiple configurations simultaneously, as opposed to the traditional solvers that solve for one configuration at a time. SimNet is integrated with parameterized constructive solid geometry as well as STL modules to generate point clouds. Furthermore, it is customizable with APIs that enable user extensions to geometry, physics and network architecture. It has advanced network architectures that are optimized for high-performance GPU computing, and offers scalable performance for multi-GPU and multi-Node implementation with accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this paper we review the neural network solver methodology, the SimNet architecture, and the various features that are needed for effective solution of the PDEs. We present real-world use cases that range from challenging forward multi-physics simulations with turbulence and complex 3D geometries, to industrial design optimization and inverse problems that are not addressed efficiently by the traditional solvers. Extensive comparisons of SimNet results with open source and commercial solvers show good correlation. The SimNet source code is available at https://developer.nvidia.com/simnet.},
   author = {Oliver Hennigh and Susheela Narasimhan and Mohammad Amin Nabian and Akshay Subramaniam and Kaustubh Tangsali and Zhiwei Fang and Max Rietmann and Wonmin Byeon and Sanjay Choudhry},
   doi = {10.1007/978-3-030-77977-1_36},
   isbn = {9783030779764},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {447-461},
   title = {NVIDIA SimNetâ¢: An AI-Accelerated Multi-Physics Simulation Framework},
   volume = {12746 LNCS},
   year = {2021},
}
@article{Britz2017,
   abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.},
   author = {Denny Britz and Anna Goldie and Minh Thang Luong and Quoc V. Le},
   doi = {10.18653/v1/d17-1151},
   isbn = {9781945626838},
   journal = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
   pages = {1442-1451},
   title = {Massive exploration of neural machine translation architectures},
   year = {2017},
}
@article{Pedram2017,
   abstract = {Unlike traditional dark silicon works that attack the computing logic, this article puts a focus on the memory part, which dissipates most of the energy for memory-bound CPU applications. This article discusses the dark memory state and present Pareto curves for compute units, accelerators, and on-chip memory, and motivates the need for HW/SW codesign for parallelism and locality. -Muhammad Shafique, Vienna University of Technology},
   author = {Ardavan Pedram and Stephen Richardson and Mark Horowitz and Sameh Galal and Shahar Kvatinsky},
   doi = {10.1109/MDAT.2016.2573586},
   issn = {21682356},
   issue = {2},
   journal = {IEEE Design and Test},
   keywords = {Dark Memory,Dark Silicon,Energy Efficient,High performance,Memory,Parallelism},
   pages = {39-50},
   title = {Dark memory and accelerator-rich system optimization in the dark silicon era},
   volume = {34},
   year = {2017},
}
@article{Zhu2017,
   abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X â Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y â X and introduce a cycle consistency loss to push F(G(X)) â X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
   author = {Jun Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
   doi = {10.1109/ICCV.2017.244},
   isbn = {9781538610329},
   issn = {15505499},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   pages = {2242-2251},
   title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
   volume = {2017-Octob},
   year = {2017},
}
@article{Zhang2016,
   abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output non-linearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.},
   author = {Saizheng Zhang and Yuhuai Wu and Tong Che and Zhouhan Lin and Roland Memisevic and Ruslan Salakhutdinov and Yoshua Bengio},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1830-1838},
   title = {Architectural complexity measures of recurrent neural networks},
   year = {2016},
}
@article{Li2017,
   abstract = {Accelerating the inference of a trained DNN is a well studied subject. In this paper we switch the focus to the training of DNNs. The training phase is compute intensive, demands complicated data communication, and contains multiple levels of data dependencies and parallelism. This paper presents an algorithm/architecture space exploration of efficient accelerators to achieve better network convergence rates and higher energy efficiency for training DNNs. We further demonstrate that an architecture with hierarchical support for collective communication semantics provides flexibility in training various networks performing both stochastic and batched gradient descent based techniques. Our results suggest that smaller networks favor non-batched techniques while performance for larger networks is higher using batched operations. At 45nm technology, CATERPILLAR achieves performance efficiencies of 177 GFLOPS/W at over 80% utilization for SGD training on small networks and 211 GFLOPS/W at over 90% utilization for pipelined SGD/CP training on larger networks using a total area of 103.2 mm2 and 178.9 mm2 respectively.},
   author = {Yuanfang Li and Ardavan Pedram},
   doi = {10.1109/ASAP.2017.7995252},
   isbn = {9781509048250},
   issn = {10636862},
   journal = {Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors},
   pages = {1-10},
   title = {CATERPILLAR: Coarse Grain Reconfigurable Architecture for accelerating the training of Deep Neural Networks},
   year = {2017},
}
@article{Borovykh2017,
   abstract = {Forecasting financial time series using past observations has been a significant topic of interest. While temporal relationships in the data exist, they are difficult to analyze and predict accurately due to the non-linear trends and noise present in the series. We propose to learn these dependencies by a convolutional neural network. In particular the focus is on multivariate time series forecasting. Effectively, we use multiple financial time series as input in the neural network, thus conditioning the forecast of a time series x(t) on both its own history as well as that of a second (or third) time series y(t). Training a model on multiple stock series allows the network to exploit the correlation structure between these series so that the network can learn the market dynamics in shorter sequences of data. We show that long-term temporal dependencies in and between financial time series can be learned by means of a deep convolutional neural network based on the WaveNet model [2]. The network makes use of dilated convolutions applied to multiple time series so that the receptive field of the network is wide enough to learn both short and long-term dependencies. The architecture includes batch normalization and uses a 1 Ã k convolution with parametrized skip connections from the input time series as well as the time series we condition on, in this way learning long-term interdependencies in an efficient manner [1]. This improves the forecast, while at the same time limiting the requirement for a long historical price series and reducing the noise. Knowing the strong performance of CNNs on classification problems we show that they can be applied successfully to forecasting financial time series, without the need of large samples of data. We compare the performance of the WaveNet model to a state-of-the-art fully convolutional network (FCN), and an autoregressive model popular in econometrics and show that our model is much better able to learn important dependencies in between financial time series resulting in a more robust and accurate forecast.},
   author = {Anastasia Borovykh and Sander Bohte and Cornelis W. Oosterlee},
   isbn = {9783319686110},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Convolutional neural network,Financial time series},
   pages = {729-730},
   title = {Conditional time series forecasting with convolutional neural networks},
   volume = {10614 LNCS},
   year = {2017},
}
@article{Yi2014,
   abstract = {Homomorphic encryption is a very useful tool with a number of attractive applications. However, the applications are limited by the fact that only one operation is possible (usually addition or multiplication in the plaintext space) to be able to manipulate the plaintext by using only the ciphertext. What would really be useful is to be able to utilize both addition and multiplication simultaneously. This would permit more manipulation of the plaintext by modifying the ciphertext. In fact, this would allow one without the secret key to compute any efficiently computable function on the plaintext when given only the ciphertext. In this chapter, we introduce fully homomorphic encryption (FHE) techniques, which allow one to evaluate both addition and multiplication of plaintext, while remaining encrypted. The concept of FHE was introduced by Rivest [14] under the name privacy homomorphisms. The problem of constructing a scheme with these properties remained unsolved until 2009, when Gentry [6] presented his breakthrough result. His scheme allows arbitrary computation on the ciphertexts and it yields the correct result when decrypted. This chapter begins with an introduction of FHE model and definitions, followed by the construction of FHE scheme over integers.},
   author = {Xun Yi and Russell Paulet and Elisa Bertino},
   doi = {10.1007/978-3-319-12229-8_3},
   issn = {21915776},
   issue = {9783319122281},
   journal = {SpringerBriefs in Computer Science},
   keywords = {Boolean function,Encrypt data,Encryption scheme,Ideal lattice,Ring homomorphism},
   pages = {47-66},
   title = {Fully homomorphic encryption},
   volume = {0},
   year = {2014},
}
@article{Lattner2020,
   abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
   author = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
   issn = {2331-8422},
   title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
   url = {http://arxiv.org/abs/2002.11054},
   year = {2020},
}
@article{Li2018,
   abstract = {Distributed training of deep nets is an important technique to address some of the present day computing challenges like memory consumption and computational demands. Classical distributed approaches, synchronous or asynchronous, are based on the parameter server architecture, i.e., worker nodes compute gradients which are communicated to the parameter server while updated parameters are returned. Recently, distributed training with AllReduce operations gained popularity as well. While many of those operations seem appealing, little is reported about wall-clock training time improvements. In this paper, we carefully analyze the AllReduce based setup, propose timing models which include network latency, bandwidth, cluster size and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training. Specifically, for a setup consisting of a four-node GPU cluster we show wall-clock time training improvements of up to 5.4Ã compared to conventional approaches.},
   author = {Youjie Li and Mingchao Yu and Songze Li and Salman Avestimehr and Nam Sung Kim and Alexander Schwing},
   issn = {10495258},
   issue = {NeurIPS},
   journal = {Advances in Neural Information Processing Systems},
   pages = {8045-8056},
   title = {PIPE-SGD: A decentralized pipelined SGD framework for distributed deep net training},
   volume = {2018-Decem},
   year = {2018},
}
@article{Ullrich2017,
   abstract = {The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version ofâsoft weight-sharingâ (Nowlan & Hinton, 1992). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.},
   author = {Karen Ullrich and Max Welling and Edward Meeds},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   pages = {1-16},
   title = {Soft weight-sharing for neural network compression},
   year = {2017},
}
@article{Gale2019,
   abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
   author = {Trevor Gale and Erich Elsen and Sara Hooker},
   title = {The State of Sparsity in Deep Neural Networks},
   url = {http://arxiv.org/abs/1902.09574},
   year = {2019},
}
@article{Yang2019,
   abstract = {Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.},
   author = {Guandao Yang and Tianyi Zhang and Polina Kirichenko and Junwen Bai and Andrew Gordon Wilson and Christopher de Sa},
   isbn = {9781510886988},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   pages = {12125-12151},
   title = {Swalp: Stochastic weight averaging in low-precision training},
   volume = {2019-June},
   year = {2019},
}
@article{Horne1996,
   abstract = {In this paper the efficiency of recurrent neural network implementations of m-state finite state machines will be explored. Specifically, it will be shown that the node complexity for the unrestricted case can be bounded above by O(âm). It will also be shown that the node complexity is O(âm log m) when the weights and thresholds are restricted to the set \{-1, 1\}, and O(m) when the fan-in is restricted to two. Matching lower bounds will be provided for each of these upper bounds assuming that the state of the FSM can be encoded in a subset of the nodes of size [log m].},
   author = {Bill G. Horne and Don R. Hush},
   doi = {10.1016/0893-6080(95)00095-X},
   issn = {08936080},
   issue = {2},
   journal = {Neural Networks},
   keywords = {automata theory,circuit complexity,finite state machines,logic synthesis,neural networks,recurrent neural networks},
   pages = {243-252},
   title = {Bounds on the complexity of recurrent neural network implementations of finite state machines},
   volume = {9},
   year = {1996},
}
@article{,
   abstract = {Low-precision computation is often used to lower the time and energy cost of machine learning, and recently hardware accelerators have been developed to support it. Still, it has been used primarily for inference - not training. Previous low-precision training algorithms suffered from a fundamental tradeoff: as the number of bits of precision is lowered, quantization noise is added to the model, which limits statistical accuracy. To address this issue, we describe a simple low-precision stochastic gradient descent variant called HALP. HALP converges at the same theoretical rate as full-precision algorithms despite the noise introduced by using low precision throughout execution. The key idea is to use SVRG to reduce gradient variance, and to combine this with a novel technique called bit centering to reduce quantization error. We show that on the CPU, HALP can run up to $4 \times$ faster than full-precision SVRG and can match its convergence trajectory. We implemented HALP in TensorQuant, and show that it exceeds the validation performance of plain low-precision SGD on two deep learning tasks.},
   author = {Christopher De Sa and Megan Leszczynski and Jian Zhang and Alana Marzoev and Christopher R. Aberger and Kunle Olukotun and Christopher RÃ©},
   issn = {2331-8422},
   title = {High-Accuracy Low-Precision Training},
   url = {http://arxiv.org/abs/1803.03383},
   year = {2018},
}
@article{Haythornthwaite2010,
   abstract = {The empirical studies, theoretical perspectives and analytical tools associated with social network analysis (SNA) comprise a wealth of knowledge that can be drawn on for interpreting, analyzing and designing networked learning. In its essentials of understanding the network connections between people, or other network nodes, SNA seems a natural addition to the networked learning researcherâs toolkit. This paper builds on the networked learning âhotseatâ discussion held online in October 2009 as one of the preliminaries to the 2010 Networked Learning Conference. The discussion aimed to explore social network principles as a way to address questions about networked learning. This paper follows up that discussion, bringing together and expanding on topics discussed during the hotseat. Our thanks go to the participants in that session for raising important and challenging questions about networks, and networked learning. Keywords},
   author = {Caroline Haythornthwaite},
   isbn = {9781862202252},
   keywords = {hotseat discussion,networked learning,social network analysis},
   pages = {183-190},
   title = {Social Networks and Learning Networks : Using social network perspectives to understand social learning},
   year = {2010},
}
@article{Gao2019,
   abstract = {The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: Intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner. This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For interlayer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer requirements and pipeline delays. We also make inter-layer pipelining applicable to NNs with complex DAG structures. These optimizations improve the performance of tiled NN accelerators by 2Ã and reduce their energy consumption by 45% across a wide range of NNs. The effectiveness of our optimizations also increases with the NN size and complexity.},
   author = {Mingyu Gao and Xuan Yang and Jing Pu and Mark Horowitz and Christos Kozyrakis},
   doi = {10.1145/3297858.3304014},
   isbn = {9781450362405},
   journal = {International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
   keywords = {dataflow,neural networks,parallelism},
   pages = {807-820},
   title = {Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators},
   year = {2019},
}
@article{,
   author = {Transposition Address and References Sources},
   title = {Row- and column-major order},
}
@article{Lattner2021,
   abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
   author = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
   doi = {10.1109/CGO51591.2021.9370308},
   isbn = {9781728186139},
   journal = {CGO 2021 - Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
   pages = {2-14},
   title = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
   year = {2021},
}
@article{Nasution2016,
   abstract = {Social Network Mining (SNM) has become one of the main themes in big data agenda. As a resultant network, we can extract social network from different sources of information, but the information sources were growing dynamically require a flexible approach. To determine the appropriate approach needs the data engineering in order to get the behavior associated with the data. Each social network has the resources and the information source, but the relationship between resources and information sources requires explanation. This paper aimed to address the behavior of the resource as a part of social network analysis (SNA) in the growth of social networks by using the statistical calculations to explain the evolutionary mechanisms. To represent the analysis unit of the SNA, this paper only considers the degree of a vertex, where it is the core of all the analysis in the SNA and it is basic for defining the relation between resources and SNA in SNM. There is a strong effect on the growth of the resources of social networks. In total, the behavior of resources has positive effects. Thus, different information sources behave similarly and have relations with SNA.},
   author = {Mahyuddin K.M. Nasution},
   doi = {10.18517/ijaseit.6.6.1390},
   issn = {24606952},
   issue = {6},
   journal = {International Journal on Advanced Science, Engineering and Information Technology},
   keywords = {Actor,Edge,Extraction,Multiple regression,Vertex,Î±-Cronbach},
   pages = {975-981},
   title = {Social network mining (SNM): A definition of relation between the resources and SNA},
   volume = {6},
   year = {2016},
}
@article{Jean2015,
   abstract = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English!German and English!French translation tasks of WMT'14.},
   author = {Sebastien Jean and Kyunghyun Cho and Roland Memisevic and Yoshua Bengio},
   doi = {10.3115/v1/p15-1001},
   isbn = {9781941643723},
   issn = {2733-9211},
   journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
   pages = {1-10},
   title = {On using very large target vocabulary for neural machine translation},
   volume = {1},
   year = {2015},
}
@article{Gibson2020,
   abstract = {DataOps is the new black, providing a combination of DevOps and agile practices to automate the creation and management of data, analytics, and visualization on cloud platforms. DataOps is revolutionizing the way we deploy and manage data platforms, and SAS Â® Viya Â® is embracing DataOps within it's core. In this session, I outline some of the key components required to adopt the new DataOps way of working and provide examples of the benefits each component will deliver. If you want to understand what DataOps is and how you should leverage it to build and manage better data platforms and processes, then this is the session for you.},
   author = {Shane Gibson},
   isbn = {9781492049227},
   issue = {September 2018},
   keywords = {Agile,DataOps,DevOps,Lean},
   pages = {1-10},
   title = {Exploring DataOps in the Brave New World of Agile and Cloud Delivery},
   year = {2020},
}
@article{Sculley2015,
   abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
   author = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young and Jean FranÃ§ois Crespo and Dan Dennison},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {2503-2511},
   title = {Hidden technical debt in machine learning systems},
   volume = {2015-Janua},
   year = {2015},
}
@article{,
   title = {Introduction to Kubeflow},
}
@article{Reuther2021,
   abstract = {Over the past several years, new machine learning accelerators were being announced and released every month for a variety of applications from speech recognition, video object detection, assisted driving, and many data center applications. This paper updates the survey of AI accelerators and processors from past two years. This paper collects and summarizes the cur-rent commercial accelerators that have been publicly announced with peak performance and power consumption numbers. The performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. This year, we also compile a list of benchmarking performance results and compute the computational efficiency with respect to peak performance.},
   author = {Albert Reuther and Peter Michaleas and Michael Jones and Vijay Gadepally and Siddharth Samsi and Jeremy Kepner},
   doi = {10.1109/hpec49654.2021.9622867},
   isbn = {9781665423694},
   pages = {1-9},
   title = {AI Accelerator Survey and Trends},
   year = {2021},
}
@article{Dai2020,
   abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
   author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
   doi = {10.18653/v1/p19-1285},
   isbn = {9781950737482},
   journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
   pages = {2978-2988},
   title = {Transformer-XL: Attentive language models beyond a fixed-length context},
   year = {2020},
}
@article{Gehrmann2021,
   abstract = {We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.},
   author = {Sebastian Gehrmann and Tosin Adewumi and Karmanya Aggarwal and Pawan Sasanka Ammanamanchi and Anuoluwapo Aremu and Antoine Bosselut and Khyathi Raghavi Chandu and Miruna-Adriana Clinciu and Dipanjan Das and Kaustubh Dhole and Wanyu Du and Esin Durmus and OndÅej DuÅ¡ek and Chris Chinenye Emezue and Varun Gangal and Cristina Garbacea and Tatsunori Hashimoto and Yufang Hou and Yacine Jernite and Harsh Jhamtani and Yangfeng Ji and Shailza Jolly and Mihir Kale and Dhruv Kumar and Faisal Ladhak and Aman Madaan and Mounica Maddela and Khyati Mahajan and Saad Mahamood and Bodhisattwa Prasad Majumder and Pedro Henrique Martins and Angelina McMillan-Major and Simon Mille and Emiel van Miltenburg and Moin Nadeem and Shashi Narayan and Vitaly Nikolaev and Andre Niyongabo Rubungo and Salomey Osei and Ankur Parikh and Laura Perez-Beltrachini and Niranjan Ramesh Rao and Vikas Raunak and Juan Diego Rodriguez and Sashank Santhanam and JoÃ£o Sedoc and Thibault Sellam and Samira Shaikh and Anastasia Shimorina and Marco Antonio Sobrevilla Cabezudo and Hendrik Strobelt and Nishant Subramani and Wei Xu and Diyi Yang and Akhila Yerukola and Jiawei Zhou},
   doi = {10.18653/v1/2021.gem-1.10},
   isbn = {9781954085671},
   pages = {96-120},
   title = {The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics},
   year = {2021},
}
@article{Li2018,
   abstract = {Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines.},
   author = {Yaguang Li and Rose Yu and Cyrus Shahabi and Yan Liu},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
   pages = {1-16},
   title = {Diffusion convolutional recurrent neural network: Data-driven traffic forecasting},
   year = {2018},
}
@article{Zhang2020,
   abstract = {Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language modeling. In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efficiency. Extensive experiments on BERT show that the proposed method achieves a 24% time reduction on average per sample and allows the pre-training to be 2.5Ã faster than the baseline to get a similar accuracy on downstream tasks. While being faster, our pre-trained models are equipped with strong knowledge transferability, achieving comparable and sometimes higher GLUE score than the baseline when pre-trained with the same number of samples.},
   author = {Minjia Zhang and Yuxiong He},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {Accelerating training of transformer-based language models with progressive layer dropping},
   volume = {2020-Decem},
   year = {2020},
}
@article{Peng2019,
   abstract = {We present ByteScheduler, a generic communication scheduler for distributed DNN training acceleration. ByteScheduler is based on our principled analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead. To make ByteScheduler work generally for various DNN training frameworks, we introduce a unified abstraction and a Dependency Proxy mechanism to enable communication scheduling without breaking the original dependencies in framework engines. We further introduce a Bayesian Optimization approach to auto-tune tensor partition size and other parameters for different training models under various networking conditions. ByteScheduler now supports TensorFlow, PyTorch, and MXNet without modifying their source code, and works well with both Parameter Server (PS) and all-reduce architectures for gradient synchronization, using either TCP or RDMA. Our experiments show that ByteScheduler accelerates training with all experimented system configurations and DNN models, by up to 196% (or 2.96Ã of original speed).},
   author = {Yanghua Peng and Yibo Zhu and Yangrui Chen and Yixin Bao and Bairen Yi and Chang Lan and Chuan Wu and Chuanxiong Guo},
   doi = {10.1145/3341301.3359642},
   isbn = {9781450368735},
   journal = {SOSP 2019 - Proceedings of the 27th ACM Symposium on Operating Systems Principles},
   keywords = {Communication scheduling,ML frameworks},
   pages = {16-29},
   title = {A generic communication scheduler for distributed DNN training acceleration},
   year = {2019},
}
@article{Shoeybi2019,
   abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).},
   author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
   title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
   url = {http://arxiv.org/abs/1909.08053},
   year = {2019},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Åukasz Kaiser and Illia Polosukhin},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {5999-6009},
   title = {Attention is all you need},
   volume = {2017-Decem},
   year = {2017},
}
@article{Raissi2019,
   abstract = {We introduce physics-informed neural networks â neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit RungeâKutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reactionâdiffusion systems, and the propagation of nonlinear shallow-water waves.},
   author = {M. Raissi and P. Perdikaris and G. E. Karniadakis},
   doi = {10.1016/j.jcp.2018.10.045},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,RungeâKutta methods},
   pages = {686-707},
   publisher = {Elsevier Inc.},
   title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
   volume = {378},
   url = {https://doi.org/10.1016/j.jcp.2018.10.045},
   year = {2019},
}
@article{Lu2021,
   abstract = {Inverse design arises in a variety of areas in engineering such as acoustic, mechanics, thermal/electronic transport, electromagnetism, and optics. Topology optimization is a major form of inverse design, where we optimize a designed geometry to achieve targeted properties and the geometry is parameterized by a density function. This optimization is challenging, because it has a very high dimensionality and is usually constrained by partial differential equations (PDEs) and additional inequalities. Here, we propose a new deep learning method -- physics-informed neural networks with hard constraints (hPINNs) -- for solving topology optimization. hPINN leverages the recent development of PINNs for solving PDEs, and thus does not rely on any numerical PDE solver. However, all the constraints in PINNs are soft constraints, and hence we impose hard constraints by using the penalty method and the augmented Lagrangian method. We demonstrate the effectiveness of hPINN for a holography problem in optics and a fluid problem of Stokes flow. We achieve the same objective as conventional PDE-constrained optimization methods based on adjoint methods and numerical PDE solvers, but find that the design obtained from hPINN is often simpler and smoother for problems whose solution is not unique. Moreover, the implementation of inverse design with hPINN can be easier than that of conventional methods.},
   author = {Lu Lu and RaphaÃ«l Pestourie and Wenjie Yao and Zhicheng Wang and Francesc Verdugo and Steven G. Johnson},
   doi = {10.1137/21m1397908},
   issn = {1064-8275},
   issue = {6},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {1,35r30,65k10,68t20,ams subject classifications,augmented lagrangian method,informed neural networks,introduction,inverse design,metamaterials are artificial materials,partial differential equations,penalty method,physics-,that achieve targeted,topology optimization},
   pages = {B1105-B1132},
   title = {Physics-Informed Neural Networks with Hard Constraints for Inverse Design},
   volume = {43},
   year = {2021},
}
@article{Huang2019,
   abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
   author = {Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and Hyouk Joong Lee and Jiquan Ngiam and Quoc V Le and Yonghui Wu and Zhifeng Chen},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1-11},
   title = {GPipe: Efficient training of giant neural networks using pipeline parallelism},
   volume = {32},
   year = {2019},
}
@article{Mirhoseini2017,
   abstract = {The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUS. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.},
   author = {Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy Bengio and Jeff Dean},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   pages = {3748-3757},
   title = {Device placement optimization with reinforcement learning},
   volume = {5},
   year = {2017},
}
@article{Lan2019,
   abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
   author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
   pages = {1-17},
   title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
   url = {http://arxiv.org/abs/1909.11942},
   year = {2019},
}
@article{,
   abstract = {Since the introduction of the pioneering CUDA GPU Computing platform over 10 years ago, each new NVIDIAÂ® GPU generation has delivered higher application performance, improved power efficiency, added important new compute features, and simplified GPU programming. Today, NVIDIA GPUs accelerate thousands of High Performance Computing (HPC), data center, and machine learning applications. NVIDIA GPUs have become the leading computational engines powering the Artificial Intelligence (AI) revolution. NVIDIA GPUs accelerate numerous deep learning systems and applications including autonomous vehicle platforms, high-accuracy speech, image, and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations, to name just a few. The new NVIDIAÂ® TeslaÂ® V100 accelerator (shown in Figure 1) incorporates the powerful new Voltaâ¢ GV100 GPU. GV100 not only builds upon the advances of its predecessor, the Pascalâ¢ GP100 GPU, it significantly improves performance and scalability, and adds many new features that improve programmability. These advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications. This white paper presents the Tesla V100 accelerator and the Volta GV100 GPU architecture.},
   author = {NVIDIA Corporation},
   issue = {v1.1},
   journal = {White Paper},
   keywords = {Volta},
   pages = {53},
   title = {Nvidia Tesla V100 GPU Volta Architecture},
   url = {http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf%0Ahttp://www.nvidia.com/content/gated-pdfs/Volta-Architecture-Whitepaper-v1.1.pdf},
   year = {2017},
}
@article{,
   abstract = {Executive Summary InfiniBand is a powerful new architecture designed to support I/O connectivity for the Internet infrastructure. InfiniBand is supported by all the major OEM server vendors as a means to expand beyond and create the next generation I/O interconnect standard in servers. For the first time, a high volume, industry standard I/O interconnect extends the role of traditional "in the box" busses. InfiniBand is unique in providing both, an "in the box" backplane solution, an external interconnect, and "Bandwidth Out of the box", thus it provides connectivity in a way previously reserved only for traditional networking interconnects. This unification of I/O and system area networking requires a new architecture that supports the needs of these two previously separate domains. Underlying this major I/O transition is InfiniBand's ability to support the Internet's requirement for RAS: reliability, availability, and serviceability. This white paper discusses the features and capabilities which demonstrate InfiniBand's superior abilities to support RAS relative to the legacy PCI bus and other proprietary switch fabric and I/O solutions. Further, it provides an overview of how the InfiniBand architecture supports a comprehensive silicon, software, and system solution. The comprehensive nature of the architecture is illustrated by providing an overview of the major sections of the InfiniBand 1.1 specification. The scope of the 1.1 specification ranges from industry standard electrical interfaces and mechanical connectors to well defined software and management interfaces. The paper is divided into four sections. The introduction sets the stage for InfiniBand and illustrates why all the major server vendors have made the decision to embrace this new standard. The next section reviews the effect Infini-Band will have on various markets that are currently being addressed by legacy technologies. The third section provides a comparison between switch fabrics and bus architecture in general and then delves into details comparing InfiniBand to PCI and other proprietary solutions. The final section goes into details about the architecture, reviewing at a high level the most important features of InfiniBand.},
   author = {Mellanox Technologies},
   journal = {Technical Report},
   pages = {1-20},
   title = {Introduction to InfiniBand},
   year = {2003},
}
@article{Tallent2018,
   abstract = {Scaling deep learning workloads across multiple GPUs on a single node has become increasingly important in data analytics. A key question is how well a PCIe-based GPU interconnect can perform relative to a custom high-performance interconnect such as NVIDIAâs NVLink. This paper evaluates two such on-node interconnects for eight NVIDIA Pascal P100 GPUs: (a) the NVIDIA DGX-1âs NVLink 1.0 âhybrid cube meshâ; and (b) the Cirrascale GX8âs two-level PCIe tree using dual SR3615 switch risers. To show the effects of a range of neural network workloads, we define a parameterized version of the popular ResNet architecture. We define a workload intensity metric that characterizes the expected computation/communication ratio; we also locate AlexNet and GoogLeNet within that space. As expected, the DGX-1 typically has superior performance. However, the GX8 is very competitive on all ResNet workloads. With 8 GPUs, the GX8 can outperform the DGX-1 on all-to-all reductions by 10% for medium-sized payloads; and in rare cases, the GX8 slightly outperforms on ResNet.},
   author = {Nathan R. Tallent and Nitin A. Gawande and Charles Siegel and Abhinav Vishnu and Adolfy Hoisie},
   doi = {10.1007/978-3-319-72971-8_1},
   isbn = {9783319729701},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Cirrascale SR3615 switch riser,Convolutional neural networks,GPU interconnects,NVIDIA DGX-1,NVIDIA NVLink},
   pages = {3-21},
   title = {Evaluating On-Node GPU interconnects for deep learning workloads},
   volume = {10724 LNCS},
   year = {2018},
}
@article{Weng2021,
   abstract = {Encoder-decoder networks are state-of-the-art approaches to biomedical image segmentation, but have two problems: i.e., the widely used pooling operations may discard spatial information, and therefore low-level semantics are lost. Feature fusion methods can mitigate these problems but feature maps of different scales cannot be easily fused because down- and upsampling change the spatial resolution of feature map. To address these issues, we propose INet, which enlarges receptive fields by increasing the kernel sizes of convolutional layers in steps (e.g., from 3times 3 to 7times 7 and then 15times 15 ) instead of downsampling. Inspired by an Inception module, INet extracts features by kernels of different sizes through concatenating the output feature maps of all preceding convolutional layers. We also find that the large kernel makes the network feasible for biomedical image segmentation. In addition, INet uses two overlapping max-poolings, i.e., max-poolings with stride 1, to extract the sharpest features. Fixed-size and fixed-channel feature maps enable INet to concatenate feature maps and add multiple shortcuts across layers. In this way, INet can recover low-level semantics by concatenating the feature maps of all preceding layers and expedite the training by adding multiple shortcuts. Because INet has additional residual shortcuts, we compare INet with a UNet system that also has residual shortcuts (ResUNet). To confirm INet as a backbone architecture for biomedical image segmentation, we implement dense connections on INet (called DenseINet) and compare it to a DenseUNet system with residual shortcuts (ResDenseUNet). INet and DenseINet require 16.9% and 37.6% fewer parameters than ResUNet and ResDenseUNet, respectively. In comparison with six encoder-decoder approaches using nine public datasets, INet and DenseINet demonstrate efficient improvements in biomedical image segmentation. INet outperforms DeepLabV3, which implementing atrous convolution instead of downsampling to increase receptive fields. INet also outperforms two recent methods (named HRNet and MS-NAS) that maintain high-resolution representations and repeatedly exchange the information across resolutions.},
   author = {Weihao Weng and Xin Zhu},
   doi = {10.1109/ACCESS.2021.3053408},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Biomedical image,convolutional networks,encoder-decoder networks,semantic segmentation},
   pages = {16591-16603},
   title = {INet: Convolutional Networks for Biomedical Image Segmentation},
   volume = {9},
   year = {2021},
}
@article{,
   doi = {10.1007/springerreference_12569},
   journal = {SpringerReference},
   title = {Direct Memory Access},
   year = {2011},
}
@article{Chiu2021,
   abstract = {In this study, novel physics-informed neural network (PINN) methods for coupling neighboring support points and automatic differentiation (AD) through Taylor series expansion are proposed to allow efficient training with improved accuracy. The computation of differential operators required for PINNs loss evaluation at collocation points are conventionally obtained via AD. Although AD has the advantage of being able to compute the exact gradients at any point, such PINNs can only achieve high accuracies with large numbers of collocation points, otherwise they are prone to optimizing towards unphysical solution. To make PINN training fast, the dual ideas of using numerical differentiation (ND)-inspired method and coupling it with AD are employed to define the loss function. The ND-based formulation for training loss can strongly link neighboring collocation points to enable efficient training in sparse sample regimes, but its accuracy is restricted by the interpolation scheme. The proposed coupled-automatic-numerical differentiation framework, labeled as can-PINN, unifies the advantages of AD and ND, providing more robust and efficient training than AD-based PINNs, while further improving accuracy by up to 1-2 orders of magnitude relative to ND-based PINNs. For a proof-of-concept demonstration of this can-scheme to fluid dynamic problems, two numerical-inspired instantiations of can-PINN schemes for the convection and pressure gradient terms were derived to solve the incompressible Navier-Stokes (N-S) equations. The superior performance of can-PINNs is demonstrated on several challenging problems, including the flow mixing phenomena, lid driven flow in a cavity, and channel flow over a backward facing step. The results reveal that for challenging problems like these, can-PINNs can consistently achieve very good accuracy whereas conventional AD-based PINNs fail.},
   author = {Pao-Hsiung Chiu and Jian Cheng Wong and Chinchun Ooi and My Ha Dao and Yew-Soon Ong},
   keywords = {coupled-automatic-numerical differentiation,inverse problem,navier-stokes equations,physics-informed neural network,taylor series expansions,training loss formulation},
   title = {CAN-PINN: A Fast Physics-Informed Neural Network Based on Coupled-Automatic-Numerical Differentiation Method},
   url = {http://arxiv.org/abs/2110.15832},
   year = {2021},
}
@article{Colombo2022,
   abstract = {In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in (i) assessing the progress of new methods along different axes and (ii) selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (e.g. GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.},
   author = {Pierre Colombo and Nathan Noiry and Ekhine Irurozki and Stephan Clemencon},
   title = {What are the best systems? New perspectives on NLP Benchmarking},
   url = {http://arxiv.org/abs/2202.03799},
   year = {2022},
}
@article{Downs1989,
   author = {Florence S. Downs},
   doi = {10.1097/00006199-198905000-00001},
   issn = {15389847},
   issue = {3},
   journal = {Nursing Research},
   pages = {133},
   pmid = {2717435},
   title = {GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND- ING},
   volume = {38},
   year = {1989},
}
@article{Mu2019,
   abstract = {We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. Through several experiments and visualizations we demonstrate that our corruptions significantly degrade performance of state-of-the-art computer vision models while preserving the semantic content of the test images. In contrast to the popular notion of adversarial robustness, our model-agnostic corruptions do not seek worst-case performance but are instead designed to be broad and diverse, capturing multiple failure modes of modern models. In fact, we find that several previously published adversarial defenses significantly degrade robustness as measured by MNIST-C. We hope that our benchmark serves as a useful tool for future work in designing systems that are able to learn robust feature representations that capture the underlying semantics of the input.},
   author = {Norman Mu and Justin Gilmer},
   doi = {10.5281/zenodo.3237938},
   title = {MNIST-C: A Robustness Benchmark for Computer Vision},
   url = {http://arxiv.org/abs/1906.02337},
   year = {2019},
}
@article{Guo2014,
   abstract = {With the rapid development of range image acquisition techniques, 3D computer vision has became a popular research area. It has numerous applications in various domains including robotics, biometrics, remote sensing, entertainment, civil construction, and medical treatment. Recently, a large number of algorithms have been proposed to address specific problems in the area of 3D computer vision. Meanwhile, several benchmark datasets have also been released to stimulate the research in this area. The availability of benchmark datasets plays an significant role in the process of technological progress. In this paper, we first introduce several major 3D acquisition techniques. We also present an overview on various popular topics in 3D computer vision including 3D object modeling, 3D model retrieval, 3D object recognition, 3D face recognition, RGB-D vision, and 3D remote sensing. Moreover, we present a contemporary summary of the existing benchmark datasets in 3D computer vision. This paper can therefore, serve as a handbook for those who are working in the related areas.},
   author = {Yulan Guo and Jun Zhang and Min Lu and Jianwei Wan and Yanxin Ma},
   doi = {10.1109/ICIEA.2014.6931468},
   isbn = {9781479943166},
   issue = {June},
   journal = {Proceedings of the 2014 9th IEEE Conference on Industrial Electronics and Applications, ICIEA 2014},
   pages = {1846-1851},
   title = {Benchmark datasets for 3D computer vision},
   year = {2014},
}
@article{Markidis2018,
   abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
   author = {Stefano Markidis and Steven Wei Der Chien and Erwin Laure and Ivy Bo Peng and Jeffrey S. Vetter},
   doi = {10.1109/IPDPSW.2018.00091},
   isbn = {9781538655559},
   journal = {Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018},
   keywords = {GEMM,GPU Programming,Mixed Precision,NVIDIA Tensor Cores},
   pages = {522-531},
   title = {NVIDIA tensor core programmability, performance & precision},
   year = {2018},
}
@article{Conneau2019,
   abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available.},
   author = {Alexis Conneau and Guillaume Lample},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {Cross-lingual language model pretraining},
   volume = {32},
   year = {2019},
}
@article{Arora2007,
   author = {Sanjeev Arora and Boaz Barak},
   issue = {January},
   title = {Computational Complexity: A Modern Approach},
   year = {2007},
}
@article{Peng2018,
   abstract = {Deep learning workloads are common in todayâs production clusters due to the proliferation of deep learning driven AI services (e.g., speech recognition, machine translation). A deep learning training job is resource-intensive and time-consuming. Efficient resource scheduling is the key to the maximal performance of a deep learning cluster. Existing cluster schedulers are largely not tailored to deep learning jobs, and typically specifying a fixed amount of resources for each job, prohibiting high resource efficiency and job performance. This paper proposes Optimus, a customized job scheduler for deep learning clusters, which minimizes job training time based on online resource-performance models. Optimus uses online fitting to predict model convergence during training, and sets up performance models to accurately estimate training speed as a function of allocated resources in each job. Based on the models, a simple yet effective method is designed and used for dynamically allocating resources and placing deep learning tasks to minimize job completion time. We implement Optimus on top of Kubernetes, a cluster manager for container orchestration, and experiment on a deep learning cluster with 7 CPU servers and 6 GPU servers, running 9 training jobs using the MXNet framework. Results show that Optimus outperforms representative cluster schedulers by about 139% and 63% in terms of job completion time and makespan, respectively.},
   author = {Yanghua Peng and Yixin Bao and Yangrui Chen and Chuan Wu and Chuanxiong Guo},
   doi = {10.1145/3190508.3190517},
   isbn = {9781450355841},
   journal = {Proceedings of the 13th EuroSys Conference, EuroSys 2018},
   keywords = {Resource management; deep learning},
   title = {Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters},
   volume = {2018-Janua},
   year = {2018},
}
@article{Statistics2018,
   abstract = {Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applica- tions, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with incon- sistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMMjointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well bal- ances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimen- tal results on several public benchmark datasets show that, DAGMMsignificantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score},
   author = {Mathematical Statistics},
   issue = {2016},
   journal = {Iclr2018},
   pages = {1-13},
   title = {DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL FOR UNSUPERVISED ANOMALY DETECTION},
   year = {2018},
}
@article{Sun2020,
   abstract = {Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pretraining procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
   author = {Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Hao Tian and Hua Wu and Haifeng Wang},
   doi = {10.1609/aaai.v34i05.6428},
   isbn = {9781577358350},
   issn = {2159-5399},
   journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
   pages = {8968-8975},
   title = {ERNIE 2.0: A continual pre-training framework for language understanding},
   year = {2020},
}
@article{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming Wei Chang and Kenton Lee and Kristina Toutanova},
   isbn = {9781950737130},
   journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {4171-4186},
   title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
   volume = {1},
   year = {2019},
}
@article{Bondhugula2008,
   abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model - far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences. Copyright copy; 2008 ACM.},
   author = {Uday Bondhugula and Albert Hartono and J. Ramanujam and P. Sadayappan},
   doi = {10.1145/1379022.1375595},
   isbn = {9781595938602},
   issn = {15232867},
   issue = {6},
   journal = {ACM SIGPLAN Notices},
   keywords = {Affine transformations,Automatic parallelization,Locality optimization,Loop transformations,Polyhedral model,Tiling},
   pages = {101-113},
   title = {A practical automatic polyhedral parallelizer and locality optimizer},
   volume = {43},
   year = {2008},
}
@article{Naumov2019,
   abstract = {With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.},
   author = {Maxim Naumov and Dheevatsa Mudigere and Hao-Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
   issn = {2331-8422},
   title = {Deep Learning Recommendation Model for Personalization and Recommendation Systems},
   url = {http://arxiv.org/abs/1906.00091},
   year = {2019},
}
@article{Frankle2019,
   abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
   author = {Jonathan Frankle and Michael Carbin},
   journal = {7th International Conference on Learning Representations, ICLR 2019},
   pages = {1-43},
   title = {The lottery ticket hypothesis: Finding sparse, trainable neural networks},
   year = {2019},
}
@article{DeVito2021,
   abstract = {Python has become the de-facto language for training deep neural networks, coupling a large suite of scientific computing libraries with efficient libraries for tensor computation such as PyTorch or TensorFlow. However, when models are used for inference they are typically extracted from Python as TensorFlow graphs or TorchScript programs in order to meet performance and packaging constraints. The extraction process can be time consuming, impeding fast prototyping. We show how it is possible to meet these performance and packaging constraints while performing inference in Python. In particular, we present a way of using multiple Python interpreters within a single process to achieve scalable inference and describe a new container format for models that contains both native Python code and data. This approach simplifies the model deployment story by eliminating the model extraction step, and makes it easier to integrate existing performance-enhancing Python libraries. We evaluate our design on a suite of popular PyTorch models on Github, showing how they can be packaged in our inference format, and comparing their performance to TorchScript. For larger models, our packaged Python models perform the same as TorchScript, and for smaller models where there is some Python overhead, our multi-interpreter approach ensures inference is still scalable.},
   author = {Zachary DeVito and Jason Ansel and Will Constable and Michael Suo and Ailing Zhang and Kim Hazelwood},
   title = {Using Python for Model Inference in Deep Learning},
   url = {http://arxiv.org/abs/2104.00254},
   year = {2021},
}
@article{Paszke2019,
   abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
   author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas KÃ¶pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
   issn = {10495258},
   issue = {NeurIPS},
   journal = {Advances in Neural Information Processing Systems},
   title = {PyTorch: An imperative style, high-performance deep learning library},
   volume = {32},
   year = {2019},
}
@article{Pan2017,
   abstract = {Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.},
   author = {Xinghao Pan and Jianmin Chen and Rajat Monga and Samy Bengio and Rafal Jozefowicz},
   issn = {2331-8422},
   pages = {1-10},
   title = {Revisiting Distributed Synchronous SGD},
   url = {http://arxiv.org/abs/1702.05800},
   year = {2017},
}
@article{Calendar2005,
   author = {Mark Your Calendar},
   issue = {2},
   pages = {2010},
   title = {COMBINER: INDUCTIVELY LEARNING TREE STRUCTURED ATTENTION IN TRANSFORMERS},
   volume = {56},
   year = {2005},
}
@article{Cordonnier2019,
   abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
   author = {Jean-baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
   journal = {Iclr2020},
   pages = {1-13},
   title = {On the relation  between self attention and conv layer},
   year = {2019},
}
@article{Loshchilov2019,
   abstract = {L2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L2 regularization (often calling it âweight decayâ in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW.},
   author = {Ilya Loshchilov and Frank Hutter},
   journal = {7th International Conference on Learning Representations, ICLR 2019},
   title = {Decoupled weight decay regularization},
   year = {2019},
}
@article{Hazelwood2018,
   abstract = {Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.},
   author = {Kim Hazelwood and Sarah Bird and David Brooks and Soumith Chintala and Utku Diril and Dmytro Dzhulgakov and Mohamed Fawzy and Bill Jia and Yangqing Jia and Aditya Kalro and James Law and Kevin Lee and Jason Lu and Pieter Noordhuis and Misha Smelyanskiy and Liang Xiong and Xiaodong Wang},
   doi = {10.1109/HPCA.2018.00059},
   isbn = {9781538636596},
   issn = {15300897},
   journal = {Proceedings - International Symposium on High-Performance Computer Architecture},
   keywords = {Computer architecture,Facebook,Hardware software codesign,Machine learning},
   pages = {620-629},
   title = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
   volume = {2018-Febru},
   year = {2018},
}
@article{Peters2020,
   abstract = {Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of a-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any a > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.},
   author = {Ben Peters and Vlad Niculae and AndrÃ© F.T. Martins},
   doi = {10.18653/v1/p19-1146},
   isbn = {9781950737482},
   journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
   pages = {1504-1519},
   title = {Sparse sequence-to-sequence models},
   year = {2020},
}
@article{Pun2019,
   abstract = {Large-scale atomistic computer simulations of materials heavily rely on interatomic potentials predicting the energy and Newtonian forces on atoms. Traditional interatomic potentials are based on physical intuition but contain few adjustable parameters and are usually not accurate. The emerging machine-learning (ML) potentials achieve highly accurate interpolation within a large DFT database but, being purely mathematical constructions, suffer from poor transferability to unknown structures. We propose a new approach that can drastically improve the transferability of ML potentials by informing them of the physical nature of interatomic bonding. This is achieved by combining a rather general physics-based model (analytical bond-order potential) with a neural-network regression. This approach, called the physicallyÂ informed neural network (PINN) potential, is demonstrated by developing a general-purpose PINN potential for Al. We suggest that the development of physics-based ML potentials is the most effective way forward in the field of atomistic simulations.},
   author = {G. P.Purja Pun and R. Batra and R. Ramprasad and Y. Mishin},
   doi = {10.1038/s41467-019-10343-5},
   isbn = {4146701910},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   pages = {1-10},
   pmid = {31138813},
   publisher = {Springer US},
   title = {PhysicallyÂ informed artificial neural networks for atomistic modeling of materials},
   volume = {10},
   url = {http://dx.doi.org/10.1038/s41467-019-10343-5},
   year = {2019},
}
@article{Prabhakar2016,
   abstract = {In recent years the computing landscape has seen an increasing shift towards specialized accelerators. Field programmable gate arrays (FPGAs) are particularly promising for the implementation of these accelerators, as they offer significant performance and energy improvements over CPUs for a wide class of applications and are far more flexible than fixed-function ASICs. However, FPGAs are difficult to program. Traditional programming models for reconfigurable logic use low-level hardware description languages like Verilog and VHDL, which have none of the productivity features of modern software languages but produce very efficient designs, and low-level software languages like C and OpenCL coupled with high-level synthesis (HLS) tools that typically produce designs that are far less efficient. Functional languages with parallel patterns are a better fit for hardware generation because they provide high-level abstractions to programmers with little experience in hardware design and avoid many of the problems faced when generating hardware from imperative languages. In this paper, we identify two important optimizations for using parallel patterns to generate efficient hardware: tiling and metapipelining. We present a general representation of tiled parallel patterns, and provide rules for automatically tiling patterns and generating metapipelines. We demonstrate experimentally that these optimizations result in speedups up to 39.4Ã on a set of benchmarks from the data analytics domain.},
   author = {Raghu Prabhakar and David Koeplinger and Kevin J. Brown and Hyoukjoong Lee and Christopher De Sa and Christos Kozyrakis and Kunle Olukotun},
   doi = {10.1145/2872362.2872415},
   isbn = {9781450340915},
   issn = {15232867},
   issue = {4},
   journal = {ACM SIGPLAN Notices},
   keywords = {FPGAs,Hardware generation,Metapipelining,Parallel patterns,Reconfigurable hardware,Tiling},
   pages = {651-665},
   title = {Generating configurable hardware from parallel patterns},
   volume = {51},
   year = {2016},
}
@article{Koeplinger2016,
   abstract = {Acceleration in the form of customized datapaths offer large performance and energy improvements over general purpose processors. Reconfigurable fabrics such as FPGAs are gaining popularity for use in implementing application-specific accelerators, thereby increasing the importance of having good high-level FPGA design tools. However, current tools for targeting FPGAs offer inadequate support for high-level programming, resource estimation, and rapid and automatic design space exploration. We describe a design framework that addresses these challenges. We introduce a new representation of hardware using parameterized templates that captures locality and parallelism information at multiple levels of nesting. This representation is designed to be automatically generated from high-level languages based on parallel patterns. We describe a hybrid area estimation technique which uses template-level models and design-level artificial neural networks to account for effects from hardware place-and-route tools, including routing overheads, register and block RAM duplication, and LUT packing. Our runtime estimation accounts for off-chip memory accesses. We use our estimation capabilities to rapidly explore a large space of designs across tile sizes, parallelization factors, and optional coarse-grained pipelining, all at multiple loop levels. We show that estimates average 4.8% error for logic resources, 6.1% error for runtimes, and are 279 to 6533 times faster than a commercial high-level synthesis tool. We compare the best-performing designs to optimized CPU code running on a server-grade 6 core processor and show speedups of up to 16.7x.},
   author = {David Koeplinger and Raghu Prabhakar and Yaqi Zhang and Christina Delimitrou and Christos Kozyrakis and Kunle Olukotun},
   doi = {10.1109/ISCA.2016.20},
   isbn = {9781467389471},
   journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
   keywords = {FPGAs,application-specific accelerators,design space exploration,hardware definition language,hardware generation,parallel patterns,reconfigurable hardware},
   pages = {115-127},
   title = {Automatic Generation of Efficient Accelerators for Reconfigurable Hardware},
   year = {2016},
}
@article{Bachrach2012,
   abstract = {In this paper we introduce Chisel, a new hardware construction language that supports advanced hardware design using highly parameterized generators and layered domain-specific hardware languages. By embedding Chisel in the Scala programming language, we raise the level of hardware design abstraction by providing concepts including object orientation, functional programming, parameterized types, and type inference. Chisel can generate a high-speed C++-based cycle-accurate software simulator, or low-level Verilog designed to map to either FPGAs or to a standard ASIC flow for synthesis. This paper presents Chisel, its embedding in Scala, hardware examples, and results for C++ simulation, Verilog emulation and ASIC synthesis. Â© 2012 ACM.},
   author = {Jonathan Bachrach and Huy Vo and Brian Richards and Yunsup Lee and Andrew Waterman and Rimas AviÅ¾ienis and John Wawrzynek and Krste AsanoviÄ},
   doi = {10.1145/2228360.2228584},
   isbn = {9781450311991},
   issn = {0738100X},
   journal = {Proceedings - Design Automation Conference},
   keywords = {CAD},
   pages = {1216-1225},
   title = {Chisel: Constructing hardware in a Scala embedded language},
   year = {2012},
}
@article{Wang2014,
   abstract = {The signicant development of high-level synthesis tools has greatly facilitated FPGAs as general computing platforms. During the parallelism optimization for the data path, mem-ory becomes a crucial bottleneck that impedes performance enhancement. Simultaneous data access is highly restricted by the data mapping strategy and memory port constraint. Memory partitioning can eciently map data elements in the same logical array onto multiple physical banks so that the accesses to the array are parallelized. Previous meth-ods for memory partitioning mainly focused on cyclic parti-tioning for single-port memory. In this work we propose a generalized memory-partitioning framework to provide high data throughput of on-chip memories. We generalize cyclic partitioning into block-cyclic partitioning for a larger de-sign space exploration. We build the conict detection algo-rithm on polytope emptiness testing, and use integer points counting in polytopes for intra-bank oset generation. Mem-ory partitioning for multi-port memory is supported in this framework. Experimental results demonstrate that com-pared to the state-of-art partitioning algorithm, our pro-posed algorithm can reduce the number of block RAM by 19.58%, slice by 20.26% and DSP by 50%.},
   author = {Yuxin Wang and Peng Li and Jason Cong},
   doi = {10.1145/2554688.2554780},
   isbn = {9781450326711},
   journal = {ACM/SIGDA International Symposium on Field Programmable Gate Arrays - FPGA},
   keywords = {High-level synthesis,Memory partitioning,Polyhedral model},
   pages = {199-208},
   title = {Theory and algorithm for generalized memory partitioning in high-level synthesis},
   year = {2014},
}
@article{Koeplinger2018,
   abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult. In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
   author = {David Koeplinger and Matthew Feldman and Raghu Prabhakar and Yaqi Zhang and Stefan Hadjis and Ruben Fiszel and Tian Zhao and Luigi Nardi and Ardavan Pedram and Christos Kozyrakis and Kunle Olukotun},
   doi = {10.1145/3192366.3192379},
   isbn = {9781450356985},
   issn = {15232867},
   issue = {4},
   journal = {ACM SIGPLAN Notices},
   keywords = {CGRAs,FPGAs,compilers,domain-specific languages,hardware accelerators,high-level synthesis,reconfigurable architectures},
   pages = {296-311},
   title = {Spatial: A language and compiler for application accelerators},
   volume = {53},
   year = {2018},
}
@article{Hooker2021,
   abstract = {After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.},
   author = {Sara Hooker},
   doi = {10.1145/3467017},
   issn = {15577317},
   issue = {12},
   journal = {Communications of the ACM},
   pages = {58-65},
   title = {The hardware lottery},
   volume = {64},
   year = {2021},
}
@article{Stamoulis2020,
   abstract = {Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the latency constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4Â h. Our contributions are as follows: 1.Â Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2.Â Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96% top-1 accuracy on ImageNet with 79Â ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar inference latency constraints (<80Â ms). 3.Â NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is upÂ to 5,000Ã faster compared to prior work. 4.Â Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.},
   author = {Dimitrios Stamoulis and Ruizhou Ding and Di Wang and Dimitrios Lymberopoulos and Bodhi Priyantha and Jie Liu and Diana Marculescu},
   doi = {10.1007/978-3-030-46147-8_29},
   isbn = {9783030461461},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Hardware-aware ConvNets,Neural Architecture Search},
   pages = {481-497},
   title = {Single-Path NAS: Designing Hardware-Efficient ConvNets in Less Than 4 Hours},
   volume = {11907 LNAI},
   year = {2020},
}
@article{Prabhakar2018,
   abstract = {Plasticine is a new spatially reconfigurable architecture designed to efficiently execute applications composed of high-level parallel patterns. With an area footprint of 113 mm2 in a 28-nm process and a 1-GHz clock, Plasticine has a peak floating-point performance of 12.3 single-precision Tflops and a total on-chip memory capacity of 16 MB, consuming a maximum power of 49 W. Plasticine provides an improvement of up to 76.9X in performance-per-watt over a conventional FPGA over a wide range of dense and sparse applications.},
   author = {Raghu Prabhakar and Yaqi Zhang and David Koeplinger and Matt Feldman and Tian Zhao and Stefan Hadjis and Ardavan Pedram and Christos Kozyrakis and Kunle Olukotun},
   doi = {10.1109/MM.2018.032271058},
   isbn = {9781450348928},
   issn = {02721732},
   issue = {3},
   journal = {IEEE Micro},
   keywords = {FPGA,coarse-grained reconfigurable architectures,dataflow architectures,hardware,hardware accelerators,parallel patterns,reconfigurable architectures},
   pages = {20-31},
   title = {Plasticine: A Reconfigurable Accelerator for Parallel Patterns},
   volume = {38},
   year = {2018},
}
@article{Nandivada2006,
   abstract = {Commonly-used memory units enable a processor to load and store multiple registers in one instruction. We showed in 2003 how to extend gcc with a stack-location-allocation (SLA) phase that reduces memory traffic by rearranging the stack and replacing some load/store instructions with load/store-multiple instructions. While speeding up the target code, our technique leaves room for improvement because of the phase ordering of register allocation before SLA. In this paper we present SARA which combines SLA and register allocation into a single phase. SARA creates a synergy among register assignment, spill-code generation, and SLA that makes the combined phase generate faster code than a sequence of the individual phases. We specify SARA by an integer linear program generated from the program text. We have implemented SARA in gcc, replacing gcc's own implementation of register allocation. For our benchmarks, our results show that the target code is up to 16% faster than gcc with a separate SLA phase. Â© Springer-Verlag Berlin Heidelberg 2006.},
   author = {V. Krishna Nandivada and Jens Palsberg},
   doi = {10.1007/11688839_19},
   isbn = {354033050X},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {232-246},
   title = {SARA: Combining stack allocation and register allocation},
   volume = {3923 LNCS},
   year = {2006},
}
@article{Li2006,
   abstract = {We investigate the role of data complexity in the context of binary classification problems. The universal data complexity is defined for a data set as the Kolmogorov complexity of the mapping enforced by the data set. It is closely related to several existing principles used in machine learning such as Occam's razor, the minimum description length, and the Bayesian approach. The data complexity can also be defined based on a learning model, which is more realistic for applications. We demonstrate the application of the data complexity in two learning problems, data decomposition and data pruning. In data decomposition, we illustrate that a data set is best approximated by its principal subsets which are Pareto optimal with respect to the complexity and the set size. In data pruning, we show that outliers usually have high complexity contributions, and propose methods for estimating the complexity contribution. Since in practice we have to approximate the ideal data complexity measures, we also discuss the impact of such approximations.},
   author = {Ling Li and Yaser S Abu-Mostafa},
   issue = {May},
   journal = {Caltech Computer Science},
   pages = {1-32},
   title = {Data complexity in supervised learning},
   url = {http://resolver.caltech.edu/CaltechCSTR:2006.004%0Ahttps://resolver.caltech.edu/CaltechCSTR:2006.004},
   year = {2006},
}
@article{Yurtsever2020,
   abstract = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.},
   author = {Ekim Yurtsever and Jacob Lambert and Alexander Carballo and Kazuya Takeda},
   doi = {10.1109/ACCESS.2020.2983149},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Autonomous vehicles,automation,control,intelligent transportation systems,intelligent vehicles,robotics},
   pages = {58443-58469},
   title = {A Survey of Autonomous Driving: Common Practices and Emerging Technologies},
   volume = {8},
   year = {2020},
}
@article{Ott2018,
   abstract = {Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation.1 On WMTâ14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMTâ14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.},
   author = {Myle Ott and Sergey Edunov and David Grangier and Michael Auli},
   doi = {10.18653/v1/w18-6301},
   isbn = {9781948087810},
   journal = {WMT 2018 - 3rd Conference on Machine Translation, Proceedings of the Conference},
   pages = {1-9},
   title = {Scaling Neural Machine Translation},
   volume = {1},
   year = {2018},
}
@article{Bach2019,
   abstract = {Labeling training data is one of the most costly bottlenecks in developing machine learning-based applications. We present a first-of-its-kind study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude, and introduce Snorkel DryBell, a new weak supervision management system for this setting. Snorkel DryBell builds on the Snorkel framework, extending it in three critical aspects: flexible, template-based ingestion of diverse organizational knowledge, cross-feature production serving, and scalable, sampling-free execution. On three classification tasks at Google, we find that Snorkel DryBell creates classifiers of comparable quality to ones trained with tens of thousands of hand-labeled examples, converts non-servable organizational resources to servable models for an average 52% performance improvement, and executes over millions of data points in tens of minutes.},
   author = {Stephen H. Bach and Daniel Rodriguez and Yintao Liu and Chong Luo and Haidong Shao and Cassandra Xia and Souvik Sen and Alex Ratner and Braden Hancock and Houman Alborzi and Rahul Kuchhal and Chris RÃ© and Rob Malkin},
   doi = {10.1145/3299869.3314036},
   isbn = {9781450356435},
   issn = {07308078},
   journal = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
   keywords = {Systems for machine learning,Weak supervision},
   pages = {362-375},
   title = {Snorkel Drybell: A case study in deploying weak supervision at industrial scale},
   year = {2019},
}
@article{He2016,
   abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
   author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
   doi = {10.1109/CVPR.2016.90},
   isbn = {9781467388504},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {770-778},
   title = {Deep residual learning for image recognition},
   volume = {2016-Decem},
   year = {2016},
}
@article{Peters2018,
   abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
   author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
   doi = {10.18653/v1/n18-1202},
   isbn = {9781948087278},
   journal = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {2227-2237},
   title = {Deep contextualized word representations},
   volume = {1},
   year = {2018},
}
@article{Hamilton2017,
   abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
   author = {William L. Hamilton and Rex Ying and Jure Leskovec},
   issn = {10495258},
   issue = {Nips},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1025-1035},
   title = {Inductive representation learning on large graphs},
   volume = {2017-Decem},
   year = {2017},
}
@article{Dai2022,
   author = {Wei Dai and Daniel Berleant},
   doi = {10.1109/bigdata52589.2021.9671976},
   keywords = {benchmark metrics,corrupted images,imperfect images,robust deep learning},
   pages = {5085-5094},
   title = {Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation},
   year = {2022},
}
@article{Liu2016,
   abstract = {Neural Networks (NN) are a family of models for a broadrange of emerging machine learning and pattern reconditionapplications. NN techniques are conventionally executed ongeneral-purpose processors (such as CPU and GPGPU), which areusually not energy-efficient since they invest excessivehardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators forneural networks have been proposed recently to improve the energy-efficiency. However, such accelerators were designed for a small set of NN techniquessharing similar computational patterns, and they adopt complex and informativeinstructions (control signals) directly corresponding to high-levelfunctional blocks of an NN (such as layers), or even an NN as awhole. Although straightforward and easy-to-implement for a limitedset of similar NN techniques, the lack of agility in the instructionset prevents such accelerator designs from supporting a varietyof different NN techniques with sufficient flexibility and efficiency. In this paper, we propose a novel domain-specific Instruction Set Architecture (ISA) for NNaccelerators, called Cambricon, which is a load-store architecture thatintegrates scalar, vector, matrix, logical, data transfer, and controlinstructions, based on a comprehensive analysis of existing NNtechniques. Our evaluation over a total of ten representative yetdistinct NN techniques have demonstrated that Cambricon exhibits strongdescriptive capacity over a broad range of NN techniques, and provideshigher code density than general-purpose ISAs such as x86, MIPS, andGPGPU. Compared to the latest state-of-the-art NN accelerator design DaDianNao[5] (which can only accommodate 3 types of NN techniques), our Cambricon-based accelerator prototype implemented in TSMC 65nm technologyincurs only negligible latency/power/area overheads, with a versatile coverage of 10 different NN benchmarks.},
   author = {Shaoli Liu and Zidong Du and Jinhua Tao and Dong Han and Tao Luo and Yuan Xie and Yunji Chen and Tianshi Chen},
   doi = {10.1109/ISCA.2016.42},
   isbn = {9781467389471},
   journal = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
   pages = {393-405},
   title = {Cambricon: An Instruction Set Architecture for Neural Networks},
   year = {2016},
}
@article{,
   author = {Invited Paper and Jeff Jun Zhang and Nicolas Bohm Agostini and Shihao Song and Cheng Tan and Ankur Limaye and Vinay Amatya},
   title = {Towards Automatic and Agile AI / ML Accelerator Design with End-to-End Synthesis},
}
@article{Russakovsky2015,
   abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5Â years of the challenge, and propose future directions and improvements.},
   author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
   doi = {10.1007/s11263-015-0816-y},
   issn = {15731405},
   issue = {3},
   journal = {International Journal of Computer Vision},
   keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
   pages = {211-252},
   title = {ImageNet Large Scale Visual Recognition Challenge},
   volume = {115},
   year = {2015},
}
@article{Ramesh2021,
   abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
   author = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
   title = {Zero-Shot Text-to-Image Generation},
   volume = {200},
   url = {http://arxiv.org/abs/2102.12092},
   year = {2021},
}
@article{Minka2000,
   abstract = {This paper contains a large number of matrix identities which cannot be absorbed by mere reading. The reader is encouraged to take time and check each equation by hand and work out the examples. This is advanced material; see Searle (1982) for basic results. 1 Derivatives},
   author = {Thomas P Minka},
   isbn = {9783642165214},
   issn = {03067734},
   issue = {1988},
   journal = {See www stat cmu eduminkapapersmatrix html},
   pages = {1-19},
   pmid = {25246403},
   title = {Old and New Matrix Algebra Useful for Statistics},
   url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.5808},
   year = {2000},
}
@article{Bouthillier2019,
   abstract = {The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.},
   author = {Xavier Bouthillier and CÃ©sar Laurent and Pascal Vincent},
   isbn = {9781510886988},
   journal = {36th International Conference on Machine Learning, ICML 2019},
   pages = {1150-1159},
   title = {Unreproducible research is reproducible},
   volume = {2019-June},
   year = {2019},
}
@article{Pineau2021,
   abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: A code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
   author = {Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent LariviÃ©re and Alina Beygelzimer and Florence d'AlchÃ©-Buc and Emily Fox and Hugo Larochelle},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {NeurIPS 2019,Reproducibility},
   pages = {1-20},
   title = {Improving reproducibility in machine learning research (a report from the neurips 2019 reproducibility program)},
   volume = {22},
   year = {2021},
}
@article{Gebru2021,
   abstract = {Documentation to facilitate communication between dataset creators and consumers.},
   author = {Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal DaumÃ© Iii and Kate Crawford},
   doi = {10.1145/3458723},
   issn = {15577317},
   issue = {12},
   journal = {Communications of the ACM},
   pages = {86-92},
   title = {Datasheets for datasets},
   volume = {64},
   year = {2021},
}
@article{Williams2009,
   abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
   author = {Samuel Williams and Andrew Waterman and David Patterson},
   doi = {10.1145/1498765.1498785},
   issn = {00010782},
   issue = {4},
   journal = {Communications of the ACM},
   pages = {65-76},
   title = {Roofline: An insightful visual performance model for multicore architectures},
   volume = {52},
   year = {2009},
}
@article{Coleman2017,
   abstract = {Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce DAWNBench, a benchmark and competition focused on end-to-end training time to achieve a state-of-the-art accuracy level, as well as inference time with that accuracy. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-GPU training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe DAWNBench will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.},
   author = {Cody Coleman and Deepak Narayanan and Daniel Kang and Tian Zhao and Jian Zhang and Luigi Nardi and Peter Bailis and Kunle Olukotun and Chris RÃ© and Matei Zaharia and Stanford Dawn},
   issue = {Nips},
   journal = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
   title = {DAWNBench: An End-to-End Deep Learning Benchmark and Competition},
   year = {2017},
}
@article{Huyen2022,
   abstract = {A year ago, I wrote a post on how machine learning is going real-time. The post must have captured many data scientistsâ pain points because, after the post, many companies reached out to me sharing their pain points and discussing how to move their pipelines real time. These conversations prompted me to start a company on real-time machine learning. We have some exciting news I canât wait to share with you, but thatâs a story for another time :-) In the last year, Iâve talked to ~30 companies in different industries about their challenges with real-time machine learning. Iâve also worked with quite a few to find the solutions. This post outlines the solutions for (1) online prediction and (2) continual learning, with step-by-step use cases, considerations, and technologies required for each level.},
   author = {Chip Huyen},
   pages = {1-18},
   title = {Real-time machine learning : challenges and solutions},
   year = {2022},
}
@article{DARPA2016,
   abstract = {For pt.I see ibid., vol.31, no.1, p.161-9 (1993). Forest inventory\nmethods based on data acquired with an airborne ranging radar are\ndiscussed. The approach can be used to partly automate present\nlabor-dominated forest inventory methods. Using these methods, the mean\nand dominant tree height can be measured with a standard deviation of 1\nm. The stem volume per hectare can be estimated with a relative accuracy\nof 15% by effectively counting the height distribution of the trees and\nby calculating the center of backscattered power from the forest canopy\nprofile for a stand with a diameter of 40 m. The methods have been\ndeveloped using a helicopter-borne eight-channel ranging scatterometer,\nHUTSCAT, which can measure a radar forest canopy profile with a range\nresolution of 65 cm},
   author = {DARPA},
   isbn = {0196-2892},
   issn = {15580644},
   pages = {1-52},
   title = {Broad Agency Announcement Explainable Artificial Intelligence (XAI) DARPA-BAA-16-53},
   url = {https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf},
   year = {2016},
}
@article{Vilone2020,
   abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
   author = {Giulia Vilone and Luca Longo},
   issue = {Dl},
   keywords = {explainable artificial intelligence,method classification,survey,systematic},
   title = {Explainable Artificial Intelligence: a Systematic Review},
   url = {http://arxiv.org/abs/2006.00093},
   year = {2020},
}
@article{Tjoa2021,
   abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide 'obviously' interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
   author = {Erico Tjoa and Cuntai Guan},
   doi = {10.1109/TNNLS.2020.3027314},
   issn = {21622388},
   issue = {11},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Explainable artificial intelligence (XAI),interpretability,machine learning (ML),medical information system,survey},
   pages = {4793-4813},
   pmid = {33079674},
   title = {A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI},
   volume = {32},
   year = {2021},
}
@article{Suhan2021,
   abstract = {Domain-specific optimizing compilers have demonstrated significant performance and portability benefits, but require programs to be represented in their specialized IRs. Existing frontends to these compilers suffer from the "language subset problem" where some host language features are unsupported in the subset of the user's program that interacts with the domain-specific compiler. By contrast, define-by-run ML frameworks-colloquially called "eager" mode-are popular due to their ease of use and expressivity, where the full power of the host programming language can be used. LazyTensor is a technique to target domain specific compilers without sacrificing define-by-run ergonomics. Initially developed to support PyTorch on Cloud TPUs, the technique, along with a substantially shared implementation, has been used by Swift for TensorFlow across CPUs, GPUs, and TPUs, demonstrating the generality of the approach across (1) Tensor implementations, (2) hardware accelerators, and (3) programming languages.},
   author = {Alex Suhan and Davide Libenzi and Ailing Zhang and Parker Schuh and Brennan Saeta and Jie Young Sohn and Denys Shabalin},
   pages = {1-15},
   title = {LazyTensor: combining eager execution with domain-specific compilers},
   url = {http://arxiv.org/abs/2102.13267},
   year = {2021},
}
@article{Huyen2020,
   author = {Chip Huyen},
   pages = {1-15},
   title = {Machine learning is going real-time},
   year = {2020},
}
@article{Plaut1986,
   abstract = {Rumelhart, Hinton and Williams [Rumelhart et al. 86] describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in "weight space." We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the "analog" networks described by Hopfield and Tank [Hopfield and Tank 85]. The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search. 1},
   author = {DC Plaut and SJ Nowlan and GE Hinton},
   issue = {June},
   journal = {Technical Report CMU-CS-86-126},
   title = {Experiments on learning by back propagation},
   year = {1986},
}
@article{Tucker2019,
   abstract = {The differentiation between obstructive and central respiratory events is one of the most recurrent tasks in the diagnosis of sleep disordered breathing. Esophageal pressure measurement is the gold-standard method to assess respiratory effort and identify these events. But as its invasiveness discourages its use in clinical routine, non-invasisve systems have been proposed for differentiation. However, their adoption has been slow due to their limited clinical validation, as the creation of manual, gold-standard validation sets by human experts is a cumbersome procedure. In this study, a new system is proposed for an objective automatic, gold-standard differentiation between obstructive and central hypopneas with the esophageal pressure signal. First, an overall of 356 hypopneas of 16 patients were manually scored by a human expert to create a gold-standard validation set. Then, features were extracted from each hypopnea to train and test classifiers (Discriminant Analysis, Support Vector Machines and adaboost classifiers) to differentiate between central and obstructive hypopneas with the gold-standard esophageal pressure signal. The automatic differentiation system achieved promising results, with a sensitivity of 0.88, a specificity of 0.93 and an accuracy of 0.90. Hence, this system seems promising for an automatic, gold-standard differentiation between obstructive and central hypopneas.},
   author = {Warwick Tucker},
   doi = {10.2307/j.ctvcm4g18.8},
   journal = {Validated Numerics},
   pages = {60-72},
   title = {Automatic Differentiation},
   year = {2019},
}
@article{Rumelhart2019,
   abstract = {What makes people smarter than computers? These volumes by a pioneering neurocomputing group suggest that the answer lies in the massively parallel architecture of the human mind. They describe a new theory of cognition called connectionism that is challenging the idea of symbolic computation that has traditionally been at the center of debate in theoretical discussions about the mind. The authors' theory assumes the mind is composed of a great number of elementary units connected in a neural network. Mental processes are interactions between these units which excite and inhibit each other in parallel rather than sequential operations. In this context, knowledge can no longer be thought of as stored in localized structures; instead, it consists of the connections between pairs of units that are distributed throughout the network. Volume 1 lays the foundations of this exciting theory of parallel distributed processing, while Volume 2 applies it to a number of specific issues in cognitive science and neuroscience, with chapters describing models of aspects of perception, memory, language, and thought.},
   author = {David E. Rumelhart and James L. McClelland},
   doi = {10.7551/mitpress/5236.001.0001},
   journal = {Parallel Distributed Processing},
   title = {Learning Internal Representations},
   year = {2019},
}
@article{Matrix2016,
   author = {Overview Matrix and Intuition Motivation Learning and Derivation Finding and Assumptions Example and Limitations History See and Notes References Further},
   title = {Backpropagation},
   year = {2016},
}
@misc{,
   abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal âhidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
   title = {Learning Representations by Error Prop Nature},
}
@article{Maxfield1960,
   author = {Margaret W. Maxfield and E. Bodewig},
   doi = {10.2307/2309201},
   issn = {00029890},
   issue = {6},
   journal = {The American Mathematical Monthly},
   pages = {601},
   title = {Matrix Calculus.},
   volume = {67},
   year = {1960},
}
@article{,
   author = {Gregorio Ricci-curbastro and Tullio Levi-civita and Jan Arnoldus Schouten},
   title = {Ricci calculus},
   year = {1900},
}
@article{Einstein1916,
   author = {Albert Einstein and Introduction Statement and Application Vector},
   title = {Einstein notation},
   year = {1916},
}
@article{Vuduc2005,
   abstract = {The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines. Â© 2005 IOP Publishing Ltd.},
   author = {Richard Vuduc and James W. Demmel and Katherine A. Yelick},
   doi = {10.1088/1742-6596/16/1/071},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   pages = {521-530},
   title = {OSKI: A library of automatically tuned sparse matrix kernels},
   volume = {16},
   year = {2005},
}
@article{PETSc2019,
   author = {PETSc},
   title = {Portable, extensible toolkit for scientific computing},
   url = {https://icl.utk.edu/slate/},
   year = {2019},
}
@article{Vasilache2018,
   abstract = {Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]},
   author = {Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zachary DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen},
   title = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
   volume = {2},
   url = {http://arxiv.org/abs/1802.04730},
   year = {2018},
}
@article{,
   author = {Uday Bondhugula and Albert Hartono},
   isbn = {9781595938602},
   keywords = {affine transformations,automatic parallelization,hedral model,locality optimization,loop transformations,poly-,tiling},
   title = {Pluto: A Practical Automatic Polyhedral Parallelizer and Locality Optimizer},
}
@article{Blondel2020,
   abstract = {The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the O(n log n) time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with O(n log n) time and O(n) space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: Differentiable Spearman's rank correlation coefficient and least trimmed squares.},
   author = {Mathieu Blondel and Olivier Teboul and Quentin Berthet and Josip Djolonga},
   isbn = {9781713821120},
   journal = {37th International Conference on Machine Learning, ICML 2020},
   pages = {927-936},
   title = {Fast differentiable sorting and ranking},
   volume = {PartF16814},
   year = {2020},
}
@article{,
   author = {Nicolas Vasilache and Oleksandr Zinenko and Aart J C Bik and Mahesh Ravishankar and Thomas Raoux and Alexander Belyaev and Matthias Springer and Tobias Gysi and Stephan Herhut and Stella Laurenzo and Albert Cohen},
   issue = {1},
   publisher = {Association for Computing Machinery},
   title = {Composable and Modular Code Generation in MLIR},
   volume = {1},
}
@article{Bondhugula2020,
   abstract = {This article is primarily meant to present an early case study on using MLIR, a new compiler intermediate representation infrastructure, for high-performance code generation. Aspects of MLIR covered in particular include memrefs, the affine dialect, and polyhedral utilities and pass infrastructure surrounding those. This article is also aimed at showing the role compiler infrastructure could play in generating code that is competitive with highly tuned manually developed libraries, albeit in a more modular, reusable, and automatable way.},
   author = {Uday Bondhugula},
   pages = {1-23},
   title = {High Performance Code Generation in MLIR: An Early Case Study with GEMM},
   volume = {2},
   url = {http://arxiv.org/abs/2003.00532},
   year = {2020},
}
@article{Ganghoffer2018,
   abstract = {The concepts of tensor analysis arose from the work of Carl Friedrich Gauss in differential geometry, and the formulation was much influenced by the theory of algebraic forms and invariants developed during the middle of the nineteenth century. The word tensor itself was introduced in 1846 by William Rowan Hamilton to describe something different from what is now meant by a tensor (Note 1) The contemporary usage was brought in by Voigt in 1898. Tensor calculus was developed around 1890 by Gregorio Ricci-Curbastro under the title absolute differential calculus, and originally presented by Ricci in 1892. It was made accessible to many mathematicians by the publication of Ricci and Tullio Levi-Civita's 1900 classic text Ma thodes de calcul diffrentiel absolu et leurs applications (translated into Methods of absolute differential calculus and their applications).},
   author = {Jean FranÃ§ois Ganghoffer},
   doi = {10.1016/B978-1-78548-208-3.50001-0},
   isbn = {9780081021156},
   journal = {Multiscale Biomechanics},
   keywords = {Covariant and contravariant tensors,Curvilinear coordinates,Eigenvalues,Eigenvectors,Euclidean tensors,Frechet and Gateaux derivatives,Orthogonal tensors,Riesz representation theorem,Tensor algebra},
   pages = {3-73},
   title = {Tensor Calculus},
   year = {2018},
}
@article{Barham2022,
   abstract = {We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (~100% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.},
   author = {Paul Barham and Aakanksha Chowdhery and Jeff Dean and Sanjay Ghemawat and Steven Hand and Dan Hurt and Michael Isard and Hyeontaek Lim and Ruoming Pang and Sudip Roy and Brennan Saeta and Parker Schuh and Ryan Sepassi and Laurent El Shafey and Chandramohan A. Thekkath and Yonghui Wu},
   title = {Pathways: Asynchronous Distributed Dataflow for ML},
   url = {http://arxiv.org/abs/2203.12533},
   year = {2022},
}
@article{Jouppi2020,
   abstract = {Google's TPU supercomputers train deep neural networks 50x faster than general-purpose supercomputers running a high-performance computing benchmark.},
   author = {Norman P. Jouppi and Doe Hyun Yoon and George Kurian and Sheng Li and Nishant Patil and James Laudon and Cliff Young and David Patterson},
   doi = {10.1145/3360307},
   issn = {15577317},
   issue = {7},
   journal = {Communications of the ACM},
   pages = {67-78},
   title = {A domain-specific supercomputer for training deep neural networks},
   volume = {63},
   year = {2020},
}
@article{Chen2021,
   abstract = {Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.},
   author = {Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re},
   pages = {1-46},
   title = {Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
   url = {http://arxiv.org/abs/2112.00029},
   year = {2021},
}
@article{Ribeiro2016,
   abstract = {Despite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection, and sentiment analysis, with classifiers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classifier from a pair, and perform basic feature engineering to improve the classifiers.},
   author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
   doi = {10.18653/v1/n16-3020},
   isbn = {9781450342322},
   journal = {NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Demonstrations Session},
   pages = {97-101},
   title = {"Why Should I Trust You?" Explaining the Predictions of Any Classifier},
   year = {2016},
}
@article{Lundberg2017,
   abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
   author = {Scott M. Lundberg and Su In Lee},
   issn = {10495258},
   issue = {Section 2},
   journal = {Advances in Neural Information Processing Systems},
   pages = {4766-4775},
   title = {A unified approach to interpreting model predictions},
   volume = {2017-Decem},
   year = {2017},
}
@article{Shrikumar2017,
   abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLlFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLlFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLlFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLlFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
   author = {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
   isbn = {9781510855144},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   pages = {4844-4866},
   title = {Learning important features through propagating activation differences},
   volume = {7},
   year = {2017},
}
@article{Huang2020,
   abstract = {Hybrid Transactional and Analytical Processing (HTAP) databases require processing transactional and analytical queries in isolation to remove the interference between them. To achieve this, it is necessary to maintain different replicas of data specified for the two types of queries. However, it is challenging to provide a consistent view for distributed replicas within a storage system, where analytical requests can efficiently read consistent and fresh data from transactional workloads at scale and with high availability. To meet this challenge, we propose extending replicated state machine-based consensus algorithms to provide consistent replicas for HTAP workloads. Based on this novel idea, we present a Raft-based HTAP database: TiDB. In the database, we design a multi-Raft storage system which consists of a row store and a column store. The row store is built based on the Raft algorithm. It is scal-able to materialize updates from transactional requests with high availability. In particular, it asynchronously replicates Raft logs to learners which transform row format to column format for tuples, forming a real-time updatable column store. This column store allows analytical queries to efficiently read fresh and consistent data with strong isolation from transactions on the row store. Based on this storage system, we build an SQL engine to process large-scale distributed transactions and expensive analytical queries. The SQL engine optimally accesses row-format and column-format replicas of data. We also include a powerful analysis engine, TiSpark, to help TiDB connect to the Hadoop ecosystem. Comprehensive experiments show that TiDB achieves isolated high performance under CH-benCHmark, a benchmark focusing on HTAP workloads.},
   author = {Dongxu Huang and Qi Liu and Qiu Cui and Zhuhe Fang and Xiaoyu Ma and Fei Xu and Li Shen and Liu Tang and Yuxing Zhou and Menglong Huang and Wan Wei and Cong Liu and Jian Zhang and Jianjun Li and Xuelian Wu and Lingyu Song and Ruoxi Sun and Shuaipeng Yu and Lei Zhao and Nicholas Cameron and Liquan Pei and Xin Tang},
   doi = {10.14778/3415478.3415535},
   issn = {2150-8097},
   issue = {12},
   journal = {Proceedings of the VLDB Endowment},
   pages = {3072-3084},
   title = {TiDB},
   volume = {13},
   year = {2020},
}
@article{Sundararajan2017,
   author = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
   title = {Axiomatic Attribution for Deep Networks},
   year = {2017},
}
@article{Burns2016,
   abstract = {Though Widespread Interest in software containers is a relatively recent phenomenon, at Google we have been managing Linux containers at scale for more than 10 years and built three different containermanagement systems in that time. Each system is heavily influenced by its predecessors, even though they were developed for different reasons. This article describes the lessons we've learned from developing and operating them. The first unified container-management system developed at Google was the system we internally call Borg.7 It was built to manage both long-running services and batch jobs, which had previously been handled by two separate systems: Babysitter and the Global Work Queue. The latter's architecture strongly influenced Borg, but was focused on batch jobs; both predated Linux control groups. Borg shares machines between these two types of applications as a way of increasing resource utilization and thereby reducing costs. Such sharing was possible because container support in the Linux kernel was becoming available (indeed, Google contributed much of the container code to the Linux kernel), which enabled better isolation between latency-sensitive user-facing services and CPU-hungry batch processes.},
   author = {Brendan Burns and Brian Grant and David Oppenheimer and Eric Brewer and John Wilkes},
   doi = {10.1145/2890784},
   issn = {15577317},
   issue = {5},
   journal = {Communications of the ACM},
   pages = {50-57},
   title = {Borg, omega, and kubernetes},
   volume = {59},
   year = {2016},
}
@article{Verma2015,
   abstract = {Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.},
   author = {Abhishek Verma and Luis Pedrosa and Madhukar Korupolu and David Oppenheimer and Eric Tune and John Wilkes},
   doi = {10.1145/2741948.2741964},
   isbn = {9781450332385},
   journal = {Proceedings of the 10th European Conference on Computer Systems, EuroSys 2015},
   title = {Large-scale cluster management at Google with Borg},
   year = {2015},
}
@article{Bian2021,
   abstract = {The Transformer architecture has improved the performance of deep learning models in domains such as Computer Vision and Natural Language Processing. Together with better performance come larger model sizes. This imposes challenges to the memory wall of the current accelerator hardware such as GPU. It is never ideal to train large models such as Vision Transformer, BERT, and GPT on a single GPU or a single machine. There is an urgent demand to train models in a distributed environment. However, distributed training, especially model parallelism, often requires domain expertise in computer systems and architecture. It remains a challenge for AI researchers to implement complex distributed training solutions for their models. In this paper, we introduce Colossal-AI, which is a unified parallel training system designed to seamlessly integrate different paradigms of parallelization techniques including data parallelism, pipeline parallelism, multiple tensor parallelism, and sequence parallelism. Colossal-AI aims to support the AI community to write distributed models in the same way as how they write models normally. This allows them to focus on developing the model architecture and separates the concerns of distributed training from the development process. The documentations can be found at https://www.colossalai.org and the source code can be found at https://github.com/hpcaitech/ColossalAI.},
   author = {Zhengda Bian and Hongxin Liu and Boxiang Wang and Haichen Huang and Yongbin Li and Chuanrui Wang and Fan Cui and Yang You},
   title = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
   url = {http://arxiv.org/abs/2110.14883},
   year = {2021},
}
@article{Bogner2021,
   abstract = {Background: With the rising popularity of Artificial Intelligence (AI), there is a growing need to build large and complex AI-based systems in a cost-effective and manageable way. Like with traditional software, Technical Debt (TD) will emerge naturally over time in these systems, therefore leading to challenges and risks if not managed appropriately. The influence of data science and the stochastic nature of AI-based systems may also lead to new types of TD or antipatterns, which are not yet fully understood by researchers and practitioners. Objective: The goal of our study is to provide a clear overview and characterization of the types of TD (both established and new ones) that appear in AI-based systems, as well as the antipatterns and related solutions that have been proposed. Method: Following the process of a systematic mapping study, 21 primary studies are identified and analyzed. Results: Our results show that (i) established TD types, variations of them, and four new TD types (data, model, configuration, and ethics debt) are present in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the majority related to data and model deficiencies, and (iii) 46 solutions have been proposed, either to address specific TD types, antipatterns, or TD in general. Conclusions: Our results can support AI professionals with reasoning about and communicating aspects of TD present in their systems. Additionally, they can serve as a foundation for future research to further our understanding of TD in AI-based systems.},
   author = {Justus Bogner and Roberto Verdecchia and Ilias Gerostathopoulos},
   doi = {10.1109/TechDebt52882.2021.00016},
   isbn = {9781665414050},
   journal = {Proceedings - 2021 IEEE/ACM International Conference on Technical Debt, TechDebt 2021},
   keywords = {Antipatterns,Artificial Intelligence,Machine Learning,Systematic Mapping Study,Technical Debt},
   pages = {64-73},
   title = {Characterizing Technical Debt and Antipatterns in AI-Based Systems: A Systematic Mapping Study},
   year = {2021},
}
@article{Schelter2018,
   abstract = {The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models-in short model management-is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and can result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this field is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-processing related challenges arising in the management of the corresponding ML models, and point out future research directions.},
   author = {Sebastian Schelter and Felix Biessmann and Tim Januschowski and David Salinas and Stephan Seufert and Gyuri Szarvas},
   journal = {Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
   pages = {5-13},
   title = {On Challenges in Machine Learning Model Management},
   url = {http://sites.computer.org/debull/A18dec/p5.pdf},
   year = {2018},
}
@article{Sculley2014,
   abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
   author = {D Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
   isbn = {9780874216561},
   issn = {13613723},
   journal = {NIPS 2014 Workshop on Software Engineering for Machine Learning (SE4ML)},
   pages = {1-9},
   pmid = {1000106311},
   title = {Machine Learning : The High-Interest Credit Card of Technical Debt},
   year = {2014},
}
@article{Ye2019,
   abstract = {The code responsible for serializing and deserializing untrusted external data is a vital component of any software that communicates with the outside world, as any bugs in these components can compromise the entire system. This is particularly true for verified systems which rely on trusted code to process external data, as any defects in the parsing code can invalidate any formal proofs about the system. One way to reduce the trusted code base of these systems is to use interface generators like Protocol Buffer and ASN.1 to generate serializers and deserializers from data descriptors. Of course, these generators are not immune to bugs. In this work, we formally verify a compiler for a realistic subset of the popular Protocol Buffer serialization format using the Coq proof assistant, proving once and for all the correctness of every generated serializer and deserializer. One of the challenges we had to overcome was the extreme flexibility of the Protocol Buffer format: The same source data can be encoded in an infinite number of ways, and the deserializer must faithfully recover the original source value from each. We have validated our verified system using the official conformance tests.},
   author = {Qianchuan Ye and Benjamin Delaware},
   doi = {10.1145/3293880.3294105},
   isbn = {9781450362221},
   journal = {CPP 2019 - Proceedings of the 8th ACM SIGPLAN International Conference on Certified Programs and Proofs, Co-located with POPL 2019},
   keywords = {Coq,Program verification,Serialization},
   pages = {222-233},
   title = {A verified protocol buffer compiler},
   year = {2019},
}
@article{Domingos2012,
   abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. 15 Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell 16 and Witten et al. 24). However, much of the "folk knowledge" that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article. Â© 2012 ACM.},
   author = {Pedro Domingos},
   doi = {10.1145/2347736.2347755},
   issn = {00010782},
   issue = {10},
   journal = {Communications of the ACM},
   pages = {78-87},
   title = {A few useful things to know about machine learning},
   volume = {55},
   year = {2012},
}
@article{,
   abstract = {Fast and reliable detection of patients with severe and heterogeneous illnesses is a major goal of precision medicine1,2. Patients with leukaemia can be identified using machine learning on the basis of their blood transcriptomes3. However, there is an increasing divide between what is technically possible and what is allowed, because of privacy legislation4,5. Here, to facilitate the integration of any medical data from any data owner worldwide without violating privacy laws, we introduce Swarm Learningâa decentralized machine-learning approach that unites edge computing, blockchain-based peer-to-peer networking and coordination while maintaining confidentiality without the need for a central coordinator, thereby going beyond federated learning. To illustrate the feasibility of using Swarm Learning to develop disease classifiers using distributed data, we chose four use cases of heterogeneous diseases (COVID-19, tuberculosis, leukaemia and lung pathologies). With more than 16,400 blood transcriptomes derived from 127 clinical studies with non-uniform distributions of cases and controls and substantial study biases, as well as more than 95,000 chest X-ray images, we show that Swarm Learning classifiers outperform those developed at individual sites. In addition, Swarm Learning completely fulfils local confidentiality regulations by design. We believe that this approach will notably accelerate the introduction of precision medicine.},
   author = {Stefanie Warnat-Herresthal and Hartmut Schultze and Krishnaprasad Lingadahalli Shastry and Sathyanarayanan Manamohan and Saikat Mukherjee and Vishesh Garg and Ravi Sarveswara and Kristian HÃ¤ndler and Peter Pickkers and N. Ahmad Aziz and Sofia Ktena and Florian Tran and Michael Bitzer and Stephan Ossowski and Nicolas Casadei and Christian Herr and Daniel Petersheim and Uta Behrends and Fabian Kern and Tobias Fehlmann and Philipp Schommers and Clara Lehmann and Max Augustin and Jan Rybniker and Janine AltmÃ¼ller and Neha Mishra and Joana P. Bernardes and Benjamin KrÃ¤mer and Lorenzo Bonaguro and Jonas Schulte-Schrepping and Elena De Domenico and Christian Siever and Michael Kraut and Milind Desai and Bruno Monnet and Maria Saridaki and Charles Martin Siegel and Anna Drews and Melanie Nuesch-Germano and Heidi Theis and Jan Heyckendorf and Stefan Schreiber and Sarah Kim-Hellmuth and Paul Balfanz and Thomas Eggermann and Peter Boor and Ralf Hausmann and Hannah Kuhn and Susanne Isfort and Julia Carolin Stingl and GÃ¼nther Schmalzing and Christiane K. Kuhl and Rainer RÃ¶hrig and Gernot Marx and Stefan Uhlig and Edgar Dahl and Dirk MÃ¼ller-Wieland and Michael Dreher and Nikolaus Marx and Jacob Nattermann and Dirk Skowasch and Ingo Kurth and Andreas Keller and Robert Bals and Peter NÃ¼rnberg and Olaf RieÃ and Philip Rosenstiel and Mihai G. Netea and Fabian Theis and Sach Mukherjee and Michael Backes and Anna C. Aschenbrenner and Thomas Ulas and Angel Angelov and Alexander BartholomÃ¤us and Anke Becker and Daniela Bezdan and Conny Blumert and Ezio Bonifacio and Peer Bork and Bunk Boyke and Helmut Blum and Thomas Clavel and Maria Colome-Tatche and Markus Cornberg and Inti Alberto De La Rosa VelÃ¡zquez and Andreas Diefenbach and Alexander Dilthey and Nicole Fischer and Konrad FÃ¶rstner and SÃ¶ren Franzenburg and Julia Stefanie Frick and Gisela Gabernet and Julien Gagneur and Tina Ganzenmueller and Marie Gauder and Janina GeiÃert and Alexander Goesmann and Siri GÃ¶pel and Adam Grundhoff and Hajo Grundmann and Torsten Hain and Frank Hanses and Ute Hehr and AndrÃ© Heimbach and Marius Hoeper and Friedemann Horn and Daniel HÃ¼bschmann and Michael Hummel and Thomas Iftner and Angelika Iftner and Thomas Illig and Stefan Janssen and JÃ¶rn Kalinowski and RenÃ© Kallies and Birte Kehr and Oliver T. Keppler and Christoph Klein and Michael Knop and Oliver Kohlbacher and Karl KÃ¶hrer and Jan Korbel and Peter G. Kremsner and Denise KÃ¼hnert and Markus Landthaler and Yang Li and Kerstin U. Ludwig and Oliwia Makarewicz and Manja Marz and Alice C. McHardy and Christian Mertes and Maximilian MÃ¼nchhoff and Sven Nahnsen and Markus NÃ¶then and Francine Ntoumi and JÃ¶rg Overmann and Silke Peter and Klaus Pfeffer and Isabell Pink and Anna R. Poetsch and Ulrike Protzer and Alfred PÃ¼hler and Nikolaus Rajewsky and Markus Ralser and Kristin Reiche and Stephan Ripke and Ulisses Nunes da Rocha and Antoine Emmanuel Saliba and Leif Erik Sander and Birgit Sawitzki and Simone Scheithauer and Philipp Schiffer and Jonathan Schmid-Burgk and Wulf Schneider and Eva Christina Schulte and Alexander Sczyrba and Mariam L. Sharaf and Yogesh Singh and Michael Sonnabend and Oliver Stegle and Jens Stoye and Janne Vehreschild and Thirumalaisamy P. Velavan and JÃ¶rg Vogel and Sonja Volland and Max von Kleist and Andreas Walker and JÃ¶rn Walter and Dagmar Wieczorek and Sylke Winkler and John Ziebuhr and Monique M.B. Breteler and Evangelos J. Giamarellos-Bourboulis and Matthijs Kox and Matthias Becker and Sorin Cheran and Michael S. Woodacre and Eng Lim Goh and Joachim L. Schultze},
   doi = {10.1038/s41586-021-03583-3},
   issn = {14764687},
   issue = {7862},
   journal = {Nature},
   pages = {265-270},
   pmid = {34040261},
   title = {Swarm Learning for decentralized and confidential clinical machine learning},
   volume = {594},
   year = {2021},
}
@article{Dean2022,
   author = {Jeffrey Dean},
   pages = {58-74},
   title = {A Golden Decade of Deep Learning: Computing Systems & Applications},
   year = {2022},
}
@article{,
   abstract = {Google's MapReduce programming model serves for processing large data sets in a massively parallel manner. We deliver the first rigorous description of the model including its advancement as Google's domain-specific language Sawzall. To this end, we reverse-engineer the seminal papers on MapReduce and Sawzall, and we capture our findings as an executable specification. We also identify and resolve some obscurities in the informal presentation given in the seminal papers. We use typed functional programming (specifically Haskell) as a tool for design recovery and executable specification. Our development comprises three components: (i) the basic program skeleton that underlies MapReduce computations; (ii) the opportunities for parallelism in executing MapReduce computations; (iii) the fundamental characteristics of Sawzall's aggregators as an advancement of the MapReduce approach. Our development does not formalize the more implementational aspects of an actual, distributed execution of MapReduce computations. Â© 2007 Elsevier B.V. All rights reserved.},
   author = {Ralf LÃ¤mmel},
   doi = {10.1016/j.scico.2007.07.001},
   issn = {01676423},
   issue = {3},
   journal = {Science of Computer Programming},
   keywords = {Data processing,Distributed programming,Executable specification,Haskell,List homomorphism,Map,MapReduce,Parallel programming,Reduce,Sawzall,Software design,Typed functional programming},
   pages = {208-237},
   title = {Google's MapReduce programming model - Revisited},
   volume = {68},
   year = {2007},
}
@book{Murray2021,
   abstract = {Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators that can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions enable users to focus on the application logic of data processing, while tf.dataâs runtime ensures that pipelines run efficiently. We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and optional non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Googleâs datacenter fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.},
   author = {Derek G. Murray and JiÅÃ­ Å imÅ¡a and Ana Klimovic and Ihor Indyk},
   doi = {10.14778/3476311.3476374},
   issn = {21508097},
   issue = {12},
   journal = {Proceedings of the VLDB Endowment},
   pages = {2945-2958},
   publisher = {Association for Computing Machinery},
   title = {Tf.Data: A machine learning data processing framework},
   volume = {14},
   year = {2021},
}
@article{Wang2020,
   abstract = {Hybrid Storage servers combining high-speed SSDs and high-capacity HDDs are designed for high cost-effectiveness and provide Î¼s-level responsiveness for applications. Observations from the production hybrid cloud storage system Pangu suggest that HDDs are often severely underutilized while SSDs are overused, especially for writes that dominate the hybrid storage. This lopsided utilization between HDDs and SSDs leads to not only fast wear-out in the latter but also very high tail latency due to frequent garbage collections induced by intensive writes to the latter. On the other hand, our extensive experimental study reveals that a series of sequential and continuous writes to HDDs exhibit a periodic, staircase shaped pattern of write latency, i.e., low (e.g., 35Î¼s), middle (e.g., 55Î¼s), and high latency (e.g., 12ms), resulting from buffered writes in HDD's controller. This suggests that HDDs can potentially provide Î¼s-level write IO delay (for appropriately scheduled writes), which is close to SSDs' write performance. These observations inspire us to effectively exploit this performance potential of HDDs to absorb as many writes as possible to avoid SSD overuse without performance degradation. To achieve this goal, we first characterize performance behaviors of hybrid storage in general and its HDDs in particular. Based on the findings on sequential and continuous writes, we propose a prediction model to accurately determine next write latency state (i.e., fast, middle and slow). With this model, a Buffer-Controlled Write approach, BCW, is proposed to proactively and effectively control buffered writes so that low- and mid-latency periods in HDDs are scheduled with application write data and high-latency periods are filled with padded data. Based on BCW, we design a mixed IO scheduler (MIOS) to adaptively steer incoming data to SSDs and HDDs according to write patterns, runtime queue lengths, and disk status. We perform extensive evaluations under production workloads and benchmarks. The results show that MIOS removes up to 93% amount of data written to SSDs, reduces average and 99th-percentile latencies of the hybrid server by 65% and 85% respectively.},
   author = {Shucheng Wang and Ziyi Lu and Qiang Cao and Hong Jiang and Jie Yao and Yuanyuan Dong and Puyuan Yang},
   isbn = {9781939133120},
   journal = {Proceedings of the 18th USENIX Conference on File and Storage Technologies, FAST 2020},
   pages = {253-266},
   title = {Quiver: An Informed Storage Cache for Deep Learning},
   year = {2020},
}
@article{Mohan2021,
   abstract = {Training Deep Neural Networks (DNNs) is a resource-hungry and time-consuming task. During training, the model performs computation at the GPU to learn weights, repeatedly, over several epochs. The learned weights reside in GPU memory, and are occasionally checkpointed (written to persistent storage) for fault-tolerance. Traditionally, model parameters are checkpointed at epoch boundaries; for modern deep networks, an epoch runs for several hours. An interruption to the training job due to preemption, node failure, or process failure, therefore results in the loss of several hours worth of GPU work on recovery. We present CheckFreq, an automatic, fine-grained checkpointing framework that (1) algorithmically determines the checkpointing frequency at the granularity of iterations using systematic online profiling, (2) dynamically tunes checkpointing frequency at runtime to bound the checkpointing overhead using adaptive rate tuning, (3) maintains the training data invariant of using each item in the dataset exactly once per epoch by checkpointing data loader state using a light-weight resumable iterator, and (4) carefully pipelines checkpointing with computation to reduce the checkpoint cost by introducing two-phase checkpointing. Our experiments on a variety of models, storage backends, and GPU generations show that CheckFreq can reduce the recovery time from hours to seconds while bounding the runtime overhead within 3.5%.},
   author = {Jayashree Mohan and Amar Phanishayee and Vijay Chidambaram},
   isbn = {9781939133205},
   journal = {Proceedings of the 19th USENIX Conference on File and Storage Technologies, FAST 2021},
   pages = {203-216},
   title = {CheckFreq: Frequent, fine-grained DNN checkpointing},
   year = {2021},
}
@article{Pinto2018,
   abstract = {Deep Learning system architects strive to design a balanced system where the computational accelerator -- FPGA, GPU, etc, is not starved for data. Feeding training data fast enough to effectively keep the accelerator utilization high is difficult when utilizing dedicated hardware like GPUs. As accelerators are getting faster, the storage media \& data buses feeding the data have not kept pace and the ever increasing size of training data further compounds the problem. We describe the design and implementation of a distributed caching system called Hoard that stripes the data across fast local disks of multiple GPU nodes using a distributed file system that efficiently feeds the data to ensure minimal degradation in GPU utilization due to I/O starvation. Hoard can cache the data from a central storage system before the start of the job or during the initial execution of the job and feeds the cached data for subsequent epochs of the same job and for different invocations of the jobs that share the same data requirements, e.g. hyper-parameter tuning. Hoard exposes a POSIX file system interface so the existing deep learning frameworks can take advantage of the cache without any modifications. We show that Hoard, using two NVMe disks per node and a distributed file system for caching, achieves a 2.1x speed-up over a 10Gb/s NFS central storage system on a 16 GPU (4 nodes, 4 GPUs per node) cluster for a challenging AlexNet ImageNet image classification benchmark with 150GB of input dataset. As a result of the caching, Hoard eliminates the I/O bottlenecks introduced by the shared storage and increases the utilization of the system by 2x compared to using the shared storage without the cache.},
   author = {Christian Pinto and Yiannis Gkoufas and Andrea Reale and Seetharami Seelam and Steven Eliuk},
   title = {Hoard: A Distributed Data Caching System to Accelerate Deep Learning Training on the Cloud},
   url = {http://arxiv.org/abs/1812.00669},
   year = {2018},
}
@article{Nakandala2020,
   abstract = {Deep neural networks (deep nets) are revolutionizing many machine learning (ML) applications. But there is a major bottleneck to wider adoption: the pain and resource intensiveness of model selection. This empirical process involves exploring deep net architectures and hyper-parameters, often requiring hundreds of trials. Alas, most ML systems focus on training one model at a time, reducing throughput and raising overall resource costs; some also sacrifice reproducibility. We present Cerebro, a new data system to raise deep net model selection throughput at scale without raising resource costs and without sacrificing reproducibility or accuracy. Cerebro uses a new parallel SGD execution strategy we call model hopper parallelism that hybridizes task-and data-parallelism to mitigate the cons of these prior paradigms and offer the best of both worlds. Experiments on large ML benchmark datasets show that Cerebro offers 3x to 10x runtime savings relative to data-parallel systems like Horovod and Parameter Server and up to 8x memory/ storage savings or up to 100x network savings relative to task-parallel systems. Cerebro also supports heterogeneous resources and fault tolerance.},
   author = {Supun Nakandala and Yuhao Zhang and Arun Kumar},
   doi = {10.14778/3407790.3407816},
   issn = {21508097},
   issue = {11},
   journal = {Proceedings of the VLDB Endowment},
   pages = {2159-2173},
   title = {Cerebro: A data system for optimized deep learning model selection},
   volume = {13},
   year = {2020},
}
@article{Armbrust2021,
   author = {Michael Armbrust and Ali Ghodsi and Reynold Xin and Matei Zaharia},
   title = {Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics},
   year = {2021},
}
@article{Kakaraparthy2019,
   abstract = {Training machine learning models involves iteratively fetching and pre-processing batches of data. Conventionally, popular ML frameworks implement data loading within a job and focus on improving the performance of a single job. However, such an approach is inefficient in shared clusters where multiple training jobs are likely to be accessing the same data and duplicating operations. To illustrate this, we present a case study which reveals that for hyper-parameter tuning experiments, we can reduce up to 89% I/O and 97% pre-processing redundancy. Based on this observation, we make the case for unifying data loading in machine learning clusters by bringing the isolated data loading systems together into a single system. Such a system architecture can remove the aforementioned redundancies that arise due to the isolation of data loading in each job. We introduce OneAccess, a unified data access layer and present a prototype implementation that shows a 47.3% improvement in I/O cost when sharing data across jobs. Finally we discuss open research challenges in designing and developing a unified data loading layer that can run across frameworks on shared multi-tenant clusters, including how to handle distributed data access, support diverse sampling schemes, and exploit new storage media.},
   author = {Aarati Kakaraparthy and Abhay Venkatesh and Amar Phanishayee and Shivaram Venkataraman},
   journal = {11th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2019, co-located with USENIX ATC 2019},
   title = {The case for unifying data loading in machine learning clusters},
   year = {2019},
}
@article{Yang2020,
   abstract = {After nearly a decade of anticipation, scalable nonvolatile memory DIMMs are finally commercially available with the release of Intel's Optane DIMM. This new nonvolatile DIMM supports byte-granularity accesses with access times on the order of DRAM, while also providing data storage that survives power outages. Researchers have not idly waited for real nonvolatile DIMMs (NVDIMMs) to arrive. Over the past decade, they have written a slew of papers proposing new programming models, file systems, libraries, and applications built to exploit the performance and flexibility that NVDIMMs promised to deliver. Those papers drew conclusions and made design decisions without detailed knowledge of how real NVDIMMs would behave or how industry would integrate them into computer architectures. Now that Optane NVDIMMs are actually here, we can provide detailed performance numbers, concrete guidance for programmers on these systems, reevaluate prior art for performance, and reoptimize persistent memory software for the real Optane DIMM. In this paper, we explore the performance properties and characteristics of Intel's new Optane DIMM at the micro and macro level. First, we investigate the basic characteristics of the device, taking special note of the particular ways in which its performance is peculiar relative to traditional DRAM or other past methods used to emulate NVM. From these observations, we recommend a set of best practices to maximize the performance of the device. With our improved understanding, we then explore and reoptimize the performance of prior art in application-level software for persistent memory.},
   author = {Jian Yang and Juno Kim and Morteza Hoseinzadeh and Joseph Izraelevitz and Steven Swanson},
   isbn = {9781939133120},
   journal = {Proceedings of the 18th USENIX Conference on File and Storage Technologies, FAST 2020},
   pages = {169-182},
   title = {An empirical guide to the behavior and use of scalable persistent memory},
   year = {2020},
}
@article{Diethe2019,
   abstract = {This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.},
   author = {Tom Diethe and Tom Borchert and Eno Thereska and Borja Balle and Neil Lawrence},
   issue = {Nips},
   title = {Continual Learning in Practice},
   url = {http://arxiv.org/abs/1903.05202},
   year = {2019},
}
@article{Narayanan2018,
   abstract = {Deep neural networks (DNNs) with millions of parameters are increasingly being applied across a variety of domains. GPUs-the compute platform of choice for DNNs-have become progressively more powerful to keep pace with this growing computational demand. However, many multi-model workloads are unable to leverage the available computational capacity of modern GPUs. For example, model search applications use smaller models to automatically design model architectures for a given task, and low-latency model serving applications operate in small minibatch regimes. We show that the natural baseline of simply launching GPU operations from different models in parallel fails to provide substantial speedups due to data transfer, memory-bound kernels, and the overhead of kernel launches for short-duration kernels. We propose HiveMind, a system designed specifically to optimize multi-model deep learning workloads. HiveMind optimizes a "model batch" by performing cross-model operator fusion, and sharing I/O across models. HiveMind then uses a parallel runtime to efficiently execute this fused graph. Preliminary results show HiveMind can accelerate simple hyperparameter tuning and multi-model inference workloads by up to 10Ã on NVIDIA P100 and V100 GPUs compared to sequential model execution.},
   author = {Deepak Narayanan and Keshav Santhanam and Amar Phanishayee and Matei Zaharia},
   journal = {NeurIPS Workshop on Systems for Machine Learning},
   pages = {1-8},
   title = {Accelerating Deep Learning Workloads through Efficient Multi-Model Execution},
   year = {2018},
}
@article{Lhoest2021,
   abstract = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
   author = {Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and Abhishek Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario Å aÅ¡ko and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and ClÃ©ment Delangue and ThÃ©o MatussiÃ¨re and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and FranÃ§ois Lagunas and Alexander Rush and Thomas Wolf},
   doi = {10.18653/v1/2021.emnlp-demo.21},
   isbn = {9781955917117},
   pages = {175-184},
   title = {Datasets: A Community Library for Natural Language Processing},
   year = {2021},
}
@article{Breck2019,
   abstract = {Machine learning is a powerful tool for gleaning knowledge from massive amounts of data. While a great deal of machine learning research has focused on improving the accuracy and efficiency of training and inference algorithms, there is less attention in the equally important problem of monitoring the quality of data fed to machine learning. The importance of this problem is hard to dispute: errors in the input data can nullify any benefits on speed and accuracy for training and inference. This argument points to a data-centric approach to machine learning that treats training and serving data as an important production asset, on par with the algorithm and infrastructure used for learning. In this paper, we tackle this problem and present a data validation system that is designed to detect anomalies specifically in data fed into machine learning pipelines. This system is deployed in production as an integral part of TFX(Baylor et al., 2017)-an end-to-end machine learning platform at Google. It is used by hundreds of product teams use it to continuously monitor and validate several petabytes of production data per day. We faced several challenges in developing our system, most notably around the ability of ML pipelines to soldier on in the face of unexpected patterns, schema-free data, or training/serving skew. We discuss these challenges, the techniques we used to address them, and the various design choices that we made in implementing the system. Finally, we present evidence from the system's deployment in production that illustrate the tangible benefits of data validation in the context of ML: early detection of errors, model-quality wins from using better data, savings in engineering hours to debug problems, and a shift towards data-centric workflows in model development.},
   author = {Eric Breck and Neoklis Polyzotis and Sudip Roy and Steven Euijong Whang and Martin Zinkevich},
   journal = {SysML},
   pages = {1--14},
   title = {DATA VALIDATION FOR MACHINE LEARNING},
   year = {2019},
}
@article{Choi2019,
   abstract = {In the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce "data echoing," which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or "echoes") intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.},
   author = {Dami Choi and Alexandre Passos and Christopher J. Shallue and George E. Dahl},
   title = {Faster Neural Network Training with Data Echoing},
   year = {2019},
}
@article{Zhao2021,
   abstract = {Domain-specific accelerators (DSAs) are integrated in datacenter-scale clusters across industry to train increasingly-complex deep learning models over massive datasets. As innovations in DSAs continue to increase training efficiency and throughput, the data storage and ingestion (DSI) pipeline, the systems and hardware responsible for storing and preprocessing training data, will dominate and constrain training capacity. Similar innovation in DSI is urgent, demanding an in-depth understanding of DSI systems, infrastructure, and characteristics. To this end, this paper presents Meta's end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service (DPP) that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across our global fleet, how massive and evolving datasets are stored and read, and how online preprocessing places intense demands on our underlying hardware. We synthesize key takeaways from our characterization and close with a discussion of lessons learned and research opportunities for both industry and academia.},
   author = {Mark Zhao and Niket Agarwal and Aarti Basant and Bugra Gedik and Satadru Pan and Mustafa Ozdal and Rakesh Komuravelli and Jerry Pan and Tianshu Bao and Haowei Lu and Sundaram Narayanan and Jack Langman and Kevin Wilfong and Harsha Rastogi and Carole-Jean Wu and Christos Kozyrakis and Parik Pol},
   pages = {1-14},
   title = {Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training},
   year = {2021},
}
@article{Mohan2021,
   abstract = {Training Deep Neural Networks (DNNs) is resource-intensive and time-consuming. While prior research has explored many different ways of reducing DNN training time, the impact of input data pipeline, i.e., fetching raw data items from storage and performing data pre-processing in memory, has been relatively unexplored. This paper makes the following contributions: (1) We present the first comprehensive analysis of how the input data pipeline affects the training time of widely-used computer vision and audio Deep Neural Networks (DNNs), that typically involve complex data preprocessing. We analyze nine different models across three tasks and four datasets while varying factors such as the amount of memory, number of CPU threads, storage device, GPU generation etc on servers that are a part of a large production cluster at Microsoft. We find that in many cases, DNN training time is dominated by data stall time: time spent waiting for data to be fetched and preprocessed. (2) We build a tool, DS-Analyzer to precisely measure data stalls using a differential technique, and perform predictive what-if analysis on data stalls. (3) Finally, based on the insights from our analysis, we design and implement three simple but effective techniques in a data-loading library, CoorDL, to mitigate data stalls. Our experiments on a range of DNN tasks, models, datasets, and hardware configs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI data loading library, DNN training time is reduced significantly (by as much as 5Ã on a single server).},
   author = {Jayashree Mohan and Amar Phanishayee and Ashish Raniwala and Vijay Chidambaram},
   doi = {10.14778/3446095.3446100},
   issn = {21508097},
   issue = {5},
   journal = {Proceedings of the VLDB Endowment},
   pages = {771-784},
   title = {Analyzing and mitigating data stalls in DNN training},
   volume = {14},
   year = {2021},
}
@article{Chowdhery2022,
   abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
   author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
   pages = {1-83},
   title = {PaLM: Scaling Language Modeling with Pathways},
   url = {http://arxiv.org/abs/2204.02311},
   year = {2022},
}
@article{Bauer2021,
   abstract = {Efficient low-variance gradient estimation enabled by the reparameterization trick (RT) has been essential to the success of variational autoencoders. Doubly-reparameterized gradients (DReGs) improve on the RT for multi-sample variational bounds by applying reparameterization a second time for an additional reduction in variance. Here, we develop two generalizations of the DReGs estimator and show that they can be used to train conditional and hierarchical VAEs on image modelling tasks more effectively. First, we extend the estimator to hierarchical models with several stochastic layers by showing how to treat additional score function terms due to the hierarchical variational posterior. We then generalize DReGs to score functions of arbitrary distributions instead of just those of the sampling distribution, which makes the estimator applicable to the parameters of the prior in addition to those of the posterior.},
   author = {Matthias Bauer and Andriy Mnih},
   title = {Generalized Doubly Reparameterized Gradient Estimators},
   url = {http://arxiv.org/abs/2101.11046},
   year = {2021},
}
@article{Houben2021,
   abstract = {The use of deep neural networks (DNNs) in safety-critical applications like mobile health and autonomous driving is challenging due to numerous model-inherent shortcomings. These shortcomings are diverse and range from a lack of generalization over insufficient interpretability to problems with malicious inputs. Cyber-physical systems employing DNNs are therefore likely to suffer from safety concerns. In recent years, a zoo of state-of-the-art techniques aiming to address these safety concerns has emerged. This work provides a structured and broad overview of them. We first identify categories of insufficiencies to then describe research activities aiming at their detection, quantification, or mitigation. Our paper addresses both machine learning experts and safety engineers: The former ones might profit from the broad range of machine learning topics covered and discussions on limitations of recent methods. The latter ones might gain insights into the specifics of modern ML methods. We moreover hope that our contribution fuels discussions on desiderata for ML systems and strategies on how to propel existing approaches accordingly.},
   author = {Sebastian Houben and Stephanie Abrecht and Maram Akila and Andreas BÃ¤r and Felix Brockherde and Patrick Feifel and Tim Fingscheidt and Sujan Sai Gannamaneni and Seyed Eghbal Ghobadi and Ahmed Hammam and Anselm Haselhoff and Felix Hauser and Christian Heinzemann and Marco Hoffmann and Nikhil Kapoor and Falk Kappel and Marvin Klingner and Jan Kronenberger and Fabian KÃ¼ppers and Jonas LÃ¶hdefink and Michael Mlynarski and Michael Mock and Firas Mualla and Svetlana Pavlitskaya and Maximilian Poretschkin and Alexander Pohl and Varun Ravi-Kumar and Julia Rosenzweig and Matthias Rottmann and Stefan RÃ¼ping and Timo SÃ¤mann and Jan David Schneider and Elena Schulz and Gesina Schwalbe and Joachim Sicking and Toshika Srivastava and Serin Varghese and Michael Weber and Sebastian Wirkert and Tim Wirtz and Matthias Woehrle},
   pages = {1-94},
   title = {Inspect, Understand, Overcome: A Survey of Practical Methods for AI Safety},
   url = {http://arxiv.org/abs/2104.14235},
   year = {2021},
}
@article{Ceschin2020,
   abstract = {Machine Learning (ML) has been widely applied to cybersecurity, and is currently considered state-of-the-art for solving many of the field's open issues. However, it is very difficult to evaluate how good the produced solutions are, since the challenges faced in security may not appear in other areas (at least not in the same way). One of these challenges is the concept drift, that actually creates an arms race between attackers and defenders, given that any attacker may create novel, different threats as time goes by (to overcome defense solutions) and this "evolution" is not always considered in many works. Due to this type of issue, it is fundamental to know how to correctly build and evaluate a ML-based security solution. In this work, we list, detail, and discuss some of the challenges of applying ML to cybersecurity, including concept drift, concept evolution, delayed labels, and adversarial machine learning. We also show how existing solutions fail and, in some cases, we propose possible solutions to fix them.},
   author = {FabrÃ­cio Ceschin and Heitor Murilo Gomes and Marcus Botacin and Albert Bifet and Bernhard Pfahringer and Luiz S. Oliveira and AndrÃ© GrÃ©gio},
   issue = {1},
   title = {Machine Learning (In) Security: A Stream of Problems},
   volume = {1},
   url = {http://arxiv.org/abs/2010.16045},
   year = {2020},
}
@article{Yang2022,
   abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.},
   author = {Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
   title = {Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
   url = {http://arxiv.org/abs/2203.03466},
   year = {2022},
}
@article{Liang2022,
   abstract = {For decades, people are developing efficient numerical methods for solving the challenging quantum many-body problem, whose Hilbert space grows exponentially with the size of the problem. However, this journey is far from over, as previous methods all have serious limitations. The recently developed deep learning methods provide a very promising new route to solve the long-standing quantum many-body problems. We report that a deep learning based simulation protocol can achieve the solution with state-of-the-art precision in the Hilbert space as large as $2^\{1296\}$ for spin system and $3^\{144\}$ for fermion system , using a HPC-AI hybrid framework on the new Sunway supercomputer. With highly scalability up to 40 million heterogeneous cores, our applications have measured 94% weak scaling efficiency and 72% strong scaling efficiency. The accomplishment of this work opens the door to simulate spin models and Fermion models on unprecedented lattice size with extreme high precision.},
   author = {Xiao Liang and Mingfan Li and Qian Xiao and Hong An and Lixin He and Xuncheng Zhao and Junshi Chen and Chao Yang and Fei Wang and Hong Qian and Li Shen and Dongning Jia and Yongjian Gu and Xin Liu and Zhiqiang Wei},
   keywords = {-,1 -,2 model,and the fermion systems,deep learning,e,g,heteroge-,lation,model,neous architecture,neural network quantum state,quantum simu-,re-,stochastic reconfiguration,sunway},
   title = {$2^\{1296\}$ Exponentially Complex Quantum Many-Body Simulation via Scalable Deep Learning Method},
   url = {http://arxiv.org/abs/2204.07816},
   year = {2022},
}
@article{Lin2019,
   abstract = {Deep learning technology is widely used in many modern fields and a number of deep learning models and software frameworks have been proposed. However, it is still very difficult to process deep learning tasks efficiently on traditional high performance computing (HPC) systems with specialized architectures such as Sunway TaihuLight. In this paper, we propose swFLOW: a TensorFlow-based dataflow deep learning framework on Sunway TaihuLight. Based on the performance analysis results on convolutional neural network (CNN), we optimize the convolution layer, reduce the data layout transpose operation and get 10.42x speedup compared to single management processing element (MPE) version. As for distributed training, we use elastic averaging stochastic gradient descent (EASGD) algorithm to reduce communication and use data prefetch to avoid data fetch being a performance bottleneck. On 512 processes, we get a parallel efficiency of 81.01% with communication period Ï = 8. Limited by the maximal executable batch size, the current performance of swFLOW is far from optimal. It is very necessary to further optimize using technology like remote direct memory access (RDMA) and model parallelism.},
   author = {Han Lin and Zeng Lin and Jose Monsalve Diaz and Mingfan Li and Hong An and Guang R. Gao},
   doi = {10.1109/HPCC/SmartCity/DSS.2019.00345},
   isbn = {9781728120584},
   journal = {Proceedings - 21st IEEE International Conference on High Performance Computing and Communications, 17th IEEE International Conference on Smart City and 5th IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2019},
   keywords = {Convolutional neural networks,Deep learning,High performance computing},
   pages = {2467-2475},
   publisher = {IEEE},
   title = {SwFLOW: A dataflow deep learning framework on sunway taihulight supercomputer},
   year = {2019},
}
@article{Sutton2011,
   abstract = {Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields. Â© 2012 C. Sutton and A. McCallum.},
   author = {Charles Sutton and Andrew McCallum},
   doi = {10.1561/2200000013},
   issn = {19358237},
   issue = {4},
   journal = {Foundations and Trends in Machine Learning},
   pages = {267-373},
   title = {An introduction to conditional random fields},
   volume = {4},
   year = {2011},
}
@article{Andreas2016,
   abstract = {Visual question answering is fundamentally compositional in nature - a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural 'modules' into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
   author = {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
   doi = {10.1109/CVPR.2016.12},
   isbn = {9781467388504},
   issn = {10636919},
   issue = {Figure 1},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {39-48},
   title = {Neural module networks},
   volume = {2016-Decem},
   year = {2016},
}
@article{Zhao2019,
   abstract = {PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI).},
   author = {Yue Zhao and Zain Nasrullah and Zheng Li},
   issn = {15337928},
   journal = {Journal of Machine Learning Research},
   keywords = {Anomaly detection,Data mining,Machine learning,Neural networks,Outlier detection,Outlier ensembles,Python},
   pages = {1-7},
   title = {PyOD: A python toolbox for scalable outlier detection},
   volume = {20},
   year = {2019},
}
@article{Etection2021,
   author = {O Utlier D Etection},
   title = {Suod : ACCELERATING LARGE-SCALE UNSUPERVISED HETEROGENEOUS OUTLIER DETECTION},
   year = {2021},
}
@article{Papadimitriou2015,
   abstract = {What is the mechanism of learning in the brain? Despite breathtaking advances in neuroscience, and in machine learning, we do not seem close to an answer. Using Valiant's neuronal model as a foundation, we introduce PJOIN (for "predictive join"), a primitive that combines association and prediction. We show that PJOIN can be implemented naturally in Valiant's conservative, formal model of cortical computation. Using PJOIN - and almost nothing else - we give a simple algorithm for unsupervised learning of arbitrary ensembles of binary patterns (solving an open problem in Valiant's work). This algorithm relies crucially on prediction, and entails significant downward traffic ("feedback") while parsing stimuli. Prediction and feedback are well-known features of neural cognition and, as far as we know, this is the first theoretical prediction of their essential role in learning.},
   author = {Christos H. Papadimitriou and Santosh S. Vempala},
   issn = {15337928},
   issue = {2015},
   journal = {Journal of Machine Learning Research},
   keywords = {Cortical Algorithms,Neuroidal Computation,Prediction,Predictive Join (PJOIN),Unsupervised Learning},
   pages = {1-21},
   title = {Cortical learning via prediction},
   volume = {40},
   year = {2015},
}
@article{Ren2020,
   author = {Yujie Ren and Changwoo Min and Virginia Tech and Changwoo Min},
   isbn = {9781939133199},
   journal = {Osdi},
   title = {CrossFS : A Cross-layered Direct-Access File System This paper is included in the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation},
   year = {2020},
}
@article{Patti2021,
   abstract = {Simulation is essential for developing quantum hardware and algorithms. However, simulating quantum circuits on classical hardware is challenging due to the exponential scaling of quantum state space. While factorized tensors can greatly reduce this overhead, tensor network-based simulators are relatively few and often lack crucial functionalities. To address this deficiency, we created TensorLy-Quantum, a Python library for quantum circuit simulation that adopts the PyTorch API. Our library leverages the optimized tensor methods of the existing TensorLy ecosystem to represent, simulate, and manipulate large-scale quantum circuits. Through compact tensor representations and efficient operations, TensorLy-Quantum can scale to hundreds of qubits on a single GPU and thousands of qubits on multiple GPUs. TensorLy-Quantum is open-source and accessible at https://github.com/tensorly/quantum},
   author = {Taylor L. Patti and Jean Kossaifi and Susanne F. Yelin and Anima Anandkumar},
   issue = {2021},
   pages = {1-6},
   title = {TensorLy-Quantum: Quantum Machine Learning with Tensor Methods},
   url = {http://arxiv.org/abs/2112.10239},
   year = {2021},
}
@article{Efthymiou2022,
   abstract = {We present a first attempt to perform circuit-based quantum simulation using the just-in-time (JIT) compilation technique on multiple hardware architectures and configurations based on single-node central processing units (CPUs) and graphics processing units (GPUs). One of the major challenges in scientific code development is to balance the level of complexity between algorithms and programming techniques without losing performance or degrading code readability. In this context, we have developed qibojit: a new module for the Qibo quantum computing framework, which uses a just-in-time compilation approach through Python. We perform systematic performance benchmarks between our JIT approach and a subset of relevant publicly available libraries for quantum computing. We show that our novel approach simplifies the complex aspects of the implementation without deteriorating performance.},
   author = {Stavros Efthymiou and Marco Lazzarin and Andrea Pasquale and Stefano Carrazza},
   pages = {1-9},
   title = {Quantum simulation with just-in-time compilation},
   url = {http://arxiv.org/abs/2203.08826},
   year = {2022},
}
@article{Kilmer2004,
   abstract = {Are there analogues to the SVD, LU, QR, and other matrix decom-positions for tensors (i.e., higher-order or multiway arrays)? What exactly do we mean by " analogue, " anyway? If such decompositions exist, are there efficient ways to compute them? These are some of the questions asked, and partially answered, at the Workshop on Tensor Decompositions, held July 19â23, 2004, at the American Institute of Mathematics in Palo Alto, California. Gene Golub, Tammy Kolda, James Nagy, and Charles Van Loan were the organizers. About 35 peopleâcomputer scientists, mathematicians, and a broad range of scholars who use tensor decompositions in their researchâhad come from eleven countries to participate in the week-long workshop. Large group discussions and smaller break-out ses-sions were interleaved with invited talks, making for lively exchanges and a creative learning environment. Carla Martin, a graduate student at Cornell University, opened the week with an overview of multilinear algebra and tensor decompositions, beginning with the definition of a pth-order tensor A as a multiway array with p indices. A third-order tensor, for example, is written A = (a ijk) â \ n 1Ã n 2Ã n 3 .},
   author = {Me Kilmer and Cdm Martin},
   issue = {9},
   journal = {SIAM News},
   pages = {2-4},
   title = {Decomposing a tensor},
   volume = {37},
   url = {http://www.mysiam.org/pdf/news/277.pdf},
   year = {2004},
}
@article{,
   author = {Lingjie Li and Wenjian Yu and Kim Batselier},
   issue = {Lingjie Li},
   keywords = {parallel-vector rounding,sparse data,tensor train decomposition,tt-rounding},
   title = {Faster Tensor Train Decomposition for Sparse Data},
}
@article{,
   author = {K I M Batselier and Wenjian Yu and Luca Daniel and Ngai Wong},
   keywords = {15a18,15a23,15a69,68w20,ams subject classifications,composition,curse of dimensionality,de-,low-rank tensor approximation,ma-,matrix factorization,randomized algorithm,singular value decompositon,svd,tensor network,tensor train,trix product operator,tt},
   pages = {1-20},
   title = {Computing low-rank approximations of large-scale matrices with the tensor network randomized svd},
}
@book{Mishra2019,
   author = {Nimish Mishra and Manik Kapil and Hemant Rakesh and Amit Anand and Nilima Mishra and Aditya Prasad Dash and Rakshit Gharat and Yagnik Chatterjee and Shuvarati Roy},
   doi = {10.1007/978-981-15-5619-7},
   isbn = {9789811556197},
   issue = {September},
   keywords = {Quantum machine learning,Quantum renormalization p},
   publisher = {Springer Singapore},
   title = {Quantum Machine Learning : A Review and Current Status Quantum Machine Learning : A Review},
   url = {http://dx.doi.org/10.1007/978-981-15-5619-7_8},
   year = {2019},
}
@article{,
   author = {Chase Roberts and Ashley Milsted and Martin Ganahl and Adam Zalcman and Yijian Zou and Jack Hidary and Guifre Vidal and Stefan Leichenauer},
   pages = {1-11},
   title = {TensorNetwork: A Library for Physics and Machine Learning},
}
@article{Stoudenmire2017,
   author = {E Miles Stoudenmire and David J Schwab},
   pages = {1-12},
   title = {Supervised Learning With Quantum-Inspired Tensor Networks},
   year = {2017},
}
@article{,
   author = {David Yakira},
   issue = {2015},
   title = {Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design},
}
@article{Thompson2021,
   author = {A P Thompson and A Cangi and S Rajamanickam},
   pages = {1-19},
   title = {Accelerating Finite-Temperature Kohn-Sham Density Functional Theory with Deep Neural Networks},
   year = {2021},
}
@article{,
   author = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Emily Denton and Seyed Kamyar and Seyed Ghasemipour and Burcu Karagol Ayan},
   title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
}
@article{Weyn2020,
   author = {Jonathan A Weyn},
   doi = {10.1029/2020MS002109},
   title = {Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere},
   year = {2020},
}
@article{Kenesei2021,
   author = {Peter Kenesei and Dennis Trujillo and Ryan Coffee and Menlo Park and Jana Thayer and Menlo Park and Nicholas Schwarz},
   title = {BRIDGING DATA CENTER AI SYSTEMS WITH EDGE COMPUTING FOR ACTIONABLE INFORMATION},
   year = {2021},
}
@article{,
   author = {Jayaraman J Thiagarajan and Peer-timo Bremer and Brian K Spears},
   title = {Improved Surrogates in Inertial Confinement Fusion with Manifold and Cycle Consistencies},
}
@article{,
   keywords = {active flow control,com,corresponding author,deep reinforcement learning,gmail,machine learning,mohamed elhawary maaelhwary,numerical simula-,plasma actuator,tion},
   pages = {1-19},
   title = {Deep Reinforcement Learning for Active Flow Control around a Circular Cylinder Using Un- steady-mode Plasma Actuators M. A. Elhawary},
}
@article{Brunton2020,
   author = {Steven L Brunton and Bernd R Noack and Petros Koumoutsakos},
   keywords = {control,data-driven modeling,machine learning,optimization},
   title = {Machine Learning for Fluid Mechanics},
   year = {2020},
}
@article{Emani2021,
   author = {Murali Emani and Venkatram Vishwanath and Corey Adams and Michael E Papka and Rick Stevens},
   pages = {1-6},
   title = {Accelerating Scientific Applications with SambaNova Reconfigurable Dataflow Architecture},
   year = {2021},
}
@article{Carpanese2022,
   author = {Francesco Carpanese and Timo Ewalds and Roland Hafner and Abbas Abdolmaleki and Diego De Casas and Craig Donner and Leslie Fritz and Cristian Galperti and Andrea Huber and James Keeling and Maria Tsimpoukelli and Jackie Kay and Antoine Merle and Jean-marc Moret and Seb Noury and Federico Pesamosca and David Pfau and Olivier Sauter and Cristian Sommariva and Stefano Coda and Basil Duval and Ambrogio Fasoli and Pushmeet Kohli and Koray Kavukcuoglu},
   issue = {February},
   title = {Magnetic control of tokamak plasmas through deep reinforcement learning},
   volume = {602},
   year = {2022},
}
@article{Fan2020,
   author = {Dixia Fan and Liu Yang and Zhicheng Wang and Michael S Triantafyllou and George Em},
   doi = {10.1073/pnas.2004939117},
   isbn = {2004939117},
   issue = {42},
   pages = {26091-26098},
   title = {Reinforcement learning for bluff body active flow control in experiments and simulations},
   volume = {117},
   year = {2020},
}
@article{,
   author = {Sam Ade and Jacobs Brian and Van Essen David and Hysom Jae-seung Yeom and Tim Moon and Rushil Anirudh and Jayaraman J Thiagaranjan and Shusen Liu and Peer-timo Bremer Jim and Gaffney Tom and Benson Peter},
   title = {Parallelizing Training of Deep Generative Models on Massive Scientific Datasets},
}
@article{,
   author = {Piotr Doll},
   title = {Benchmarking Detection Transfer Learning with Vision Transformers},
}
@article{Summary2019,
   author = {Review Summary},
   doi = {10.1126/science.aau0323},
   issue = {March},
   title = {Machine learning for data-driven discovery in solid Earth geoscience},
   volume = {1299},
   year = {2019},
}
@article{,
   author = {Ricardo Vinuesa and Steven L Brunton},
   pages = {1-13},
   title = {The Potential of Machine Learning to Enhance Computational Fluid Dynamics},
}
@article{,
   author = {Maithra Raghu},
   pages = {1-48},
   title = {A Survey of Deep Learning for Scientific Discovery},
}
@article{Willard2020,
   author = {Jared Willard and Xiaowei Jia and Shaoming Xu and Michael Steinbach and Vipin Kumar},
   issue = {1},
   pages = {1-35},
   title = {Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems JARED},
   volume = {1},
   year = {2020},
}
@article{Simonyan2016,
   author = {Karen Simonyan and Sander Dieleman and Andrew Senior and Alex Graves},
   pages = {1-15},
   title = {WAVENET: A GENERATIVE MODEL FOR RAW AUDIO},
   year = {2016},
}
@article{,
   author = {Nicholas Malaya},
   isbn = {9781450379830},
   keywords = {acm reference format,ai for science,computational fluid dynamics,deep learning,learning coupled framework,physics-machine,turbulent flows},
   title = {CFDNet : A deep learning-based accelerator for fluid simulations},
}
@article{Kochkov2021,
   author = {Dmitrii Kochkov and Jamie A Smith and Ayya Alieva and Qing Wang and Michael P Brenner and Stephan Hoyer},
   doi = {10.1073/pnas.2101784118/-/DCSupplemental.y},
   title = {Machine learning â accelerated computational fluid dynamics},
   year = {2021},
}
@article{Thompson2015,
   abstract = {We present a new interatomic potential for solids and liquids called Spectral Neighbor Analysis Potential (SNAP). The SNAP potential has a very general form and uses machine-learning techniques to reproduce the energies, forces, and stress tensors of a large set of small configurations of atoms, which are obtained using high-accuracy quantum electronic structure (QM) calculations. The local environment of each atom is characterized by a set of bispectrum components of the local neighbor density projected onto a basis of hyperspherical harmonics in four dimensions. The bispectrum components are the same bond-orientational order parameters employed by the GAP potential [1]. The SNAP potential, unlike GAP, assumes a linear relationship between atom energy and bispectrum components. The linear SNAP coefficients are determined using weighted least-squares linear regression against the full QM training set. This allows the SNAP potential to be fit in a robust, automated manner to large QM data sets using many bispectrum components. The calculation of the bispectrum components and the SNAP potential are implemented in the LAMMPS parallel molecular dynamics code. We demonstrate that a previously unnoticed symmetry property can be exploited to reduce the computational cost of the force calculations by more than one order of magnitude. We present results for a SNAP potential for tantalum, showing that it accurately reproduces a range of commonly calculated properties of both the crystalline solid and the liquid phases. In addition, unlike simpler existing potentials, SNAP correctly predicts the energy barrier for screw dislocation migration in BCC tantalum.},
   author = {A. P. Thompson and L. P. Swiler and C. R. Trott and S. M. Foiles and G. J. Tucker},
   doi = {10.1016/j.jcp.2014.12.018},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Gaussian approximation potentials,Interatomic potential,Machine learning,Molecular dynamics,SNAP,Spectral neighbor analysis potential},
   pages = {316-330},
   publisher = {Elsevier Inc.},
   title = {Spectral neighbor analysis method for automated generation of quantum-accurate interatomic potentials},
   volume = {285},
   url = {http://dx.doi.org/10.1016/j.jcp.2014.12.018},
   year = {2015},
}
@article{,
   abstract = {We introduce a class of interatomic potential models that can be automatically generated from data consisting of the energies and forces experienced by atoms, as derived from quantum mechanical calculations. The models do not have a fixed functional form and hence are capable of modeling complex potential energy landscapes. They are systematically improvable with more data. We apply the method to bulk crystals, and test it by calculating properties at high temperatures. Using the interatomic potential to generate the long molecular dynamics trajectories required for such calculations saves orders of magnitude in computational cost. Â© 2010 The American Physical Society.},
   author = {Albert P. BartÃ³k and Mike C. Payne and Risi Kondor and GÃ¡bor CsÃ¡nyi},
   doi = {10.1103/PhysRevLett.104.136403},
   issn = {00319007},
   issue = {13},
   journal = {Physical Review Letters},
   pmid = {20481899},
   title = {Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons},
   volume = {104},
   year = {2010},
}
@article{Dette2005,
   abstract = {We determine optimal designs for some regression models which are frequently used for describing three-dimensional shapes. These models are based on a Fourier expansion of a function defined on the unit sphere in terms of spherical harmonic basis functions. In particular, it is demonstrated that the uniform distribution on the sphere is optimal with respect to all Î¦p criteria proposed by Kiefer in 1974 and also optimal with respect to a criterion which maximizes a p mean of the r smallest eigenvalues of the variance-covariance matrix. This criterion is related to principal component analysis, which is the common tool for analyzing this type of image data. Moreover, discrete designs on the sphere are derived, which yield the same information matrix in the spherical harmonic regression model as the uniform distribution and are therefore directly implementable in practice. It is demonstrated that the new designs are substantially more efficient than the commonly used designs in three-dimensional shape analysis. Â© Institute of Mathematical Statistics, 2005.},
   author = {Holger Dette and Viatcheslav B. Melas and Andrey Pepelyshev},
   doi = {10.1214/009053605000000552},
   issn = {00905364},
   issue = {6},
   journal = {Annals of Statistics},
   keywords = {3D image data,Optimal designs,Principal component analysis,Quadrature formulas,Shape analysis,Spherical harmonic descriptors},
   pages = {2758-2788},
   title = {Optimal designs for three-dimensional shape analysis with spherical harmonic descriptors},
   volume = {33},
   year = {2005},
}
@article{Asim2020,
   abstract = {Trained generative models have shown remarkable performance as priors for inverse problems in imaging for example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors. Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting. Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios, and due to their lack of representation error, invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images. We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.},
   author = {Muhammad Asim and Max Daniels and Oscar Leong and Ali Ahmed and Paul Hand},
   isbn = {9781713821120},
   journal = {37th International Conference on Machine Learning, ICML 2020},
   pages = {376-386},
   title = {Invertible generative models for inverse problems: Mitigating representation error and dataset bias},
   volume = {PartF16814},
   year = {2020},
}
@article{Krylov2021,
   abstract = {In this paper authors deal with tasks of reliably recover a hidden multi-dimensional model parameter from indirect process observations. Such task is known as inverse problem. There are a lot of inverse problems that have practical value, for example in seismic wave propagation, low-dose tomography. To solve many of these problems in a practical style, this article proposes an approach based on the many simulations of the corresponding forward problem and using the set of simulation data as the training dataset. Most of physical processes have computer models that generate precise results. The existing simulators provide ways to predict process output by input parameters. A difficulty in solving of most inverse problems is that the solution is sensitive to variations in data, which is referred to as ill-posedness. From broad spectrum of methods to overcome ill-posedness authors use machine learning model trained on special simulated data. The paper describes the deep network model using some regularization. The key idea is to use Generative Adversarial Network (GAN) to generate correct input parameters values and support the unique existence. This network is trained by parameter examples that are real solutions of inverse problem. The small manually built dataset transforms to infinite dataset automatically by GAN. The augmented dataset feeds the simulator toget output data to train deep learning network. The network has regularization layers to support stability. The paper describes details of this model using deep augmentation to solve inverse problems on the easy example: the task of throwing a heavy ball at an angle to the horizon, taking into account the force of friction against air.},
   author = {Sergei Krylov and Vladimir Krylov},
   doi = {10.1088/1742-6596/1727/1/012002},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   title = {Inverse Problem Solving Approach Using Deep Network Trained by GAN Simulated Data},
   volume = {1727},
   year = {2021},
}
@article{Gillhofer2019,
   abstract = {We propose a GAN based approach to solve inverse problems which have non-differentiable or even black-box forward relations. The idea is to find solutions via an adversarial game where the generator has to propose new samples and the discriminator has to assess the quality of the samples with respect to the forward relation f. However, instead of attempting to approximate f directly, the discriminator only has to solve a binary classification task in local regions populated by the generated samples. We demonstrate the efficacy of our approach by applying it to an artificially generated topology optimization problem. We show that our method leads to similar results like more traditional topology optimization methods.},
   author = {Michael Gillhofer and Hubert Ramsauer and Johannes Brandstetter and Bernhard SchÃ¤fl and Sepp Hochreiter},
   issue = {NeurIPS},
   pages = {1-5},
   title = {A GAN based solver of black-box inverse problems},
   year = {2019},
}
@article{Han2018,
   abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
   author = {Zhao Yu Han and Jun Wang and Heng Fan and Lei Wang and Pan Zhang},
   doi = {10.1103/PhysRevX.8.031012},
   issn = {21603308},
   issue = {3},
   journal = {Physical Review X},
   keywords = {computational physics,condensed matter physics,doi:10.1103/PhysRevX.8.031012 url:https://doi.org/},
   pages = {31012},
   publisher = {American Physical Society},
   title = {Unsupervised Generative Modeling Using Matrix Product States},
   volume = {8},
   url = {https://doi.org/10.1103/PhysRevX.8.031012},
   year = {2018},
}
@article{Shao2020,
   abstract = {Atomic neural networks (ANNs) constitute a class of machine learning methods for predicting potential energy surfaces and physicochemical properties of molecules and materials. Despite many successes, developing interpretable ANN architectures and implementing existing ones efficiently are still challenging. This calls for reliable, general-purpose, and open-source codes. Here, we present a python library named PiNN as a solution toward this goal. In PiNN, we designed a new interpretable and high-performing graph convolutional neural network variant, PiNet, as well as implemented the established Behler-Parrinello neural network. These implementations were tested using datasets of isolated small molecules, crystalline materials, liquid water, and an aqueous alkaline electrolyte. PiNN comes with a visualizer called PiNNBoard to extract chemical insight "learned"by ANNs. It provides analytical stress tensor calculations and interfaces to both the atomic simulation environment and a development version of the Amsterdam Modeling Suite. Moreover, PiNN is highly modularized, which makes it useful not only as a standalone package but also as a chain of tools to develop and to implement novel ANNs. The code is distributed under a permissive BSD license and is freely accessible at https://github.com/Teoroo-CMC/PiNN/with full documentation and tutorials.},
   author = {Yunqi Shao and Matti HellstrÃ¶m and Pavlin D. Mitev and Lisanne Knijff and Chao Zhang},
   doi = {10.1021/acs.jcim.9b00994},
   issn = {1549960X},
   issue = {3},
   journal = {Journal of Chemical Information and Modeling},
   pages = {1184-1193},
   pmid = {31935100},
   title = {PiNN: A Python Library for Building Atomic Neural Networks of Molecules and Materials},
   volume = {60},
   year = {2020},
}
@article{Zolnouri2020,
   author = {Mahdi Zolnouri and Xinlin Li and Vahid Partovi Nia},
   issue = {1},
   pages = {1-7},
   title = {Importance of data loading pIpeline in training},
   year = {2020},
}
@article{Lu2021,
   abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
   author = {Lu Lu and Pengzhan Jin and Guofei Pang and Zhongqiang Zhang and George Em Karniadakis},
   doi = {10.1038/s42256-021-00302-5},
   issn = {25225839},
   issue = {3},
   journal = {Nature Machine Intelligence},
   pages = {218-229},
   title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
   volume = {3},
   year = {2021},
}
@article{Raissi2017,
   abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves.},
   author = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
   issue = {Part I},
   keywords = {data-driven scientific computing,machine learning,modeling,nonlinear dynamics,predictive,runge-kutta methods},
   pages = {1-22},
   title = {Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},
   url = {http://arxiv.org/abs/1711.10566},
   year = {2017},
}
@article{Yang2019,
   abstract = {Data loading can dominate deep neural network training time on large-scale systems. We present a comprehensive study on accelerating data loading performance in large-scale distributed training. We first identify performance and scalability issues in current data loading implementations. We then propose optimizations that utilize CPU resources to the data loader design. We use an analytical model to characterize the impact of data loading on the overall training time and establish the performance trend as we scale up distributed training. Our model suggests that I/O rate limits the scalability of distributed training, which inspires us to design a locality-aware data loading method. By utilizing software caches, our method can drastically reduce the data loading communication volume in comparison with the original data loading implementation. Finally, we evaluate the proposed optimizations with various experiments. We achieved more than 30x speedup in data loading using 256 nodes with 1,024 learners.},
   author = {Chih Chieh Yang and Guojing Cong},
   doi = {10.1109/HiPC.2019.00037},
   isbn = {9781728145358},
   journal = {Proceedings - 26th IEEE International Conference on High Performance Computing, HiPC 2019},
   keywords = {data loading,data locality,distributed training,machine learning,scalability},
   pages = {235-245},
   title = {Accelerating Data Loading in Deep Neural Network Training},
   year = {2019},
}
@article{Matsui2017,
   author = {Yusuke Matsui},
   isbn = {9781450349062},
   keywords = {billion-scale,clustering,clustering include large memory,con-,costs,issues,k-means,owing to these two,product quantization,sumption and prohibitive runtime,the problems of large-scale},
   pages = {1725-1733},
   title = {PQk-means : Billion-scale Clustering for Product-quantized Codes},
   year = {2017},
}
@article{Li2020,
   abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
   author = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
   issue = {2016},
   pages = {1-16},
   title = {Fourier Neural Operator for Parametric Partial Differential Equations},
   url = {http://arxiv.org/abs/2010.08895},
   year = {2020},
}
@article{Dryden2019,
   abstract = {We identify communication as a major bottleneck for training deep neural networks on large-scale GPU clusters, taking over 10x as long as computation. To reduce this overhead, we discuss techniques to overlap communication and computation as much as possible. This leads to much of the communication being latency-bound instead of bandwidth-bound, and we find that using a combination of latency- and bandwidth-optimized allreduce algorithms significantly reduces communication costs. We also discuss a semantic mismatch between MPI and CUDA that increases overheads and limits asynchrony, and propose a solution that enables communication to be aware of CUDA streams. We implement these optimizations in the open-source Aluminum communication library, enabling optimized, asynchronous, GPU-aware communication. Aluminum demonstrates improved performance in benchmarks and end-to-end training of deep networks, for both strong and weak scaling.},
   author = {Nikoli Dryden and Naoya Maruyama and Tim Moon and Tom Benson and Andy Yoo and Marc Snir and Brian Van Essen},
   doi = {10.1109/MLHPC.2018.8638639},
   isbn = {9781728101804},
   journal = {Proceedings of MLHPC 2018: Machine Learning in HPC Environments, Held in conjunction with SC 2018: The International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {Collective algorithms,Communication optimization,Deep learning,HPC,Machine learning},
   pages = {1-13},
   title = {Aluminum: An Asynchronous, GPU-Aware Communication Library Optimized for Large-Scale Training of Deep Neural Networks on HPC Systems},
   year = {2019},
}
@article{Dryden2019,
   abstract = {Accelerating large-scale CNN training is needed to keep training times reasonable as datasets grow larger and models become more complex. Existing frameworks primarily scale using data-parallelism, but this is limited by the mini-batch size, which cannot grow arbitrarily. We introduce three algorithms that partition channel or filter data to exploit parallelism beyond the sample dimension. Further, they partition the parameters of convolutional layers, replacing global all reduces with segmented allreduces - -smaller, concurrent allreduces among disjoint processor sets. These algorithms enable strong scaling, reduced communication overhead, and reduced memory pressure, enabling training of very wide CNNs. We demonstrate improved strong and weak scaling, including up to 4.1x reductions in training time for residual networks and 4x reductions in allreduce overhead. We also show that wider models provide improved accuracy on ImageNet. We study the current limitations of our algorithms and provide a direction for future optimizations of large-scale deep learning frameworks.},
   author = {Nikoli Dryden and Naoya Maruyama and Tim Moon and Tom Benson and Marc Snir and Brian Van Essen},
   doi = {10.1145/3295500.3356207},
   isbn = {9781450362290},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   keywords = {Algorithms,CNN,Convolution,Deep learning,Scaling},
   title = {Channel and filter parallelism for large-scale CNN training},
   year = {2019},
}
@article{,
   author = {Brian Van Essen and Hyojin Kim and Roger Pearce and Kofi Boakye and Barry Chen},
   doi = {10.1145/2834892.2834897},
   isbn = {9781450340069},
   keywords = {artificial neural networks,deep learning,high performance},
   pages = {1-6},
   title = {LBANN: Livermore Big Artificial Neural Network HPC Toolkit},
   year = {2015},
}
@article{Dryden2019,
   abstract = {Scaling CNN training is necessary to keep up with growing datasets and reduce training time. We also see an emerging need to handle datasets with very large samples, where memory requirements for training are large. Existing training frameworks use a data-parallel approach that partitions samples within a mini-batch, but limits to scaling the minibatch size and memory consumption makes this untenable for large samples. We describe and implement new approaches to convolution, which parallelize using spatial decomposition or a combination of sample and spatial decomposition. This introduces many performance knobs for a network, so we develop a performance model for CNNs and present a method for using it to automatically determine efficient parallelization strategies. We evaluate our algorithms with microbenchmarks and image classification with ResNet-50. Our algorithms allow us to prototype a model for a mesh-tangling dataset, where sample sizes are very large. We show that our parallelization achieves excellent strong and weak scaling and enables training for previously unreachable datasets.},
   author = {Nikoli Dryden and Naoya Maruyama and Tom Benson and Tim Moon and Marc Snir and Brian Van Essen},
   doi = {10.1109/IPDPS.2019.00031},
   isbn = {9781728112466},
   journal = {Proceedings - 2019 IEEE 33rd International Parallel and Distributed Processing Symposium, IPDPS 2019},
   keywords = {Algorithms,Convolution,Deep learning,HPC,Performance modeling},
   pages = {210-220},
   title = {Improving strong-scaling of CNN training by exploiting finer-grained parallelism},
   year = {2019},
}
@article{Walsh2006,
   abstract = {Without Abstract},
   author = {John B. Walsh},
   doi = {10.1007/bfb0074920},
   isbn = {9783540707806},
   journal = {Wikipedia},
   pages = {265-439},
   title = {An introduction to stochastic partial differential equations},
   year = {2006},
}
@article{Weinan2018,
   abstract = {We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
   author = {E. Weinan and Bing Yu},
   doi = {10.1007/s40304-018-0127-z},
   issn = {2194671X},
   issue = {1},
   journal = {Communications in Mathematics and Statistics},
   keywords = {Deep Ritz Method,Eigenvalue problems,PDE,Variational problems},
   pages = {1-12},
   title = {The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
   volume = {6},
   year = {2018},
}
@article{Hesameddini2015,
   abstract = {In this article, we propose a new approach for solving fractional partial differential equations with variable coefficients, which is very effective and can also be applied to other types of differential equations. The main advantage of the method lies in its flexibility for obtaining the approximate solutions of time fractional and space fractional equations. The fractional derivatives are described based on the Caputo sense. Our method contains an iterative formula that can provide rapidly convergent successive approximations of the exact solution if such a closed form solution exists. Several examples are given, and the numerical results are shown to demonstrate the efficiency of the newly proposed method.},
   author = {Esmail Hesameddini and Azam Rahimi},
   doi = {10.1515/zna-2015-1017},
   issn = {09320784},
   issue = {5},
   journal = {Zeitschrift fur Naturforschung - Section A Journal of Physical Sciences},
   keywords = {Caputo derivative,Fractional partial differential equation,Reconstruction of variational iteration method},
   pages = {375-382},
   title = {Solving fractional partial differential equations with variable coefficients by the reconstruction of variational iteration method},
   volume = {70},
   year = {2015},
}
@article{Javed2017,
   abstract = {The Adomian decomposition method is a semi-analytical method for solving ordinary and partial nonlinear differential equations. The aim of this paper is to apply Adomian decomposition method to obtain approximate solutions of nonlinear fractional order partial differential equations with fractional derivatives. The fractional derivatives are taken in the sense of Caputo. The solutions of fractional PDEs are calculated in the form of convergent series. Approximate solutions obtained through the decomposition method have been numerically evaluated, and presented in the form of graphs and tables, and then these solutions are compared with the exact solutions and the results rendering the explicitness, effectiveness and good accuracy of the applied method. Finally, it is observed that the applied method (i.e. Adomian decomposition method) is prevailing and convergent method for the solutions of nonlinear fractional-order partial deferential problems.},
   author = {Iqra Javed and Ashfaq Ahmad and Muzammil Hussain and S. Iqbal},
   keywords = {adomian decomposition method,fractional derivatives,partial differential equations},
   title = {Some Solutions of Fractional Order Partial Differential Equations Using Adomian Decomposition Method},
   url = {http://arxiv.org/abs/1712.09207},
   year = {2017},
}
@article{Hairer2009,
   abstract = {These notes are based on a series of lectures given first at the University of Warwick in spring 2008 and then at the Courant Institute in spring 2009. It is an attempt to give a reasonably self-contained presentation of the basic theory of stochastic partial differential equations, taking for granted basic measure theory, functional analysis and probability theory, but nothing else. The approach taken in these notes is to focus on semilinear parabolic problems driven by additive noise. These can be treated as stochastic evolution equations in some infinite-dimensional Banach or Hilbert space that usually have nice regularising properties and they already form a very rich class of problems with many interesting properties. Furthermore, this class of problems has the advantage of allowing to completely pass under silence many subtle problems arising from stochastic integration in infinite-dimensional spaces.},
   author = {Martin Hairer},
   title = {An Introduction to Stochastic PDEs},
   url = {http://arxiv.org/abs/0907.4178},
   year = {2009},
}
@article{Dalang2009,
   abstract = {Most documented extinctions of vertebrates in the last 400 years have\nbeen island endemics. In this paper, we focus on the need to develop\na historical framework to establish conservation priorities for insular\nfaunas and, in particular, to test the validity of nominal endemics.\nWe use the example of the islands of the North Pacific Coast (NPC)\nof North America, a region that includes approximately one-half of\nall mammals endemic to North American islands north of Mexico (seven\nspecies and 67 subspecies). Few of these endemics have been re-evaluated\nsince their original descriptions, although many of these islands\nhave been heavily impacted by habitat conversion, species introductions,\nover-exploitation, and secondary ripple effects. Evidence from molecular\ngenetics and paleontology suggests that many taxa arrived in the\nregion since the last glacial advance. Some of these nominal endemics\nshow minimal differentiation, while others comprise multiple evolutionary\nlineages. The NPC may also have played an important role in the in\nsitu diversification of some taxa (paleoendemics) during the Pleistocene.\nEvidence, such as new fossil discoveries and numerous described endemics,\nlends support for refugia hypotheses. However, these ideas cannot\nbe tested until a better understanding of geographic variation and\nthe evolutionary relationships of the fauna and flora of the North\nPacific Coast is developed. This framework would provide significant\ninsight into the dynamic biogeographic history of the region and\nhelp prioritize conservation efforts},
   author = {Robert C Dalang and Carl Mueller and Salt Lake City},
   doi = {10.1007/978-3-540-85994-9},
   issn = {0075-8434},
   journal = {A Minicourse on Stochastic Partial Differential Equations},
   title = {A Minicourse on Stochastic Partial Differential Equations},
   year = {2009},
}
@article{Wang2018,
   abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 Ã - 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
   author = {Ting Chun Wang and Ming Yu Liu and Jun Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   doi = {10.1109/CVPR.2018.00917},
   isbn = {9781538664209},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {8798-8807},
   title = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
   year = {2018},
}
@article{Khoshnevisan2009,
   abstract = {These notes form a brief introductory tutorial to elements of Gaussian noise analysis and basic stochastic partial differential equations (SPDEs) in general, and the stochastic heat equation, in particular. The chief aim here is to get to the heart of the matter quickly. We achieve this by studying a few concrete equations only. This chapter provides sufficient preparation for learning more advanced theory from the remainder of this volume. Â© 2009 Springer.},
   author = {Davar Khoshnevisan},
   doi = {10.1007/978-3-540-85994-9_1},
   isbn = {9783540859932},
   issn = {00758434},
   issue = {1},
   journal = {Lecture Notes in Mathematics},
   keywords = {erential equations,gaussian processes,martingale,measures,regularity of processes,stochastic partial di ff,white noise},
   pages = {1-38},
   title = {A primer on stochastic partial differential equations},
   volume = {1962},
   year = {2009},
}
@article{Isola2017,
   abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
   author = {Phillip Isola and Jun Yan Zhu and Tinghui Zhou and Alexei A. Efros},
   doi = {10.1109/CVPR.2017.632},
   isbn = {9781538604571},
   journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   pages = {5967-5976},
   title = {Image-to-image translation with conditional adversarial networks},
   volume = {2017-Janua},
   year = {2017},
}
@book{,
   abstract = {A neural network (NN)-based method is presented in this paper which allows the identification of parameters for material cards used in Finite Element simulations. Contrary to the conventionally used computationally intensive material parameter identification (MPI) by numerical optimization with internal or commercial software, a machine learning (ML)-based method is time saving when used repeatedly. Within this article, a self-developed ML-based Python framework is presented, which offers advantages, especially in the development of structural components in early development phases. In this procedure, different machine learning methods are used and adapted to the specific MPI problem considered herein. Using the developed NN-based and the common optimization-based method with LS-OPT, the material parameters of the LS-DYNA material card MAT_187_SAMP-1 and the failure model GISSMO were exemplarily calibrated for a virtually generated test dataset. Parameters for the description of elasticity, plasticity, tensionâcompression asymmetry, variable plastic Poissonâs ratio (VPPR), strain rate dependency and failure were taken into account. The focus of this paper is on performing a comparative study of the two different MPI methods with varying settings (algorithms, hyperparameters, etc.). Furthermore, the applicability of the NN-based procedure for the specific usage of both material cards was investigated. The studies reveal the general applicability for the calibration of a complex material card by the example of the used MAT_187_SAMP-1.},
   author = {Paul MeiÃner and Jens Winter and Thomas Vietor},
   doi = {10.3390/ma15020643},
   isbn = {4953139165019},
   issn = {19961944},
   issue = {2},
   journal = {Materials},
   keywords = {GISSMO failure model,Hyperparameter optimization,LS-DYNA,MAT_187_SAMP-1,Machine learning,Parameter identification},
   pages = {1-36},
   title = {Methodology for Neural Network-Based Material Card Calibration Using LS-DYNA MAT_187_SAMP-1 Considering Failure with GISSMO},
   volume = {15},
   year = {2022},
}
@article{Wang2021,
   abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reactionâdiffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.},
   author = {Sifan Wang and Hanwen Wang and Paris Perdikaris},
   doi = {10.1016/j.cma.2021.113938},
   issn = {00457825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Deep learning,Neural Tangent Kernel,Partial differential equations,Scientific machine learning,Spectral bias},
   pages = {1-27},
   title = {On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks},
   volume = {384},
   year = {2021},
}
@article{Sitzmann2020,
   abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail. They also fail to accurately model spatial and temporal derivatives, which is necessary to represent signals defined implicitly by differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, three-dimensional shapes, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
   author = {Vincent Sitzmann and Julien N.P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   title = {Implicit neural representations with periodic activation functions},
   volume = {2020-Decem},
   year = {2020},
}
@article{Bu2021,
   abstract = {We propose quadratic residual networks (QRes) as a new type of parameter-efficient neural network architecture, by adding a quadratic residual term to the weighted sum of inputs before applying activation functions. With sufficiently high functional capacity (or expressive power), we show that it is especially powerful for solving forward and inverse physics problems involving partial differential equations (PDEs). Using tools from algebraic geometry, we theoretically demonstrate that, in contrast to plain neural networks, QRes shows better parameter efficiency in terms of network width and depth thanks to higher non-linearity in every neuron. Finally, we empirically show that QRes shows faster convergence speed in terms of number of training epochs especially in learning complex patterns.},
   author = {Jie Bu and Anuj Karpatne},
   doi = {10.1137/1.9781611976700.76},
   isbn = {9781611976700},
   journal = {SIAM International Conference on Data Mining, SDM 2021},
   pages = {675-683},
   title = {Quadratic residual networks: A new class of neural networks for solving forward and inverse problems in physics involving PDEs},
   year = {2021},
}
@article{Fathony2021,
   abstract = {Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance fields or neural radiance fields. Key to these recent successes has been the use of new elements such as sinusoidal nonlinearities, or Fourier features in positional encodings, which vastly outperform simple ReLU networks. In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks. In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis functions, respectively. Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these recent approaches on the domains highlighted in these past works.},
   author = {Rizal Fathony and Anit Kumar Sahu and Devin Willmott and J. Zico Kolter},
   journal = {Iclr},
   pages = {1-10},
   title = {Multiplicative Filter Networks},
   year = {2021},
}
@article{,
   abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of $\{1920\!\times\!1080\}$.},
   author = {Thomas MÃ¼ller and Alex Evans and Christoph Schied and Alexander Keller},
   doi = {10.1145/3528223.3530127},
   issue = {4},
   journal = {ACM Transactions on Graphics},
   keywords = {Image Synthesis, Neural Networks, Encodings, Hashi},
   publisher = {Association for Computing Machinery},
   title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
   volume = {41},
   url = {http://arxiv.org/abs/2201.05989%0Ahttp://dx.doi.org/10.1145/3528223.3530127},
   year = {2022},
}
@article{Heinzerling2017,
   abstract = {We present BPEmb, a collection of pre-trained subword unit embeddings in 275 languages, based on Byte-Pair Encoding (BPE). In an evaluation using fine-grained entity typing as testbed, BPEmb performs competitively, and for some languages bet- ter than alternative subword approaches, while requiring vastly fewer resources and no tokenization. BPEmb is available at https://github.com/bheinzerling/bpemb},
   author = {Benjamin Heinzerling and Michael Strube},
   month = {10},
   title = {BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages},
   url = {http://arxiv.org/abs/1710.02187},
   year = {2017},
}
@article{Guibas2021,
   abstract = {Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms.},
   author = {John Guibas and Morteza Mardani and Zongyi Li and Andrew Tao and Anima Anandkumar and Bryan Catanzaro},
   month = {11},
   title = {Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers},
   url = {http://arxiv.org/abs/2111.13587},
   year = {2021},
}
@article{Mildenhall2020,
   abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
   author = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
   month = {3},
   title = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
   url = {http://arxiv.org/abs/2003.08934},
   year = {2020},
}
@article{Peharz2020,
   abstract = {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.},
   author = {Robert Peharz and Steven Lang and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Guy Van den Broeck and Kristian Kersting and Zoubin Ghahramani},
   month = {4},
   title = {Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits},
   url = {http://arxiv.org/abs/2004.06231},
   year = {2020},
}
@misc{,
   abstract = {Inference in discrete graphical models with variational methods is difficult because of the inability to re-parameterize gradients of the Evidence Lower Bound (ELBO). Many sampling-based methods have been proposed for estimating these gradients, but they suffer from high bias or variance. In this paper, we propose a new approach that leverages the tractability of probabilistic circuit models, such as Sum Product Networks (SPN), to compute ELBO gradients exactly (without sampling) for a certain class of densities. In particular, we show that selective-SPNs are suitable as an expressive variational distribution, and prove that when the log-density of the target model is a polynomial the corresponding ELBO can be computed analytically. To scale to graphical models with thousands of variables, we develop an efficient and effective construction of selective-SPNs with size O(kn), where n is the number of variables and k is an adjustable hyperparameter. We demonstrate our approach on three types of graphical models-Ising models, Latent Dirichlet Allocation, and factor graphs from the UAI Inference Competition. Selective-SPNs give a better lower bound than mean-field and structured mean-field, and is competitive with approximations that do not provide a lower bound, such as Loopy Belief Propagation and Tree-Reweighted Belief Propagation. Our results show that probabilistic circuits are promising tools for variational inference in discrete graphical models as they combine tractability and expressivity.},
   author = {Andy Shih and Stefano Ermon},
   title = {Probabilistic Circuits for Variational Inference in Discrete Graphical Models},
}
@article{Sorscher2022,
   abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
   author = {Ben Sorscher and Robert Geirhos and Shashank Shekhar and Surya Ganguli and Ari S. Morcos},
   month = {6},
   title = {Beyond neural scaling laws: beating power law scaling via data pruning},
   url = {http://arxiv.org/abs/2206.14486},
   year = {2022},
}
@article{Dao2022,
   abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).},
   author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher RÃ©},
   month = {5},
   title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
   url = {http://arxiv.org/abs/2205.14135},
   year = {2022},
}
@article{Johnson2020,
   abstract = {When using large-batch training to speed up stochastic gradient descent, learning rates must adapt to new batch sizes in order to maximize speed-ups and preserve model quality. Re-tuning learning rates is resource intensive, while fixed scaling rules often degrade model quality. We propose AdaScale SGD, an algorithm that reliably adapts learning rates to large-batch training. By continually adapting to the gradient's variance, AdaScale automatically achieves speed-ups for a wide range of batch sizes. We formally describe this quality with AdaScale's convergence bound, which maintains final objective values, even as batch sizes grow large and the number of iterations decreases. In empirical comparisons, AdaScale trains well beyond the batch size limits of popular "linear learning rate scaling" rules. This includes large-batch training with no model degradation for machine translation, image classification, object detection, and speech recognition tasks. AdaScale's qualitative behavior is similar to that of "warm-up" heuristics, but unlike warm-up, this behavior emerges naturally from a principled mechanism. The algorithm introduces negligible computational overhead and no new hyperparameters, making AdaScale an attractive choice for large-scale training in practice.},
   author = {Tyler B. Johnson and Pulkit Agrawal and Haijie Gu and Carlos Guestrin},
   month = {7},
   title = {AdaScale SGD: A User-Friendly Algorithm for Distributed Training},
   url = {http://arxiv.org/abs/2007.05105},
   year = {2020},
}
@article{Ranade2021,
   abstract = {Numerical simulations for engineering applications solve partial differential equations (PDE) to model various physical processes. Traditional PDE solvers are very accurate but computationally costly. On the other hand, Machine Learning (ML) methods offer a significant computational speedup but face challenges with accuracy and generalization to different PDE conditions, such as geometry, boundary conditions, initial conditions and PDE source terms. In this work, we propose a novel ML-based approach, CoAE-MLSim (Composable AutoEncoder Machine Learning Simulation), which is an unsupervised, lower-dimensional, local method, that is motivated from key ideas used in commercial PDE solvers. This allows our approach to learn better with relatively fewer samples of PDE solutions. The proposed ML-approach is compared against commercial solvers for better benchmarks as well as latest ML-approaches for solving PDEs. It is tested for a variety of complex engineering cases to demonstrate its computational speed, accuracy, scalability, and generalization across different PDE conditions. The results show that our approach captures physics accurately across all metrics of comparison (including measures such as results on section cuts and lines).},
   author = {Rishikesh Ranade and Chris Hill and Haiyang He and Amir Maleki and Norman Chang and Jay Pathak},
   month = {10},
   title = {A composable autoencoder-based iterative algorithm for accelerating numerical simulations},
   url = {http://arxiv.org/abs/2110.03780},
   year = {2021},
}
@article{Bredin2019,
   abstract = {We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.},
   author = {HervÃ© Bredin and Ruiqing Yin and Juan Manuel Coria and Gregory Gelly and Pavel Korshunov and Marvin Lavechin and Diego Fustes and Hadrien Titeux and Wassim Bouaziz and Marie-Philippe Gill},
   month = {11},
   title = {pyannote.audio: neural building blocks for speaker diarization},
   url = {http://arxiv.org/abs/1911.01255},
   year = {2019},
}
@article{Wei2022,
   abstract = {Dialogue act classification (DAC) is a critical task for spoken language understanding in dialogue systems. Prosodic features such as energy and pitch have been shown to be useful for DAC. Despite their importance, little research has explored neural approaches to integrate prosodic features into end-to-end (E2E) DAC models which infer dialogue acts directly from audio signals. In this work, we propose an E2E neural architecture that takes into account the need for characterizing prosodic phenomena co-occurring at different levels inside an utterance. A novel part of this architecture is a learnable gating mechanism that assesses the importance of prosodic features and selectively retains core information necessary for E2E DAC. Our proposed model improves DAC accuracy by 1.07% absolute across three publicly available benchmark datasets.},
   author = {Kai Wei and Dillon Knox and Martin Radfar and Thanh Tran and Markus Muller and Grant P. Strimel and Nathan Susanj and Athanasios Mouchtaris and Maurizio Omologo},
   month = {5},
   title = {A neural prosody encoder for end-ro-end dialogue act classification},
   url = {http://arxiv.org/abs/2205.05590},
   year = {2022},
}
@article{Park2021,
   abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
   author = {Tae Jin Park and Naoyuki Kanda and Dimitrios Dimitriadis and Kyu J. Han and Shinji Watanabe and Shrikanth Narayanan},
   month = {1},
   title = {A Review of Speaker Diarization: Recent Advances with Deep Learning},
   url = {http://arxiv.org/abs/2101.09624},
   year = {2021},
}
@article{Hsu2021,
   abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
   author = {Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
   month = {6},
   title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
   url = {http://arxiv.org/abs/2106.07447},
   year = {2021},
}
@article{Li2018,
   abstract = {Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.},
   author = {Jing Li and Aixin Sun and Jianglei Han and Chenliang Li},
   month = {12},
   title = {A Survey on Deep Learning for Named Entity Recognition},
   url = {http://arxiv.org/abs/1812.09449},
   year = {2018},
}
@article{Lu2021,
   abstract = {Neural operators can learn nonlinear mappings between function spaces and offer a new simulation paradigm for real-time prediction of complex dynamics for realistic diverse applications as well as for system identification in science and engineering. Herein, we investigate the performance of two neural operators, and we develop new practical extensions that will make them more accurate and robust and importantly more suitable for industrial-complexity applications. The first neural operator, DeepONet, was published in 2019, and the second one, named Fourier Neural Operator or FNO, was published in 2020. In order to compare FNO with DeepONet for realistic setups, we develop several extensions of FNO that can deal with complex geometric domains as well as mappings where the input and output function spaces are of different dimensions. We also endow DeepONet with special features that provide inductive bias and accelerate training, and we present a faster implementation of DeepONet with cost comparable to the computational cost of FNO. We consider 16 different benchmarks to demonstrate the relative performance of the two neural operators, including instability wave analysis in hypersonic boundary layers, prediction of the vorticity field of a flapping airfoil, porous media simulations in complex-geometry domains, etc. The performance of DeepONet and FNO is comparable for relatively simple settings, but for complex geometries and especially noisy data, the performance of FNO deteriorates greatly. For example, for the instability wave analysis with only 0.1% noise added to the input data, the error of FNO increases 10000 times making it inappropriate for such important applications, while there is hardly any effect of such noise on the DeepONet. We also compare theoretically the two neural operators and obtain similar error estimates for DeepONet and FNO under the same regularity assumptions.},
   author = {Lu Lu and Xuhui Meng and Shengze Cai and Zhiping Mao and Somdatta Goswami and Zhongqiang Zhang and George Em Karniadakis},
   doi = {10.1016/j.cma.2022.114778},
   title = {A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data},
   year = {2021},
}
@article{Liu2022,
   abstract = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
   author = {Zhijian Liu and Haotian Tang and Alexander Amini and Xinyu Yang and Huizi Mao and Daniela Rus and Song Han},
   title = {BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation},
   year = {2022},
}
@misc{,
   abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
   author = {Daniel Povey and Arnab Ghoshal and Gilles Boulianne and LukÃ¡Å¡ Burget and OndÅej Glembek and Nagendra Goel and Mirko Hannemann and Petr MotlÃ­Äek and Yanmin Qian and Petr Schwarz and Jan SilovskÂ´y SilovskÂ´y and Georg Stemmer and Karel VeselÂ´y VeselÂ´y},
   title = {The Kaldi Speech Recognition Toolkit},
   url = {http://kaldi.sf.net/},
}
@article{Ravanelli2021,
   abstract = {SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.},
   author = {Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and FranÃ§ois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
   month = {6},
   title = {SpeechBrain: A General-Purpose Speech Toolkit},
   url = {http://arxiv.org/abs/2106.04624},
   year = {2021},
}
@article{Mishra2020,
   abstract = {Physics informed neural networks (PINNs) have recently been very successfully applied for efficiently approximating inverse problems for PDEs. We focus on a particular class of inverse problems, the so-called data assimilation or unique continuation problems, and prove rigorous estimates on the generalization error of PINNs approximating them. An abstract framework is presented and conditional stability estimates for the underlying inverse problem are employed to derive the estimate on the PINN generalization error, providing rigorous justification for the use of PINNs in this context. The abstract framework is illustrated with examples of four prototypical linear PDEs. Numerical experiments, validating the proposed theory, are also presented.},
   author = {Siddhartha Mishra and Roberto Molinaro},
   month = {6},
   title = {Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating a class of inverse problems for PDEs},
   url = {http://arxiv.org/abs/2007.01138},
   year = {2020},
}
@article{Cuomo2022,
   abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
   author = {Salvatore Cuomo and Vincenzo Schiano di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli},
   month = {1},
   title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
   url = {http://arxiv.org/abs/2201.05624},
   year = {2022},
}
@article{Cai2021,
   abstract = {Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier-Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implementing them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.},
   author = {Shengze Cai and Zhiping Mao and Zhicheng Wang and Minglang Yin and George Em Karniadakis},
   month = {5},
   title = {Physics-informed neural networks (PINNs) for fluid mechanics: A review},
   url = {http://arxiv.org/abs/2105.09506},
   year = {2021},
}
@article{Lu2019,
   abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from the implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an education tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry, and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging Scientific Machine Learning field.},
   author = {Lu Lu and Xuhui Meng and Zhiping Mao and George E. Karniadakis},
   doi = {10.1137/19M1274067},
   month = {7},
   title = {DeepXDE: A deep learning library for solving differential equations},
   url = {http://arxiv.org/abs/1907.04502 http://dx.doi.org/10.1137/19M1274067},
   year = {2019},
}
@misc{Karniadakis2021,
   abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
   author = {George Em Karniadakis and Ioannis G. Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
   doi = {10.1038/s42254-021-00314-5},
   issn = {25225820},
   issue = {6},
   journal = {Nature Reviews Physics},
   month = {6},
   pages = {422-440},
   publisher = {Springer Nature},
   title = {Physics-informed machine learning},
   volume = {3},
   year = {2021},
}
@article{Thuerey2021,
   abstract = {This digital book contains a practical and comprehensive introduction of everything related to deep learning in the context of physical simulations. As much as possible, all topics come with hands-on code examples in the form of Jupyter notebooks to quickly get started. Beyond standard supervised learning from data, we'll look at physical loss constraints, more tightly coupled learning algorithms with differentiable simulations, as well as reinforcement learning and uncertainty modeling. We live in exciting times: these methods have a huge potential to fundamentally change what computer simulations can achieve.},
   author = {Nils Thuerey and Philipp Holl and Maximilian Mueller and Patrick Schnell and Felix Trost and Kiwon Um},
   month = {9},
   title = {Physics-based Deep Learning},
   url = {http://arxiv.org/abs/2109.05237},
   year = {2021},
}
@article{Raissi2017,
   abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
   author = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
   month = {11},
   title = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
   url = {http://arxiv.org/abs/1711.10561},
   year = {2017},
}
@article{Workshop2022,
   abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
   author = {BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana IliÄ and Daniel Hesslow and Roman CastagnÃ© and Alexandra Sasha Luccioni and FranÃ§ois Yvon and Matthias GallÃ© and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and BenoÃ®t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo LaurenÃ§on and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo GonzÃ¡lez Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and GÃ©rard Dupont and GermÃ¡n Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and JÃ¶rg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero MuÃ±oz and Maraim Masoud and MarÃ­a Grandury and Mario Å aÅ¡ko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis LÃ³pez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre TaÅar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre FranÃ§ois LavallÃ©e and RÃ©mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and StÃ©phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and AurÃ©lie NÃ©vÃ©ol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and ZdenÄk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos MuÃ±oz Ferrandis and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and ClÃ©mentine Fourrier and Daniel LeÃ³n PeriÃ±Ã¡n and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc PÃ mies and Maria A Castillo and Marianna Nezhurina and Mario SÃ¤nger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and ThÃ©o Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
   month = {11},
   title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
   url = {http://arxiv.org/abs/2211.05100},
   year = {2022},
}
@article{Reynolds2021,
   abstract = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
   author = {Laria Reynolds and Kyle McDonell},
   month = {2},
   title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
   url = {http://arxiv.org/abs/2102.07350},
   year = {2021},
}
@article{Liu2021,
   abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
   author = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
   month = {7},
   title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
   url = {http://arxiv.org/abs/2107.13586},
   year = {2021},
}
@misc{,
   abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summariza-tion. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outper-forms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
   author = {Xiang Lisa Li and Percy Liang},
   pages = {4582-4597},
   title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
}
@article{Gao2020,
   abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
   author = {Tianyu Gao and Adam Fisch and Danqi Chen},
   month = {12},
   title = {Making Pre-trained Language Models Better Few-shot Learners},
   url = {http://arxiv.org/abs/2012.15723},
   year = {2020},
}
@article{Paolini2021,
   abstract = {We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.},
   author = {Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},
   month = {1},
   title = {Structured Prediction as Translation between Augmented Natural Languages},
   url = {http://arxiv.org/abs/2101.05779},
   year = {2021},
}
@article{Schick2023,
   abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
   author = {Timo Schick and Jane Dwivedi-Yu and Roberto DessÃ¬ and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
   month = {2},
   title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
   url = {http://arxiv.org/abs/2302.04761},
   year = {2023},
}
@article{Bai2022,
   abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
   author = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
   month = {4},
   title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
   url = {http://arxiv.org/abs/2204.05862},
   year = {2022},
}
@article{Wei2022,
   abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
   author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
   month = {1},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   url = {http://arxiv.org/abs/2201.11903},
   year = {2022},
}
@article{Sanh2021,
   abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
   author = {Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},
   month = {10},
   title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
   url = {http://arxiv.org/abs/2110.08207},
   year = {2021},
}
@article{Leike2018,
   abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
   author = {Jan Leike and David Krueger and Tom Everitt and Miljan Martic and Vishal Maini and Shane Legg},
   month = {11},
   title = {Scalable agent alignment via reward modeling: a research direction},
   url = {http://arxiv.org/abs/1811.07871},
   year = {2018},
}
@article{Lester2021,
   abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
   author = {Brian Lester and Rami Al-Rfou and Noah Constant},
   title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
   year = {2021},
}
@article{Tillet2019,
   author = {Philippe Tillet and H. T. Kung and David Cox},
   doi = {10.1145/3315508.3329973},
   pages = {10-19},
   title = {Triton: an intermediate language and compiler for tiled neural network computations},
   year = {2019},
}
@article{Cao2023,
   abstract = {Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.},
   author = {Yihan Cao and Siyu Li and Yixin Liu and Zhiling Yan and Yutong Dai and Philip S. Yu and Lichao Sun},
   month = {3},
   title = {A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT},
   url = {http://arxiv.org/abs/2303.04226},
   year = {2023},
}
@misc{Akidau2150,
   abstract = {Unbounded, unordered, global-scale datasets are increasingly common in day-today business (e.g. Web logs, mobile usage statistics, and sensor networks). At the same time, consumers of these datasets have evolved sophisticated requirements , such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of cor-rectness, latency, and cost for these types of input. As a result , data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems. We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive, old data may be retracted, and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness, latency, and cost. In this paper, we present one such approach, the Dataflow Model 1 , along with a detailed examination of the semantics it enables, an overview of the core principles that guided its design, and a validation of the model itself via the real-world experiences that led to its development.},
   author = {Tyler Akidau and Robert Bradshaw and Craig Chambers and Slava Chernyak and Rafael J FernÃ¡ndez and FernÂ´ FernÃ¡ndez-Moctezuma and Reuven Lax and Sam Mcveety and Daniel Mills and Frances Perry and Eric Schmidt and Sam Whittle Google},
   title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing},
   year = {2150},
}
@misc{,
   abstract = {SUNDR is a network file system designed to store data securely on untrusted servers. SUNDR lets clients detect any attempts at unauthorized file modification by malicious server operators or users. SUNDR's protocol achieves a property called fork consistency, which guarantees that clients can detect any integrity or consistency failures as long as they see each other's file modifications. An implementation is described that performs comparably with NFS (sometimes better and sometimes worse), while offering significantly stronger security.},
   author = {Jinyuan Li and Maxwell Krohn and David MazÃ¬ and Dennis Shasha},
   title = {Secure Untrusted Data Repository (SUNDR)},
}
@misc{,
   abstract = {Chain replication is a new approach to coordinating clusters of fail-stop storage servers. The approach is intended for supporting large-scale storage services that exhibit high throughput and availability without sacrificing strong consistency guarantees. Besides outlining the chain replication protocols themselves , simulation experiments explore the performance characteristics of a prototype implementation. Throughput, availability, and several object-placement strategies (including schemes based on distributed hash table routing) are discussed.},
   author = {Robbert Van Renesse and Fred B Schneider},
   title = {Chain Replication for Supporting High Throughput and Availability},
}
@misc{Castro1999,
   abstract = {This paper describes a new replication algorithm that is able to tolerate Byzantine faults. We believe that Byzantine-fault-tolerant algorithms will be increasingly important in the future because malicious attacks and software errors are increasingly common and can cause faulty nodes to exhibit arbitrary behavior. Whereas previous algorithms assumed a synchronous system or were too slow to be used in practice, the algorithm described in this paper is practical: it works in asynchronous environments like the Internet and incorporates several important optimizations that improve the response time of previous algorithms by more than an order of magnitude. We implemented a Byzantine-fault-tolerant NFS service using our algorithm and measured its performance. The results show that our service is only 3% slower than a standard unreplicated NFS.},
   author = {Miguel Castro and Barbara Liskov},
   title = {Practical Byzantine Fault Tolerance},
   year = {1999},
}
@misc{,
   abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the pro-gram's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many ter-abytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
   author = {Jeffrey Dean and Sanjay Ghemawat},
   title = {MapReduce: Simplified Data Processing on Large Clusters},
}
@misc{,
   abstract = {Memcached is a well known, simple, in-memory caching solution. This paper describes how Facebook leverages memcached as a building block to construct and scale a distributed key-value store that supports the world's largest social network. Our system handles billions of requests per second and holds trillions of items to deliver a rich experience for over a billion users around the world.},
   author = {Rajesh Nishtala and Hans Fugal and Steven Grimm and Marc Kwiatkowski and Herman Lee and Harry C Li and Ryan Mcelroy and Mike Paleczny and Daniel Peek and Paul Saab and David Stafford and Tony Tung and Venkateshwaran Venkataramani},
   pages = {385},
   title = {Scaling Memcache at Facebook},
}
@misc{,
   abstract = {Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: non-blocking reads in the past, lock-free read-only transactions , and atomic schema changes, across all of Spanner.},
   author = {James C Corbett and Jeffrey Dean and Michael Epstein and Andrew Fikes and Christopher Frost and J J Furman and Sanjay Ghemawat and Andrey Gubarev and Christopher Heiser and Peter Hochschild and Wilson Hsieh and Sebastian Kanthak and Eugene Kogan and Hongyi Li and Alexander Lloyd and Sergey Melnik and David Mwaura and David Nagle and Sean Quinlan and Rajesh Rao and Lindsay Rolig and Yasushi Saito and Michal Szymaniak and Christopher Taylor and Ruth Wang and Dale Woodford},
   title = {Spanner: Google's Globally-Distributed Database},
}
@misc{,
   abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandabil-ity, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
   author = {Diego Ongaro and John Ousterhout},
   title = {In Search of an Understandable Consensus Algorithm (Extended Version)},
}
@misc{,
   abstract = {The ideal distributed file system would provide all its users with coherent , shared access to the same set of files,yet would be arbitrarily scalable to provide more storage space and higher performance to a growing user community. It would be highly available in spite of component failures. It would require minimal human administration , and administration would not become more complex as more components were added. Frangipani is a new file system that approximates this ideal, yet was relatively easy to build because of its two-layer structure. The lower layer is Petal (described in an earlier paper), a distributed storage service that provides incrementally scalable, highly available , automatically managed virtual disks. In the upper layer, multiple machines run the same Frangipani file system code on top of a shared Petal virtual disk, using a distributed lock service to ensure coherence. Frangipani is meant to run in a cluster of machines that are under a common administration and can communicate securely. Thus the machines trust one another and the shared virtual disk approach is practical. Of course, a Frangipani file system can be exported to untrusted machines using ordinary network file access protocols. We have implemented Frangipani on a collection of Alphas running DIGITAL Unix 4.0. Initial measurements indicate that Frangipani has excellent single-server performance and scales well as servers are added.},
   author = {Chandramohan A Thekkath and Timothy Mann and Edward K Lee},
   title = {Frangipani: A Scalable Distributed File System},
}
@misc{,
   abstract = {We have implemented a commercial enterprise-grade system for providing fault-tolerant virtual machines, based on the approach of replicating the execution of a primary virtual machine (VM) via a backup virtual machine on another server. We have designed a complete system in VMware vSphere 4.0 that is easy to use, runs on commodity servers, and typically reduces performance of real applications by less than 10%. In addition, the data bandwidth needed to keep the primary and secondary VM executing in lockstep is less than 20 Mbit/s for several real applications, which allows for the possibility of implementing fault tolerance over longer distances. An easy-to-use, commercial system that automatically restores redundancy after failure requires many additional components beyond replicated VM execution. We have designed and implemented these extra components and addressed many practical issues encountered in supporting VMs running enterprise applications. In this paper , we describe our basic design, discuss alternate design choices and a number of the implementation details, and provide performance results for both micro-benchmarks and real applications.},
   author = {Daniel J Scales and Mike Nelson and Ganesh Venkitachalam},
   title = {The Design of a Practical System for Fault-Tolerant Virtual Machines},
}
@misc{,
   abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
   author = {Matei Zaharia and Mosharaf Chowdhury and Tathagata Das and Ankur Dave and Justin Ma and Murphy Mccauley and Michael J Franklin and Scott Shenker},
   title = {Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing},
}
@misc{,
   abstract = {In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a repli-cated, centralized service. The interface exposed by Zoo-Keeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability for all requests that change the ZooKeeper state. These design decisions enable the implementation of a high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2:1 to 100:1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications.},
   author = {Patrick Hunt and Mahadev Konar and Yahoo ! Grid and Flavio P Junqueira and Benjamin Reed and Yahoo ! Research},
   title = {ZooKeeper: Wait-free coordination for Internet-scale systems},
}
@article{Yin2018,
   abstract = {We present HotStuff, a leader-based Byzantine fault-tolerant replication protocol for the partially synchronous model. Once network communication becomes synchronous, HotStuff enables a correct leader to drive the protocol to consensus at the pace of actual (vs. maximum) network delay--a property called responsiveness--and with communication complexity that is linear in the number of replicas. To our knowledge, HotStuff is the first partially synchronous BFT replication protocol exhibiting these combined properties. HotStuff is built around a novel framework that forms a bridge between classical BFT foundations and blockchains. It allows the expression of other known protocols (DLS, PBFT, Tendermint, Casper), and ours, in a common framework. Our deployment of HotStuff over a network with over 100 replicas achieves throughput and latency comparable to that of BFT-SMaRt, while enjoying linear communication footprint during leader failover (vs. quadratic with BFT-SMaRt).},
   author = {Maofan Yin and Dahlia Malkhi and Michael K. Reiter and Guy Golan Gueta and Ittai Abraham},
   month = {3},
   title = {HotStuff: BFT Consensus in the Lens of Blockchain},
   url = {http://arxiv.org/abs/1803.05069},
   year = {2018},
}
@inproceedings{,
   abstract = {Transactions with strong consistency and high availability simplify building and reasoning about distributed systems. However, previous implementations performed poorly. This forced system designers to avoid transactions completely, to weaken consistency guarantees, or to provide singlemachine transactions that require programmers to partition their data. In this paper, we show that there is no need to compromise in modern data centers. We show that a main memory distributed computing platform called FaRM can provide distributed transactions with strict serializability, high performance, durability, and high availability. FaRM achieves a peak throughput of 140 million TATP transactions per second on 90 machines with a 4.9 TB database, and it recovers from a failure in less than 50 ms. Key to achieving these results was the design of new transaction, replication, and recovery protocols from first principles to leverage commodity networks with RDMA and a new, inexpensive approach to providing non-volatile DRAM.},
   author = {Aleksandar DragojeviÄ and Dushyanth Narayanan and Edmund B. Nightingale and Matthew Renzelmann and Alex Shamis and Anirudh Badam and Miguel Castro},
   doi = {10.1145/2815400.2815425},
   isbn = {9781450338349},
   journal = {SOSP 2015 - Proceedings of the 25th ACM Symposium on Operating Systems Principles},
   month = {10},
   pages = {54-70},
   publisher = {Association for Computing Machinery, Inc},
   title = {No compromises: Distributed transactions with consistency, availability, and performance},
   year = {2015},
}
@article{Raffel2019,
   abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
   author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
   month = {10},
   title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
   url = {http://arxiv.org/abs/1910.10683},
   year = {2019},
}
@article{Rombach2021,
   abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
   author = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},
   title = {High-Resolution Image Synthesis with Latent Diffusion Models},
   year = {2021},
}
@article{Min2021,
   abstract = {Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.},
   author = {Bonan Min and Hayley Ross and Elior Sulem and Amir Pouran Ben Veyseh and Thien Huu Nguyen and Oscar Sainz and Eneko Agirre and Ilana Heinz and Dan Roth},
   title = {Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey},
   year = {2021},
}
@article{Shen2023,
   abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.},
   author = {Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
   month = {3},
   title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
   url = {http://arxiv.org/abs/2303.17580},
   year = {2023},
}
@article{Mialon2023,
   abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
   author = {GrÃ©goire Mialon and Roberto DessÃ¬ and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste RoziÃ¨re and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
   title = {Augmented Language Models: a Survey},
   year = {2023},
}
@article{Phuong2022,
   abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
   author = {Mary Phuong and Marcus Hutter},
   month = {7},
   title = {Formal Algorithms for Transformers},
   url = {http://arxiv.org/abs/2207.09238},
   year = {2022},
}
@article{Hu2021,
   abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
   author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
   month = {6},
   title = {LoRA: Low-Rank Adaptation of Large Language Models},
   url = {http://arxiv.org/abs/2106.09685},
   year = {2021},
}
@article{Park2023,
   abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
   author = {Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
   month = {4},
   title = {Generative Agents: Interactive Simulacra of Human Behavior},
   url = {http://arxiv.org/abs/2304.03442},
   year = {2023},
}
@article{Mnih2013,
   abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   month = {12},
   title = {Playing Atari with Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/1312.5602},
   year = {2013},
}
@article{Schulman2017,
   abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
   author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
   month = {7},
   title = {Proximal Policy Optimization Algorithms},
   url = {http://arxiv.org/abs/1707.06347},
   year = {2017},
}
@misc{,
   abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
   author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L Wainwright and Pamela Mishkin and Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askell and Peter Welinder Paul Christiano and Jan Leike and Ryan Lowe},
   title = {Training language models to follow instructions with human feedback},
}
@article{Askell2021,
   abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
   author = {Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
   month = {12},
   title = {A General Language Assistant as a Laboratory for Alignment},
   url = {http://arxiv.org/abs/2112.00861},
   year = {2021},
}
@article{Qin2023,
   abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
   author = {Chengwei Qin and Aston Zhang and Zhuosheng Zhang and Jiaao Chen and Michihiro Yasunaga and Diyi Yang},
   month = {2},
   title = {Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
   url = {http://arxiv.org/abs/2302.06476},
   year = {2023},
}
@article{Shi2017,
   abstract = {Deep learning frameworks have been widely deployed on GPU servers for deep learning applications in both academia and industry. In training deep neural networks (DNNs), there are many standard processes or algorithms, such as convolution and stochastic gradient descent (SGD), but the running performance of different frameworks might be different even running the same deep model on the same GPU hardware. In this study, we evaluate the running performance of four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet, and TensorFlow) over single-GPU, multi-GPU, and multi-node environments. We first build performance models of standard processes in training DNNs with SGD, and then we benchmark the running performance of these frameworks with three popular convolutional neural networks (i.e., AlexNet, GoogleNet and ResNet-50), after that, we analyze what factors that result in the performance gap among these four frameworks. Through both analytical and experimental analysis, we identify bottlenecks and overheads which could be further optimized. The main contribution is that the proposed performance models and the analysis provide further optimization directions in both algorithmic design and system configuration.},
   author = {Shaohuai Shi and Qiang Wang and Xiaowen Chu},
   month = {11},
   title = {Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs},
   url = {http://arxiv.org/abs/1711.05979},
   year = {2017},
}
@article{Wang2017,
   abstract = {Graphics Processing Units (GPUs) support dynamic voltage and frequency scaling (DVFS) in order to balance computational performance and energy consumption. However, there still lacks simple and accurate performance estimation of a given GPU kernel under different frequency settings on real hardware, which is important to decide best frequency configuration for energy saving. This paper reveals a fine-grained model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Over a 2.5x range of both core and memory frequencies among 12 GPU kernels, our model achieves accurate results (within 3.5\%) on real hardware. Compared with the cycle-level simulators, our model only needs some simple micro-benchmark to extract a set of hardware parameters and performance counters of the kernels to produce this high accuracy.},
   author = {Qiang Wang and Xiaowen Chu},
   month = {1},
   title = {GPGPU Performance Estimation with Core and Memory Frequency Scaling},
   url = {http://arxiv.org/abs/1701.05308},
   year = {2017},
}
@misc{,
   title = {GPU performance modeling and optimization},
   url = {www.tue.nl/taverne},
}
@article{Khairy2019,
   abstract = {With the skyrocketing advances of process technology, the increased need to process huge amount of data, and the pivotal need for power efficiency, the usage of Graphics Processing Units (GPUs) for General Purpose Computing becomes a trend and natural. GPUs have high computational power and excellent performance per watt, for data parallel applications, relative to traditional multicore processors. GPUs appear as discrete or embedded with CPUs, leading to a scheme of heterogeneous computing. Heterogeneous computing brings as many challenges as it brings opportunities. To get the most of such systems, we need to guarantee high GPU utilization, deal with irregular control flow of some workloads, and struggle with far-friendly-programming models. The aim of this paper is to provide a survey about GPUs from two perspectives: architectural advances to improved performance and programmability and advances to enhance CPUâGPU integration in heterogeneous systems. This will help researchers see the opportunities and challenges of using GPUs for general purpose computing, especially in the era of big data and the continuous need of high-performance computing.},
   author = {Mahmoud Khairy and Amr G. Wassal and Mohamed Zahran},
   doi = {10.1016/j.jpdc.2018.11.012},
   issn = {07437315},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Control divergence,GPGPU,Heterogeneous architecture,Memory systems},
   month = {5},
   pages = {65-88},
   publisher = {Academic Press Inc.},
   title = {A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity},
   volume = {127},
   year = {2019},
}
@article{Reuther2019,
   abstract = {Advances in multicore processors and accelerators have opened the flood gates to greater exploration and application of machine learning techniques to a variety of applications. These advances, along with breakdowns of several trends including Moore's Law, have prompted an explosion of processors and accelerators that promise even greater computational and machine learning capabilities. These processors and accelerators are coming in many forms, from CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys the current state of these processors and accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. We then select and benchmark two commercially-available low size, weight, and power (SWaP) accelerators as these processors are the most interesting for embedded and mobile machine learning inference applications that are most applicable to the DoD and other SWaP constrained users. We determine how they actually perform with real-world images and neural network models, compare those results to the reported performance and power consumption values and evaluate them against an Intel CPU that is used in some embedded applications.},
   author = {Albert Reuther and Peter Michaleas and Michael Jones and Vijay Gadepally and Siddharth Samsi and Jeremy Kepner},
   doi = {10.1109/HPEC.2019.8916327},
   month = {8},
   title = {Survey and Benchmarking of Machine Learning Accelerators},
   url = {http://arxiv.org/abs/1908.11348 http://dx.doi.org/10.1109/HPEC.2019.8916327},
   year = {2019},
}
@article{Baischer2021,
   abstract = {Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of application such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required memory size. For this reason, optimized hardware accelerators are used to increase the performance of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), application specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for reducing the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable applications is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the effect of DNN optimization methods on accuracy and throughput. For this reason, the focus of this work is more on FPGA-based implementations.},
   author = {Lukas Baischer and Matthias Wess and Nima TaheriNejad},
   month = {4},
   title = {Learning on Hardware: A Tutorial on Neural Network Accelerators and Co-Processors},
   url = {http://arxiv.org/abs/2104.09252},
   year = {2021},
}
@misc{Ardestani2018,
   author = {Ali Shafiee Ardestani},
   title = {DESIGN AND OPTIMIZATION OF HARDWARE ACCELERATORS FOR DEEP LEARNING},
   year = {2018},
}
@misc{Chen2020,
   abstract = {Recently, due to the availability of big data and the rapid growth of computing power, artificial intelligence (AI) has regained tremendous attention and investment. Machine learning (ML) approaches have been successfully applied to solve many problems in academia and in industry. Although the explosion of big data applications is driving the development of ML, it also imposes severe challenges of data processing speed and scalability on conventional computer systems. Computing platforms that are dedicatedly designed for AI applications have been considered, ranging from a complement to von Neumann platforms to a âmust-haveâ and stand-alone technical solution. These platforms, which belong to a larger category named âdomain-specific computing,â focus on specific customization for AI. In this article, we focus on summarizing the recent advances in accelerator designs for deep neural networks (DNNs)âthat is, DNN accelerators. We discuss various architectures that support DNN executions in terms of computing units, dataflow optimization, targeted network topologies, architectures on emerging technologies, and accelerators for emerging applications. We also provide our visions on the future trend of AI chip designs.},
   author = {Yiran Chen and Yuan Xie and Linghao Song and Fan Chen and Tianqi Tang},
   doi = {10.1016/j.eng.2020.01.007},
   issn = {20958099},
   issue = {3},
   journal = {Engineering},
   keywords = {Accelerator,Deep neural network,Domain-specific architecture},
   month = {3},
   pages = {264-274},
   publisher = {Elsevier Ltd},
   title = {A Survey of Accelerator Architectures for Deep Neural Networks},
   volume = {6},
   year = {2020},
}
@article{Reuther2020,
   abstract = {New machine learning accelerators are being announced and released each month for a variety of applications from speech recognition, video object detection, assisted driving, and many data center applications. This paper updates the survey of of AI accelerators and processors from last year's IEEE-HPEC paper. This paper collects and summarizes the current accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. This year, there are many more announced accelerators that are implemented with many more architectures and technologies from vector engines, dataflow engines, neuromorphic designs, flash-based analog memory processing, and photonic-based processing.},
   author = {Albert Reuther and Peter Michaleas and Michael Jones and Vijay Gadepally and Siddharth Samsi and Jeremy Kepner},
   doi = {10.1109/HPEC43674.2020.9286149},
   month = {8},
   title = {Survey of Machine Learning Accelerators},
   url = {http://arxiv.org/abs/2009.00993 http://dx.doi.org/10.1109/HPEC43674.2020.9286149},
   year = {2020},
}
@misc{Sze2017,
   abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
   author = {Vivienne Sze and Yu Hsin Chen and Tien Ju Yang and Joel S. Emer},
   doi = {10.1109/JPROC.2017.2761740},
   issn = {15582256},
   issue = {12},
   journal = {Proceedings of the IEEE},
   keywords = {ASIC,VLSI,computer architecture,convolutional neural networks,dataflow processing,deep learning,deep neural networks,energy-efficient accelerators,low power,machine learning,spatial architectures},
   month = {12},
   pages = {2295-2329},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
   volume = {105},
   year = {2017},
}
@article{Bulatov2023,
   abstract = {This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.},
   author = {Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
   month = {4},
   title = {Scaling Transformer to 1M tokens and beyond with RMT},
   url = {http://arxiv.org/abs/2304.11062},
   year = {2023},
}
@article{Wang2022,
   abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
   author = {Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
   title = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
   year = {2022},
}
@article{Liang2022,
   abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
   author = {Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher RÃ© and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
   title = {Holistic Evaluation of Language Models},
   year = {2022},
}
@article{Zhao2023,
   abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
   author = {Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Shen Li},
   month = {4},
   title = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
   url = {http://arxiv.org/abs/2304.11277},
   year = {2023},
}
@article{Aminabadi2022,
   abstract = {The past several years have witnessed the success of transformer-based models, and their scale and application scenarios continue to grow aggressively. The current landscape of transformer models is increasingly diverse: the model size varies drastically with the largest being of hundred-billion parameters; the model characteristics differ due to the sparsity introduced by the Mixture-of-Experts; the target application scenarios can be latency-critical or throughput-oriented; the deployment hardware could be single- or multi-GPU systems with different types of memory and storage, etc. With such increasing diversity and the fast-evolving pace of transformer models, designing a highly performant and efficient inference system is extremely challenging. In this paper, we present DeepSpeed Inference, a comprehensive system solution for transformer model inference to address the above-mentioned challenges. DeepSpeed Inference consists of (1) a multi-GPU inference solution to minimize latency while maximizing the throughput of both dense and sparse transformer models when they fit in aggregate GPU memory, and (2) a heterogeneous inference solution that leverages CPU and NVMe memory in addition to the GPU memory and compute to enable high inference throughput with large models which do not fit in aggregate GPU memory. DeepSpeed Inference reduces latency by up to 7.3X over the state-of-the-art for latency-oriented scenarios and increases throughput by over 1.5x for throughput-oriented scenarios. Moreover, it enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of GPUs, an unprecedented scale for inference. It can inference 25x larger models than with GPU-only solutions, while delivering a high throughput of 84 TFLOPS (over $50\%$ of A6000 peak).},
   author = {Reza Yazdani Aminabadi and Samyam Rajbhandari and Minjia Zhang and Ammar Ahmad Awan and Cheng Li and Du Li and Elton Zheng and Jeff Rasley and Shaden Smith and Olatunji Ruwase and Yuxiong He},
   month = {6},
   title = {DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
   url = {http://arxiv.org/abs/2207.00032},
   year = {2022},
}
@article{Dao2023,
   abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).},
   author = {Tri Dao},
   month = {7},
   title = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
   url = {http://arxiv.org/abs/2307.08691},
   year = {2023},
}
@article{Press2021,
   abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
   author = {Ofir Press and Noah A. Smith and Mike Lewis},
   month = {8},
   title = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
   url = {http://arxiv.org/abs/2108.12409},
   year = {2021},
}
@article{Lepikhin2020,
   abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
   author = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
   month = {6},
   title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
   url = {http://arxiv.org/abs/2006.16668},
   year = {2020},
}
@article{Naveed2023,
   abstract = {Large Language Models (LLMs) have shown excellent generalization capabilities that have led to the development of numerous models. These models propose various new architectures, tweaking existing architectures with refined training strategies, increasing context length, using high-quality training data, and increasing training time to outperform baselines. Analyzing new developments is crucial for identifying changes that enhance training stability and improve generalization in LLMs. This survey paper comprehensively analyses the LLMs architectures and their categorization, training strategies, training datasets, and performance evaluations and discusses future research directions. Moreover, the paper also discusses the basic building blocks and concepts behind LLMs, followed by a complete overview of LLMs, including their important features and functions. Finally, the paper summarizes significant findings from LLM research and consolidates essential architectural and training strategies for developing advanced LLMs. Given the continuous advancements in LLMs, we intend to regularly update this paper by incorporating new sections and featuring the latest LLM models.},
   author = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Nick Barnes and Ajmal Mian},
   month = {7},
   title = {A Comprehensive Overview of Large Language Models},
   url = {http://arxiv.org/abs/2307.06435},
   year = {2023},
}
@article{Zhao2023,
   abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
   author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
   month = {3},
   title = {A Survey of Large Language Models},
   url = {http://arxiv.org/abs/2303.18223},
   year = {2023},
}
@article{Zhai2021,
   abstract = {We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.},
   author = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
   month = {5},
   title = {An Attention Free Transformer},
   url = {http://arxiv.org/abs/2105.14103},
   year = {2021},
}
@article{Hinton2015,
   abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
   author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
   month = {3},
   title = {Distilling the Knowledge in a Neural Network},
   url = {http://arxiv.org/abs/1503.02531},
   year = {2015},
}
@article{Han2015,
   abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
   author = {Song Han and Jeff Pool and John Tran and William J. Dally},
   month = {6},
   title = {Learning both Weights and Connections for Efficient Neural Networks},
   url = {http://arxiv.org/abs/1506.02626},
   year = {2015},
}
@article{Yu2017,
   abstract = {To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the "final response layer" (FRL), which is the second-to-last layer before classification, for a pruned network to retrain its predictive power. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and then fine-tuned to retain its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.},
   author = {Ruichi Yu and Ang Li and Chun-Fu Chen and Jui-Hsin Lai and Vlad I. Morariu and Xintong Han and Mingfei Gao and Ching-Yung Lin and Larry S. Davis},
   month = {11},
   title = {NISP: Pruning Networks using Neuron Importance Score Propagation},
   url = {http://arxiv.org/abs/1711.05908},
   year = {2017},
}
@article{Jacob2017,
   abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
   author = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
   month = {12},
   title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
   url = {http://arxiv.org/abs/1712.05877},
   year = {2017},
}
@misc{,
   abstract = {Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as "WAGE" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices , we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.},
   author = {Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},
   isbn = {1802.04680v1},
   title = {Published as a conference paper at ICLR 2018 TRAINING AND INFERENCE WITH INTEGERS IN DEEP NEURAL NETWORKS},
   url = {https://github.com/boluoweifenda/WAGE},
}
@article{Zheng2020,
   abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy. We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to $3.8\times$, $2.6\times$, and $1.7\times$, respectively.},
   author = {Lianmin Zheng and Chengfan Jia and Minmin Sun and Zhao Wu and Cody Hao Yu and Ameer Haj-Ali and Yida Wang and Jun Yang and Danyang Zhuo and Koushik Sen and Joseph E. Gonzalez and Ion Stoica},
   month = {6},
   title = {Ansor : Generating High-Performance Tensor Programs for Deep Learning},
   url = {http://arxiv.org/abs/2006.06762},
   year = {2020},
}
@article{Xia2022,
   abstract = {We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\times$ faster than CSWin and 2.0$\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\times$.},
   author = {Xin Xia and Jiashi Li and Jie Wu and Xing Wang and Xuefeng Xiao and Min Zheng and Rui Wang},
   month = {5},
   title = {TRT-ViT: TensorRT-oriented Vision Transformer},
   url = {http://arxiv.org/abs/2205.09579},
   year = {2022},
}
@article{Zhang2020,
   abstract = {Deep learning (DL) models have become core modules for many applications. However, deploying these models without careful performance benchmarking that considers both hardware and software's impact often leads to poor service and costly operational expenditure. To facilitate DL models' deployment, we implement an automatic and comprehensive benchmark system for DL developers. To accomplish benchmark-related tasks, the developers only need to prepare a configuration file consisting of a few lines of code. Our system, deployed to a leader server in DL clusters, will dispatch users' benchmark jobs to follower workers. Next, the corresponding requests, workload, and even models can be generated automatically by the system to conduct DL serving benchmarks. Finally, developers can leverage many analysis tools and models in our system to gain insights into the trade-offs of different system configurations. In addition, a two-tier scheduler is incorporated to avoid unnecessary interference and improve average job compilation time by up to 1.43x (equivalent of 30\% reduction). Our system design follows the best practice in DL clusters operations to expedite day-to-day DL service evaluation efforts by the developers. We conduct many benchmark experiments to provide in-depth and comprehensive evaluations. We believe these results are of great values as guidelines for DL service configuration and resource allocation.},
   author = {Huaizheng Zhang and Yizheng Huang and Yonggang Wen and Jianxiong Yin and Kyle Guan},
   month = {11},
   title = {InferBench: Understanding Deep Learning Inference Serving with an Automatic Benchmarking System},
   url = {http://arxiv.org/abs/2011.02327},
   year = {2020},
}
@article{,
   abstract = {OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool's usefulness to society and how the learning and validation procedures for such systems should be established.},
   author = {Jan KocoÅ and Igor Cichecki and Oliwier Kaszyca and Mateusz Kochanek and Dominika SzydÅo and Joanna Baran and Julita Bielaniewicz and Marcin Gruza and Arkadiusz Janz and Kamil Kanclerz and Anna KocoÅ and BartÅomiej Koptyra and Wiktoria Mieleszczenko-Kowszewicz and Piotr MiÅkowski and Marcin Oleksy and Maciej Piasecki and Åukasz RadliÅski and Konrad Wojtasik and StanisÅaw WoÅºniak and PrzemysÅaw Kazienko},
   doi = {10.1016/j.inffus.2023.101861},
   month = {2},
   title = {ChatGPT: Jack of all trades, master of none},
   url = {http://arxiv.org/abs/2302.10724 http://dx.doi.org/10.1016/j.inffus.2023.101861},
   year = {2023},
}
@article{Touvron2023,
   abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
   author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and TimothÃ©e Lacroix and Baptiste RoziÃ¨re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
   month = {2},
   title = {LLaMA: Open and Efficient Foundation Language Models},
   url = {http://arxiv.org/abs/2302.13971},
   year = {2023},
}
@inproceedings{Choi2020,
   abstract = {With the growing number of GPU-based supercomputing platforms and GPU-enabled applications, the ability to accurately model the performance of such applications is becoming increasingly important. Most current performance models for GPU-enabled applications are limited to single node performance. In this work, we propose a methodology for end-to-end performance modeling of distributed GPU applications. Our work strives to create performance models that are both accurate and easily applicable to any distributed GPU application. We combine trace-driven simulation of MPI communication using the TraceR-CODES framework with a profiling-based roofline model for GPU kernels. We make substantial modifications to these models to capture the complex effects of both on-node and off-node networks in today's multi-GPU supercomputers. We validate our model against empirical data from GPU platforms and also vary tunable parameters of our model to observe how they might affect application performance.},
   author = {Jaemin Choi and David F. Richards and Laxmikant V. Kale and Abhinav Bhatele},
   doi = {10.1145/3392717.3392737},
   isbn = {9781450379830},
   journal = {Proceedings of the International Conference on Supercomputing},
   keywords = {GPU computing,communication,performance modeling,trace-driven simulation},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {End-to-end performance modeling of distributed GPU applications},
   year = {2020},
}
@article{,
   abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
   author = {Baptiste RoziÃ¨re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and JÃ©rÃ©my Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre DÃ©fossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
   month = {8},
   title = {Code Llama: Open Foundation Models for Code},
   url = {http://arxiv.org/abs/2308.12950},
   year = {2023},
}
@inproceedings{Gao2020,
   abstract = {Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL framework runtime. We have evaluated DNNMem on 5 real-world representative models with different hyperparameters under 3 mainstream frameworks (TensorFlow, PyTorch, and MXNet). Our extensive experiments show that DNNMem is effective in estimating GPU memory consumption.},
   author = {Yanjie Gao and Yu Liu and Hongyu Zhang and Zhengxian Li and Yonghao Zhu and Haoxiang Lin and Mao Yang},
   doi = {10.1145/3368089.3417050},
   isbn = {9781450370431},
   journal = {ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {Deep learning,Estimation model,Memory consumption,Program analysis},
   month = {11},
   pages = {1342-1352},
   publisher = {Association for Computing Machinery, Inc},
   title = {Estimating GPU memory consumption of deep learning models},
   year = {2020},
}
@misc{,
   abstract = {Deep learning (DL) has dramatically evolved and become one of the most successful machine learning techniques. A variety of DL-enabled applications have been widely integrated into software systems, including embedded ones. Although having achieved very successful results in accuracy, the large size of deep neural networks could require significant runtime and computing resource consumption. To overcome these drawbacks, TensorRT has been developed and may be incorporated into popular DL frameworks such as PyTorch and Open Neural Network Exchange (ONNX). In this paper, focusing on inference, we provide a comprehensive evaluation on the performances of TensorRT. Specifically, we evaluate inference output validation, inference time, inference throughput, and GPU memory usage. Our results demonstrate that TensorRT is able to significantly improve the inference efficiency metrics without compromising inference accuracy. Furthermore, TensorRT may be adopted via several alternative workflows. Our evaluation also shows the pros and cons of these TensorRT workflows. We analyze that for each workflow and discuss the workflow selection for different application scenarios.},
   author = {Yuxiao Zhou and Kecheng Yang},
   keywords = {Index Terms-deep learning,TensorRT,learning-enabled embedded systems,real-time inference},
   title = {Exploring TensorRT to Improve Real-Time Inference for Deep Learning},
}
@article{Payvar2021,
   abstract = {Efficient usage of heterogeneous computing architectures requires distribution of the workload on available processing elements. Traditionally, the mapping is based on information acquired from application profiling and utilized in architecture exploration. To reduce the amount of manual work required, statistical application modeling and architecture modeling can be combined with exploration heuristics. While the application modeling side of the problem has been studied extensively, architecture modeling has received less attention. Linear System Level Architecture (LSLA) is a Model of Architecture that aims at separating the architectural concerns from algorithmic ones when predicting performance. This work builds on the LSLA model and introduces non-linear semantics, specifically to support GPU performance and power modeling, by modeling also the degree of parallelism. The model is evaluated with three signal processing applications with various workload distributions on a desktop GPU and mobile GPU. The measured average fidelity of the new model is 93% for performance, and 84% for power, which can fit design space exploration purposes.},
   author = {Saman Payvar and Maxime Pelcat and Timo D. HÃ¤mÃ¤lÃ¤inen},
   doi = {10.1007/s10617-020-09244-4},
   issn = {15728080},
   issue = {1},
   journal = {Design Automation for Embedded Systems},
   keywords = {Design space exploration,Model of architecture,Modeling,Signal processing systems},
   month = {3},
   pages = {43-63},
   publisher = {Springer},
   title = {A model of architecture for estimating GPU processing performance and power},
   volume = {25},
   year = {2021},
}
@article{Zhang2018,
   abstract = {The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, agile management of network resource to maximize user experience, and extraction of fine-grained real-time analytics. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques to help managing the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.},
   author = {Chaoyun Zhang and Paul Patras and Hamed Haddadi},
   month = {3},
   title = {Deep Learning in Mobile and Wireless Networking: A Survey},
   url = {http://arxiv.org/abs/1803.04311},
   year = {2018},
}
@article{Arulkumaran2017,
   abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
   author = {Kai Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
   doi = {10.1109/MSP.2017.2743240},
   month = {8},
   title = {A Brief Survey of Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/1708.05866 http://dx.doi.org/10.1109/MSP.2017.2743240},
   year = {2017},
}
@article{Jagannath2019,
   abstract = {The Internet of Things (IoT) is expected to require more effective and efficient wireless communications than ever before. For this reason, techniques such as spectrum sharing, dynamic spectrum access, extraction of signal intelligence and optimized routing will soon become essential components of the IoT wireless communication paradigm. Given that the majority of the IoT will be composed of tiny, mobile, and energy-constrained devices, traditional techniques based on a priori network optimization may not be suitable, since (i) an accurate model of the environment may not be readily available in practical scenarios; (ii) the computational requirements of traditional optimization techniques may prove unbearable for IoT devices. To address the above challenges, much research has been devoted to exploring the use of machine learning to address problems in the IoT wireless communications domain. This work provides a comprehensive survey of the state of the art in the application of machine learning techniques to address key problems in IoT wireless communications with an emphasis on its ad hoc networking aspect. First, we present extensive background notions of machine learning techniques. Then, by adopting a bottom-up approach, we examine existing work on machine learning for the IoT at the physical, data-link and network layer of the protocol stack. Thereafter, we discuss directions taken by the community towards hardware implementation to ensure the feasibility of these techniques. Additionally, before concluding, we also provide a brief discussion of the application of machine learning in IoT beyond wireless communication. Finally, each of these discussions is accompanied by a detailed analysis of the related open problems and challenges.},
   author = {Jithin Jagannath and Nicholas Polosky and Anu Jagannath and Francesco Restuccia and Tommaso Melodia},
   doi = {10.1016/j.adhoc.2019.101913},
   month = {1},
   title = {Machine Learning for Wireless Communications in the Internet of Things: A Comprehensive Survey},
   url = {http://arxiv.org/abs/1901.07947 http://dx.doi.org/10.1016/j.adhoc.2019.101913},
   year = {2019},
}
@inproceedings{Jernite2022,
   abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
   author = {Yacine Jernite and Huu Nguyen and Stella Biderman and Anna Rogers and Maraim Masoud and Valentin Danchev and Samson Tan and Alexandra Sasha Luccioni and Nishant Subramani and Isaac Johnson and Gerard Dupont and Jesse Dodge and Kyle Lo and Zeerak Talat and Dragomir Radev and Aaron Gokaslan and Somaieh Nikpoor and Peter Henderson and Rishi Bommasani and Margaret Mitchell},
   doi = {10.1145/3531146.3534637},
   isbn = {9781450393522},
   journal = {ACM International Conference Proceeding Series},
   keywords = {data rights,datasets,language data,technology governance},
   month = {6},
   pages = {2206-2222},
   publisher = {Association for Computing Machinery},
   title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
   year = {2022},
}
@article{Fan2022,
   abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
   author = {Linxi Fan and Guanzhi Wang and Yunfan Jiang and Ajay Mandlekar and Yuncong Yang and Haoyi Zhu and Andrew Tang and De-An Huang and Yuke Zhu and Anima Anandkumar},
   month = {6},
   title = {MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge},
   url = {http://arxiv.org/abs/2206.08853},
   year = {2022},
}
@article{Albrecht2022,
   abstract = {Despite impressive successes, deep reinforcement learning (RL) systems still fall short of human performance on generalization to new tasks and environments that differ from their training. As a benchmark tailored for studying RL generalization, we introduce Avalon, a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks. Avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. We find that standard RL baselines make progress on most tasks but are still far from human performance, suggesting Avalon is challenging enough to advance the quest for generalizable RL.},
   author = {Joshua Albrecht and Abraham J. Fetterman and Bryden Fogelman and Ellie Kitanidis and Bartosz WrÃ³blewski and Nicole Seo and Michael Rosenthal and Maksis Knutins and Zachary Polizzi and James B. Simon and Kanjun Qiu},
   month = {10},
   title = {Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds},
   url = {http://arxiv.org/abs/2210.13417},
   year = {2022},
}
@article{Jeong2022,
   abstract = {As deep learning inference applications are increasing in embedded devices, an embedded device tends to equip neural processing units (NPUs) in addition to a multi-core CPU and a GPU. NVIDIA Jetson AGX Xavier is an example. For fast and efficient development of deep learning applications, TensorRT is provided as the SDK for high-performance inference, including an optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. Like most deep learning frameworks, TensorRT assumes that the inference is executed on a single processing element, GPU or NPU, not both. In this article, we present a TensorRT-based framework supporting various optimization parameters to accelerate a deep learning application targeted on an NVIDIA Jetson embedded platform with heterogeneous processors, including multi-threading, pipelining, buffer assignment, and network duplication. Since the design space of allocating layers to diverse processing elements and optimizing other parameters is huge, we devise a parameter optimization methodology that consists of a heuristic for balancing pipeline stages among heterogeneous processors and fine-tuning the process for optimizing parameters. With nine real-life benchmarks, we could achieve 101%â¼680% performance improvement and up to 55% energy reduction over the baseline inference using a GPU only.},
   author = {Eunjin Jeong and Jangryul Kim and Soonhoi Ha},
   doi = {10.1145/3508391},
   issn = {15583465},
   issue = {5},
   journal = {ACM Transactions on Embedded Computing Systems},
   keywords = {Deep learning,acceleration,framework,optimization},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {TensorRT-Based Framework and Optimization Methodology for Deep Learning Inference on Jetson Boards},
   volume = {21},
   year = {2022},
}
@misc{,
   abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachan-dran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available 1 .},
   author = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin JaggÃ­ JaggÃ­},
   title = {ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS},
}
@misc{,
   author = {Michael Wharton and Zhangzhang Si},
   title = {How to Put AI Models Into Production A Guide to Accelerated Inference},
}
@article{He2023,
   abstract = {The recent dramatic progress in machine learning is partially attributed to the availability of high-performant computers and development tools. The accelerated linear algebra (XLA) compiler is one such tool that automatically optimises array operations (mostly fusion to reduce memory operations) and compiles the optimised operations into high-performant programs specific to target computing platforms. Like machine-learning models, numerical models are often expressed in array operations, and thus their performance can be boosted by XLA. This study is the first of its kind to examine the efficiency of XLA for numerical models, and the efficiency is examined stringently by comparing its performance with that of optimal implementations. Two shared-memory computing platforms are examinedâthe CPU platform and the GPU platform. To obtain optimal implementations, the computing speed and its optimisation are rigorously studied by considering different workloads and the corresponding computer performance. Two simple equations are found to faithfully modell the computing speed of numerical models with very few easily-measureable parameters. Regarding operation optimisation within XLA, results show that models expressed in low-level operations (e.g., slice, concatenation, and arithmetic operations) are successfully fused while high-level operations (e.g., convolution and roll) are not. Regarding compilation within XLA, results show that for the CPU platform of certain computers and certain simple numerical models on the GPU platform, XLA achieves high efficiency (> 80%) for large problems and acceptable efficiency (10%~80%) for medium-size problemsâthe gap is from the overhead cost of Python. Unsatisfactory performance is found for the CPU platform of other computers (operations are compiled in a non-optimal way) and for high-dimensional complex models for the GPU platform, where each GPU thread in XLA handles 4 (single precision) or 2 (double precision) output elementsâhoping to exploit the high-performant instructions that can read/write 4 or 2 floating-point numbers with one instruction. However, these instructions are rarely used in the generated code for complex models and performance is negatively affected. Therefore, flags should be added to control the compilation for these non-optimal scenarios.},
   author = {Xuzhen He},
   doi = {10.1371/journal.pone.0282265},
   issn = {19326203},
   issue = {2 February},
   journal = {PLoS ONE},
   month = {2},
   pmid = {36827434},
   publisher = {Public Library of Science},
   title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
   volume = {18},
   year = {2023},
}
@article{,
   title = {inference-whitepaper-mar23-update},
}
@misc{,
   title = {NVIDIA TensorRT Developer Guide | NVIDIA Docs},
   year = {2023},
}
@article{Leviathan2022,
   abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
   author = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
   month = {11},
   title = {Fast Inference from Transformers via Speculative Decoding},
   url = {http://arxiv.org/abs/2211.17192},
   year = {2022},
}
@article{Thoppilan2022,
   abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
   author = {Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
   month = {1},
   title = {LaMDA: Language Models for Dialog Applications},
   url = {http://arxiv.org/abs/2201.08239},
   year = {2022},
}
@article{Gururangan2023,
   abstract = {Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.},
   author = {Suchin Gururangan and Margaret Li and Mike Lewis and Weijia Shi and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
   month = {3},
   title = {Scaling Expert Language Models with Unsupervised Domain Discovery},
   url = {http://arxiv.org/abs/2303.14177},
   year = {2023},
}
@misc{,
   abstract = {Larger networks generally have greater rep-resentational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static reg-ularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that Deep-MoEs are able to achieve higher accuracy with lower computation than standard con-volutional networks.},
   author = {Xin Wang and Fisher Yu and Lisa Dunlap and Yi-An Ma and Ruth Wang and Azalia Mirhoseini and Trevor Darrell and Joseph E Gonzalez},
   title = {Deep Mixture of Experts via Shallow Embedding},
}
@article{Zhou2022,
   abstract = {Sparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy (e.g. one resulting in load imbalance) can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2x. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.},
   author = {Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew Dai and Zhifeng Chen and Quoc Le and James Laudon},
   month = {2},
   title = {Mixture-of-Experts with Expert Choice Routing},
   url = {http://arxiv.org/abs/2202.09368},
   year = {2022},
}
@article{Chen2022,
   abstract = {The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.},
   author = {Zixiang Chen and Yihe Deng and Yue Wu and Quanquan Gu and Yuanzhi Li},
   month = {8},
   title = {Towards Understanding Mixture of Experts in Deep Learning},
   url = {http://arxiv.org/abs/2208.02813},
   year = {2022},
}
@inproceedings{Ma2018,
   abstract = {Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeos between task-specic objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with dierent levels of task relatedness, we rst apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benet, depending on dierent levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classication benchmark, and a large-scale content recommendation system at Google.},
   author = {Jiaqi Ma and Zhe Zhao and Xinyang Yi and Jilin Chen and Lichan Hong and Ed H. Chi},
   doi = {10.1145/3219819.3220007},
   isbn = {9781450355520},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {Mixture of experts,Multi-task learning,Neural network,Recommendation system},
   month = {7},
   pages = {1930-1939},
   publisher = {Association for Computing Machinery},
   title = {Modeling task relationships in multi-task learning with multi-gate mixture-of-experts},
   year = {2018},
}
@misc{,
   abstract = {Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfica-tion consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides , MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference , i.e., 2x speedup with 25% of FFN parameters , and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/ thunlp/MoEfication.},
   author = {Zhengyan Zhang and Yankai Lin and Zhiyuan Liu and Peng Li and Maosong Sun and Jie Zhou},
   title = {Findings of the Association for Computational Linguistics MoEfication: Transformer Feed-forward Layers are Mixtures of Experts},
   url = {https://github.com/},
}
@misc{,
   abstract = {Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities. In this paper, we present FastMoE , a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license.},
   author = {Jiaao He and Jiezhong Qiu and Aohan Zeng and Zhilin Yang and Jidong Zhai and Jie Tang},
   title = {FASTMOE: A FAST MIXTURE-OF-EXPERT TRAINING SYSTEM},
   url = {https://github.com/laekov/fastmoe},
}
@misc{,
   abstract = {Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.},
   author = {William Fedus and Google Brain and Jeff Dean and Barret Zoph},
   title = {A REVIEW OF SPARSE EXPERT MODELS IN DEEP LEARNING},
}
@article{Shen2023,
   abstract = {Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instructiontuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied byFLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning.},
   author = {Sheng Shen and Le Hou and Yanqi Zhou and Nan Du and Shayne Longpre and Jason Wei and Hyung Won Chung and Barret Zoph and William Fedus and Xinyun Chen and Tu Vu and Yuexin Wu and Wuyang Chen and Albert Webson and Yunxuan Li and Vincent Zhao and Hongkun Yu and Kurt Keutzer and Trevor Darrell and Denny Zhou},
   title = {Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models},
   year = {2023},
}
@article{Xi2023,
   abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
   author = {Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
   title = {The Rise and Potential of Large Language Model Based Agents: A Survey},
   year = {2023},
}
@article{Gesmundo2022,
   abstract = {Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence.We propose an evolutionary method capable of generating large scale multitask models that support the dynamic addition of new tasks. The generated multitask models are sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands.The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. We demonstrate empirically that the proposed method can jointly solve and achieve competitive results on 69public image classification tasks, for example improving the state of the art on a competitive benchmark such as cifar10 by achieving a 15% relative error reduction compared to the best model trained on public data.},
   author = {Andrea Gesmundo and Jeff Dean},
   month = {5},
   title = {An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems},
   url = {http://arxiv.org/abs/2205.12755},
   year = {2022},
}
@article{Gesmundo2022,
   abstract = {Most uses of machine learning today involve training a model from scratch for a particular task, or sometimes starting with a model pretrained on a related task and then fine-tuning on a downstream task. Both approaches offer limited knowledge transfer between different tasks, time-consuming human-driven customization to individual tasks and high computational costs especially when starting from randomly initialized models. We propose a method that uses the layers of a pretrained deep neural network as building blocks to construct an ML system that can jointly solve an arbitrary number of tasks. The resulting system can leverage cross tasks knowledge transfer, while being immune from common drawbacks of multitask approaches such as catastrophic forgetting, gradients interference and negative transfer. We define an evolutionary approach designed to jointly select the prior knowledge relevant for each task, choose the subset of the model parameters to train and dynamically auto-tune its hyperparameters. Furthermore, a novel scale control method is employed to achieve quality/size trade-offs that outperform common fine-tuning techniques. Compared with standard fine-tuning on a benchmark of 10 diverse image classification tasks, the proposed model improves the average accuracy by 2.39% while using 47% less parameters per task.},
   author = {Andrea Gesmundo and Jeff Dean},
   month = {5},
   title = {muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems},
   url = {http://arxiv.org/abs/2205.10937},
   year = {2022},
}
@article{Liu2023,
   abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
   author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
   month = {4},
   title = {Visual Instruction Tuning},
   url = {http://arxiv.org/abs/2304.08485},
   year = {2023},
}
@article{Puigcerver2023,
   abstract = {Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better.},
   author = {Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
   month = {8},
   title = {From Sparse to Soft Mixtures of Experts},
   url = {http://arxiv.org/abs/2308.00951},
   year = {2023},
}
@article{Clark2022,
   abstract = {The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.},
   author = {Aidan Clark and Diego de las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake Hechtman and Trevor Cai and Sebastian Borgeaud and George van den Driessche and Eliza Rutherford and Tom Hennigan and Matthew Johnson and Katie Millican and Albin Cassirer and Chris Jones and Elena Buchatskaya and David Budden and Laurent Sifre and Simon Osindero and Oriol Vinyals and Jack Rae and Erich Elsen and Koray Kavukcuoglu and Karen Simonyan},
   month = {2},
   title = {Unified Scaling Laws for Routed Language Models},
   url = {http://arxiv.org/abs/2202.01169},
   year = {2022},
}
@article{Agostinelli2021,
   abstract = {Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-actions and find that this 157-fold increase in the size of the action space incurs less than a 4-fold increase in computation time and less than a 3-fold increase in number of nodes generated when performing Q* search. Furthermore, Q* search is up to 129 times faster and generates up to 1288 times fewer nodes than A* search. Finally, although obtaining admissible heuristic functions from deep neural networks is an ongoing area of research, we prove that Q* search is guaranteed to find a shortest path given a heuristic function that neither overestimates the cost of a shortest path nor underestimates the transition cost.},
   author = {Forest Agostinelli and Alexander Shmakov and Stephen McAleer and Roy Fox and Pierre Baldi},
   month = {2},
   title = {A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks},
   url = {http://arxiv.org/abs/2102.04518},
   year = {2021},
}
@article{Mialon2023,
   abstract = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
   author = {GrÃ©goire Mialon and ClÃ©mentine Fourrier and Craig Swift and Thomas Wolf and Yann LeCun and Thomas Scialom},
   month = {11},
   title = {GAIA: a benchmark for General AI Assistants},
   url = {http://arxiv.org/abs/2311.12983},
   year = {2023},
}
@misc{Pope2023,
   abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32Ã larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
   author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
   title = {EFFICIENTLY SCALING TRANSFORMER INFERENCE},
   year = {2023},
}
@article{Du2021,
   abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
   author = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
   month = {12},
   title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
   url = {http://arxiv.org/abs/2112.06905},
   year = {2021},
}
@article{Touvron2023,
   abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
   author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
   title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
   year = {2023},
}
@article{Ren2023,
   abstract = {The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-\{\Sigma\}. With parameter inherent from PanGu-\{\alpha\}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-\{\Sigma\} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.},
   author = {Xiaozhe Ren and Pingyi Zhou and Xinfan Meng and Xinjing Huang and Yadao Wang and Weichao Wang and Pengfei Li and Xiaoda Zhang and Alexander Podolskiy and Grigory Arshinov and Andrey Bout and Irina Piontkovskaya and Jiansheng Wei and Xin Jiang and Teng Su and Qun Liu and Jun Yao},
   month = {3},
   title = {PanGu-\{\Sigma\}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},
   url = {http://arxiv.org/abs/2303.10845},
   year = {2023},
}
@article{Liu2022,
   abstract = {Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.},
   author = {Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Yasemin Altun},
   month = {12},
   title = {DePlot: One-shot visual language reasoning by plot-to-table translation},
   url = {http://arxiv.org/abs/2212.10505},
   year = {2022},
}
@article{Chen2023,
   abstract = {Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining. Our results show that by only introducing a limited number of extra experts while keeping the computation cost constant, our model can steadily adapt to data distribution shifts while preserving the previous knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves better few-shot performance on 19 downstream NLP tasks.},
   author = {Wuyang Chen and Yanqi Zhou and Nan Du and Yanping Huang and James Laudon and Zhifeng Chen and Claire Cu},
   month = {5},
   title = {Lifelong Language Pretraining with Distribution-Specialized Experts},
   url = {http://arxiv.org/abs/2305.12281},
   year = {2023},
}
@article{Li2023,
   abstract = {The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.},
   author = {Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and JoÃ£o Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos MuÃ±oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
   month = {5},
   title = {StarCoder: may the source be with you!},
   url = {http://arxiv.org/abs/2305.06161},
   year = {2023},
}
@article{Lewis2020,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich KÃ¼ttler and Mike Lewis and Wen-tau Yih and Tim RocktÃ¤schel and Sebastian Riedel and Douwe Kiela},
   month = {5},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {http://arxiv.org/abs/2005.11401},
   year = {2020},
}
@article{Gale2022,
   abstract = {We present MegaBlocks, a system for efficient Mixture-of-Experts (MoE) training on GPUs. Our system is motivated by the limitations of current frameworks, which restrict the dynamic routing in MoE layers to satisfy the constraints of existing software and hardware. These formulations force a tradeoff between model quality and hardware efficiency, as users must choose between dropping tokens from the computation or wasting computation and memory on padding. To address these limitations, we reformulate MoE computation in terms of block-sparse operations and develop new block-sparse GPU kernels that efficiently handle the dynamism present in MoEs. Our approach never drops tokens and maps efficiently to modern hardware, enabling end-to-end training speedups of up to 40% over MoEs trained with the state-of-the-art Tutel library and 2.4x over DNNs trained with the highly-optimized Megatron-LM framework.},
   author = {Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
   month = {11},
   title = {MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
   url = {http://arxiv.org/abs/2211.15841},
   year = {2022},
}
@misc{,
   abstract = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.},
   author = {Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa},
   title = {Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations},
}
@article{Hwang2022,
   abstract = {Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.},
   author = {Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},
   month = {6},
   title = {Tutel: Adaptive Mixture-of-Experts at Scale},
   url = {http://arxiv.org/abs/2206.03382},
   year = {2022},
}
@article{Wang2022,
   abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
   author = {Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
   month = {12},
   title = {Text Embeddings by Weakly-Supervised Contrastive Pre-training},
   url = {http://arxiv.org/abs/2212.03533},
   year = {2022},
}
@article{Muennighoff2022,
   abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
   author = {Niklas Muennighoff and Nouamane Tazi and LoÃ¯c Magne and Nils Reimers},
   month = {10},
   title = {MTEB: Massive Text Embedding Benchmark},
   url = {http://arxiv.org/abs/2210.07316},
   year = {2022},
}
@article{,
   author = {Nikoli Dryden etc. Andrei Ivanov},
   title = {Data movement is all you need},
}
@misc{,
   abstract = {The rise of open-source, commercially permissive large language models (LLMs) is revolutionizing generative AI, presenting organizations with enhanced control, minimized data risks, and cost benefits compared to proprietary models. However, in the field of tool use and function-calling LLMs, many open-source models, such as Gorilla and ToolLLAMA, are dependent on proprietary LLMs like GPT-4 for high-quality training data, which often faces legal restrictions for competitive commercial applications. In this paper, we introduce NexusRaven-13B, an open-source LLM designed for function calls. Originating from the CodeLLAMA-13B lineage, NexusRaven-13B employs a unique data curation via multi-step refinement , ensuring high-quality training data without relying on GPT-4 distillation. NexusRaven-13B matches GPT-3.5 in zero-shot function-calling accuracy. When combined with our second core technique, demonstration retrieval augmentation, its performance significantly surpasses GPT-4. The code, model, and demo will be available after the review process.},
   author = {Venkat Krishna Srinivasan Nexusflow Zhen Dong Nexusflow Banghua Zhu and Brian Yu and Damon Mosk-Aoyama and Kurt Keutzer Nexusflow Jiantao Jiao Nexusflow Jian Zhang Nexusflow},
   title = {NexusRaven: a Commercially-Permissive Language Model for Function Calling},
}
@article{Chen2024,
   abstract = {In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \textit\{conditional masking\} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.},
   author = {Wei Chen and Zhiyuan Li and Mingyuan Ma},
   month = {4},
   title = {Octopus: On-device language model for function calling of software APIs},
   url = {http://arxiv.org/abs/2404.01549},
   year = {2024},
}
@article{Ong2024,
   abstract = {Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.},
   author = {Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
   month = {6},
   title = {RouteLLM: Learning to Route LLMs with Preference Data},
   url = {http://arxiv.org/abs/2406.18665},
   year = {2024},
}
@article{Wang2023,
   abstract = {Pretraining auto-regressive large language models~(LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at: https://huggingface.co/nvidia/retro-48b-instruct-4k.},
   author = {Boxin Wang and Wei Ping and Lawrence McAfee and Peng Xu and Bo Li and Mohammad Shoeybi and Bryan Catanzaro},
   month = {10},
   title = {InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining},
   url = {http://arxiv.org/abs/2310.07713},
   year = {2023},
}
@article{Wang2023,
   abstract = {Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT in both fine-tuning and zero-shot evaluation settings. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our code and model at: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md},
   author = {Boxin Wang and Wei Ping and Peng Xu and Lawrence McAfee and Zihan Liu and Mohammad Shoeybi and Yi Dong and Oleksii Kuchaiev and Bo Li and Chaowei Xiao and Anima Anandkumar and Bryan Catanzaro},
   month = {4},
   title = {Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study},
   url = {http://arxiv.org/abs/2304.06762},
   year = {2023},
}
@inproceedings{Narayanan2021,
   abstract = {Large language models have led to state-of-The-Art accuracies across several tasks. However, training these models efficiently is challenging because: A) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).},
   author = {Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
   doi = {10.1145/3458817.3476209},
   isbn = {9781450384421},
   issn = {21674337},
   journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
   month = {11},
   publisher = {IEEE Computer Society},
   title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
   year = {2021},
}
@article{Borgeaud2021,
   abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
   author = {Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},
   month = {12},
   title = {Improving language models by retrieving from trillions of tokens},
   url = {http://arxiv.org/abs/2112.04426},
   year = {2021},
}
@misc{Kruchten1995,
   abstract = {This article presents a model for describing the architecture of software-intensive systems, based on the use of multiple, concurrent views. This use of multiple views allows to address separately the concerns of the various 'stakeholders' of the architecture: end-user, developers, systems engineers, project managers, etc., and to handle separately the functional and non functional requirements. Each of the five views is described, together with a notation to capture it. The views are designed using an architecture-centered, scenario-driven, iterative development process.},
   author = {Philippe Kruchten},
   issue = {6},
   journal = {IEEE Software},
   keywords = {object-oriented design,software architecture,software development process,view},
   pages = {42-50},
   title = {Architectural Blueprints-The "4+1" View Model of Software Architecture},
   volume = {12},
   year = {1995},
}
@article{Shao2024,
   abstract = {We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.},
   author = {Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
   month = {2},
   title = {Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models},
   url = {http://arxiv.org/abs/2402.14207},
   year = {2024},
}
@article{Kim2023,
   abstract = {The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% compared to ReAct. Our code is available at https://github.com/SqueezeAILab/LLMCompiler.},
   author = {Sehoon Kim and Suhong Moon and Ryan Tabrizi and Nicholas Lee and Michael W. Mahoney and Kurt Keutzer and Amir Gholami},
   month = {12},
   title = {An LLM Compiler for Parallel Function Calling},
   url = {http://arxiv.org/abs/2312.04511},
   year = {2023},
}
@misc{,
   abstract = {Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup of up to two orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives on CPUs and a speedup of about three orders of magnitude on GPUs.},
   author = {SÃ¶ren Laue and Matthias Mitterreiter and Joachim Giesen},
   title = {Computing Higher Order Derivatives of Matrix and Tensor Expressions},
   url = {https://github.com/sympy/sympy/issues/10509.},
}
@misc{,
   abstract = {Computing derivatives of tensor expressions, also known as tensor calculus, is a fundamental task in machine learning. A key concern is the efficiency of evaluating the expressions and their derivatives that hinges on the representation of these expressions. Recently, an algorithm for computing higher order derivatives of tensor expressions like Jacobians or Hessians has been introduced that is a few orders of magnitude faster than previous state-of-the-art approaches. Unfortunately, the approach is based on Ricci notation and hence cannot be incorporated into automatic differentiation frameworks like TensorFlow, PyTorch, autograd, or JAX that use the simpler Einstein notation. This leaves two options, to either change the underlying tensor representation in these frameworks or to develop a new, provably correct algorithm based on Einstein notation. Obviously, the first option is impractical. Hence, we pursue the second option. Here, we show that using Ricci notation is not necessary for an efficient tensor calculus and develop an equally efficient method for the simpler Einstein notation. It turns out that turning to Einstein notation enables further improvements that lead to even better efficiency.},
   author = {SÃ¶ren Laue and Matthias Mitterreiter and Joachim Giesen},
   title = {A Simple and Efficient Tensor Calculus},
   url = {www.aaai.org},
}
@article{Edge2024,
   abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
   author = {Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
   month = {4},
   title = {From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
   url = {http://arxiv.org/abs/2404.16130},
   year = {2024},
}
@article{Chen2024,
   abstract = {The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \url\{https://github.com/OpenBMB/IoA\}.},
   author = {Weize Chen and Ziming You and Ran Li and Yitong Guan and Chen Qian and Chenyang Zhao and Cheng Yang and Ruobing Xie and Zhiyuan Liu and Maosong Sun},
   month = {7},
   title = {Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence},
   url = {http://arxiv.org/abs/2407.07061},
   year = {2024},
}
@article{Chen2024,
   abstract = {The area of portrait image animation, propelled by audio input, has witnessed notable progress in the generation of lifelike and dynamic portraits. Conventional methods are limited to utilizing either audios or facial key points to drive images into videos, while they can yield satisfactory results, certain issues exist. For instance, methods driven solely by audios can be unstable at times due to the relatively weaker audio signal, while methods driven exclusively by facial key points, although more stable in driving, can result in unnatural outcomes due to the excessive control of key point information. In addressing the previously mentioned challenges, in this paper, we introduce a novel approach which we named EchoMimic. EchoMimic is concurrently trained using both audios and facial landmarks. Through the implementation of a novel training strategy, EchoMimic is capable of generating portrait videos not only by audios and facial landmarks individually, but also by a combination of both audios and selected facial landmarks. EchoMimic has been comprehensively compared with alternative algorithms across various public datasets and our collected dataset, showcasing superior performance in both quantitative and qualitative evaluations. Additional visualization and access to the source code can be located on the EchoMimic project page.},
   author = {Zhiyuan Chen and Jiajiong Cao and Zhiquan Chen and Yuming Li and Chenguang Ma},
   month = {7},
   title = {EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions},
   url = {http://arxiv.org/abs/2407.08136},
   year = {2024},
}
@misc{,
   abstract = {Large language models (LLMs) with long-context instruction following ability has unlocked new potentials, such as supporting long interactive chat sessions. In this paper, we introduce a test suite, LongEval, which enables us to evaluate the long-range retrieval ability of LLMs at various context lengths. We use LongEval to evaluate open-sourced LLMs, and surprisingly, we find many of them fail to achieve their promised context length. In addition, we present a recipe to fine-tune a long-context chatbot based on LLaMA models, and introduce LongChat models that supporting conversations of up to 16,384 tokens. We have released our code at https://github.com/DachengLi1/LongChat.},
   author = {Dacheng Li and Rulin Shao and Anze E Xie Ying Sheng Lianmin Zheng Joseph Gonzalez Ion Stoica Xuezhe Ma Hao Zhang UC Berkeley University of Washington UCSD CMU MBZUAI USC},
   title = {How Long Can Context Length of Open-Source LLMs truly Promise?},
   url = {https://github.com/DachengLi1/LongChat.},
}
@article{Wang2024,
   abstract = {Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.},
   author = {Xindi Wang and Mahsa Salmani and Parsa Omidi and Xiangyu Ren and Mehdi Rezagholizadeh and Armaghan Eshaghi},
   month = {2},
   title = {Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models},
   url = {http://arxiv.org/abs/2402.02244},
   year = {2024},
}
@article{Shi2023,
   abstract = {Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. Our MultiresLayer is straightforward to implement, requires significantly fewer parameters, and maintains at most a $\mathcal\{O\}(N\log N)$ memory footprint for a length $N$ sequence. Yet, by stacking such layers, our model yields state-of-the-art performance on a number of sequence classification and autoregressive density estimation tasks using CIFAR-10, ListOps, and PTB-XL datasets.},
   author = {Jiaxin Shi and Ke Alexander Wang and Emily B. Fox},
   month = {5},
   title = {Sequence Modeling with Multiresolution Convolutional Memory},
   url = {http://arxiv.org/abs/2305.01638},
   year = {2023},
}
@article{An2024,
   abstract = {The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \url\{https://github.com/HKUNLP/ChunkLlama\}.},
   author = {Chenxin An and Fei Huang and Jun Zhang and Shansan Gong and Xipeng Qiu and Chang Zhou and Lingpeng Kong},
   month = {2},
   title = {Training-Free Long-Context Scaling of Large Language Models},
   url = {http://arxiv.org/abs/2402.17463},
   year = {2024},
}
@article{Song2024,
   abstract = {While recent research endeavors have focused on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of long-context benchmarks, relatively little is known about how well the performance of long-context LLMs. To address this gap, we propose a multi-evidence, position-aware, and scalable benchmark for evaluating long-context LLMs, named Counting-Stars, which evaluates long-context LLMs by using two tasks: multi-evidence acquisition and multi-evidence reasoning. Based on the Counting-Stars test, we conduct experiments to evaluate long-context LLMs (i.e., GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1). Experimental results demonstrate that Gemini 1.5 Pro achieves the best overall results, while the performance of GPT-4 Turbo is the most stable across various tasks. Furthermore, our analysis of these LLMs, which are extended to handle long-context scenarios, indicates that there is potential for improvement as the length of the input context and the intricacy of the tasks are increasing.},
   author = {Mingyang Song and Mao Zheng and Xuan Luo},
   month = {3},
   title = {Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models},
   url = {http://arxiv.org/abs/2403.11802},
   year = {2024},
}
@article{Yao2022,
   abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
   author = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
   month = {10},
   title = {ReAct: Synergizing Reasoning and Acting in Language Models},
   url = {http://arxiv.org/abs/2210.03629},
   year = {2022},
}
@article{Tang2023,
   abstract = {Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.},
   author = {Qiaoyu Tang and Ziliang Deng and Hongyu Lin and Xianpei Han and Qiao Liang and Boxi Cao and Le Sun},
   month = {6},
   title = {ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
   url = {http://arxiv.org/abs/2306.05301},
   year = {2023},
}
@article{Zhuang2023,
   abstract = {Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.},
   author = {Yuchen Zhuang and Yue Yu and Kuan Wang and Haotian Sun and Chao Zhang},
   month = {6},
   title = {ToolQA: A Dataset for LLM Question Answering with External Tools},
   url = {http://arxiv.org/abs/2306.13304},
   year = {2023},
}
@article{Ding2023,
   abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
   author = {Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
   month = {7},
   title = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
   url = {http://arxiv.org/abs/2307.02486},
   year = {2023},
}
@article{Zhang2024,
   abstract = {Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.},
   author = {Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun},
   month = {2},
   title = {$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens},
   url = {http://arxiv.org/abs/2402.13718},
   year = {2024},
}
@article{Qin2023,
   abstract = {Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. Overall, we hope this paper could inspire future research in integrating tools with foundation models.},
   author = {Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
   month = {4},
   title = {Tool Learning with Foundation Models},
   url = {http://arxiv.org/abs/2304.08354},
   year = {2023},
}
@article{Peng2023,
   abstract = {Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn},
   author = {Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
   month = {8},
   title = {YaRN: Efficient Context Window Extension of Large Language Models},
   url = {http://arxiv.org/abs/2309.00071},
   year = {2023},
}
@article{Zhang2024,
   abstract = {Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.},
   author = {Yikai Zhang and Junlong Li and Pengfei Liu},
   month = {1},
   title = {Extending LLMs' Context Window with 100 Samples},
   url = {http://arxiv.org/abs/2401.07004},
   year = {2024},
}
@article{Zheng2023,
   abstract = {Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang},
   author = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
   month = {12},
   title = {SGLang: Efficient Execution of Structured Language Model Programs},
   url = {http://arxiv.org/abs/2312.07104},
   year = {2023},
}
@article{Duan2024,
   abstract = {Large Language Models (LLMs) like GPT and LLaMA are revolutionizing the AI industry with their sophisticated capabilities. Training these models requires vast GPU clusters and significant computing time, posing major challenges in terms of scalability, efficiency, and reliability. This survey explores recent advancements in training systems for LLMs, including innovations in training infrastructure with AI accelerators, networking, storage, and scheduling. Additionally, the survey covers parallelism strategies, as well as optimizations for computation, communication, and memory in distributed LLM training. It also includes approaches of maintaining system reliability over extended training periods. By examining current innovations and future directions, this survey aims to provide valuable insights towards improving LLM training systems and tackling ongoing challenges. Furthermore, traditional digital circuit-based computing systems face significant constraints in meeting the computational demands of LLMs, highlighting the need for innovative solutions such as optical computing and optical networks.},
   author = {Jiangfei Duan and Shuo Zhang and Zerui Wang and Lijuan Jiang and Wenwen Qu and Qinghao Hu and Guoteng Wang and Qizhen Weng and Hang Yan and Xingcheng Zhang and Xipeng Qiu and Dahua Lin and Yonggang Wen and Xin Jin and Tianwei Zhang and Peng Sun},
   month = {7},
   title = {Efficient Training of Large Language Models on Distributed Infrastructures: A Survey},
   url = {http://arxiv.org/abs/2407.20018},
   year = {2024},
}
@article{Xie2024,
   abstract = {Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.},
   author = {Junlin Xie and Zhihong Chen and Ruifei Zhang and Xiang Wan and Guanbin Li},
   month = {2},
   title = {Large Multimodal Agents: A Survey},
   url = {http://arxiv.org/abs/2402.15116},
   year = {2024},
}
@article{Masterman2024,
   abstract = {This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.},
   author = {Tula Masterman and Sandi Besen and Mason Sawtell and Alex Chao},
   month = {4},
   title = {The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey},
   url = {http://arxiv.org/abs/2404.11584},
   year = {2024},
}
@article{Kelley2024,
   abstract = {Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise "language-model agents." In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.},
   author = {Richard Kelley},
   month = {4},
   title = {Behavior Trees Enable Structured Programming of Language Model Agents},
   url = {http://arxiv.org/abs/2404.07439},
   year = {2024},
}
@misc{Wang2024,
   abstract = {Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
   author = {Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Jirong Wen},
   doi = {10.1007/s11704-024-40231-1},
   issn = {20952236},
   issue = {6},
   journal = {Frontiers of Computer Science},
   keywords = {autonomous agent,human-level intelligence,large language model},
   month = {12},
   publisher = {Higher Education Press Limited Company},
   title = {A survey on large language model based autonomous agents},
   volume = {18},
   year = {2024},
}
@article{Colledanchise2017,
   abstract = {A Behavior Tree (BT) is a way to structure the switching between different tasks in an autonomous agent, such as a robot or a virtual entity in a computer game. BTs are a very efficient way of creating complex systems that are both modular and reactive. These properties are crucial in many applications, which has led to the spread of BT from computer game programming to many branches of AI and Robotics. In this book, we will first give an introduction to BTs, then we describe how BTs relate to, and in many cases generalize, earlier switching structures. These ideas are then used as a foundation for a set of efficient and easy to use design principles. Properties such as safety, robustness, and efficiency are important for an autonomous system, and we describe a set of tools for formally analyzing these using a state space description of BTs. With the new analysis tools, we can formalize the descriptions of how BTs generalize earlier approaches. We also show the use of BTs in automated planning and machine learning. Finally, we describe an extended set of tools to capture the behavior of Stochastic BTs, where the outcomes of actions are described by probabilities. These tools enable the computation of both success probabilities and time to completion.},
   author = {Michele Colledanchise and Petter Ãgren},
   doi = {10.1201/9780429489105},
   month = {8},
   title = {Behavior Trees in Robotics and AI: An Introduction},
   url = {http://arxiv.org/abs/1709.00084 http://dx.doi.org/10.1201/9780429489105},
   year = {2017},
}
